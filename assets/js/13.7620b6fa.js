(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{343:function(e,t,a){"use strict";a.r(t);var o=a(7),i=Object(o.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("div",{staticClass:"custom-block warning"},[t("p",{staticClass:"custom-block-title"},[e._v("Warning")]),e._v(" "),t("p",[e._v("This entry needs to be supplemented")])]),e._v(" "),t("p",[e._v("Please refer to "),t("a",{attrs:{href:"https://wangwei1237.github.io/2021/01/26/HDR-introduction/",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://wangwei1237.github.io/2021/01/26/HDR-introduction/"),t("OutboundLink")],1)]),e._v(" "),t("div",{staticClass:"custom-block tip"},[t("p",{staticClass:"custom-block-title"},[e._v("Tip")]),e._v(" "),t("p",[e._v("SVFI is able to automatically recognize if the input video has HDR and handle it properly. To specify the HDR format, please change "),t("code",[e._v("Source HDR")]),e._v(" to a type other than "),t("code",[e._v("AUTO")]),e._v(", for example, to specify metadata by yourself, select "),t("code",[e._v("CUSTOM HDR")])])]),e._v(" "),t("h1",{attrs:{id:"introduction-to-hdr-technology"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction-to-hdr-technology"}},[e._v("#")]),e._v(" Introduction to HDR Technology")]),e._v(" "),t("p",[e._v("The brightness range in the real world is very wide, and the brightness range that the human eye can perceive is about 100,000 nits. For example, using a spectrocolorimeter to measure a flower blooming towards the sun, the brightness of the yellow area can reach up to 14700 nits, the red edge is 2300 nits, and the central stamen and green leaves are only below 200 nits. However, under an SDR display with a narrow color gamut, brightness generally not exceeding 100 nits, and a contrast ratio of only 1000:1, the color of this photo will be much dimmer. But with the development of technology, HDR technology can achieve wide color gamut, 1000 nit brightness and tens of thousands of contrast. Although there is still a big difference from the actual standard, compared with the SDR of 30 years ago, HDR is still a big step forward.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://s2.loli.net/2022/09/03/zPTfvReNcH7Vb2g.jpg",alt:"color_space.jpg"}})]),e._v(" "),t("h2",{attrs:{id:"introduction-to-hdr"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction-to-hdr"}},[e._v("#")]),e._v(" Introduction to HDR")]),e._v(" "),t("p",[e._v("HDR is an abbreviation for "),t("em",[e._v("High Dynamic Range")]),e._v(". HDR is a digital image technology that can be used to enhance the color and contrast range of digital images.")]),e._v(" "),t("p",[e._v("Compared with SDR ("),t("em",[e._v("Standard Dynamic Range")]),e._v(": standard dynamic range), HDR has the following characteristics:")]),e._v(" "),t("ul",[t("li",[e._v("Wider color range")]),e._v(" "),t("li",[e._v("Brighter brightness cap")]),e._v(" "),t("li",[e._v("Darker brightness lower limit")]),e._v(" "),t("li",[e._v("The overall image quality has been improved in terms of contrast, grayscale resolution and other dimensions")])]),e._v(" "),t("p",[e._v("Thus, HDR can bring users a more immersive experience.")]),e._v(" "),t("p",[e._v("HDR technology can be applied to photography ("),t("em",[e._v("photography")]),e._v(") and video ("),t("em",[e._v("video")]),e._v("), but it should be noted that although the goals of "),t("code",[e._v("photo HDR")]),e._v(" and "),t("code",[e._v("video HDR")]),e._v(" are to make the digital content we see more similar to the real experience of human eyes, but these two are indeed two completely different concepts. Essentially, "),t("code",[e._v("photo HDR")]),e._v(" is a process in which a camera captures photos ("),t("em",[e._v("capture")]),e._v("), while "),t("code",[e._v("video HDR")]),e._v(" is a process in which "),t("em",[e._v("display")]),e._v(" is displayed.")]),e._v(" "),t("h2",{attrs:{id:"photometric-concepts-and-units"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#photometric-concepts-and-units"}},[e._v("#")]),e._v(" Photometric concepts and units")]),e._v(" "),t("p",[e._v("Before further introducing HDR, we need to understand some related concepts of "),t("a",{attrs:{href:"http://openstd.samr.gov.cn/bzgk/gb/newGbInfo?hcno=AB4FAB66987D97573EA90F4DD56ABA36",target:"_blank",rel:"noopener noreferrer"}},[e._v("Photometry"),t("OutboundLink")],1),e._v(" so that we can have a better understanding of HDR understanding and awareness.")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Radiation flux")]),e._v(": In optical radiometry, the basic physical quantity used is "),t("code",[e._v("radiation flux")]),e._v(" or "),t("code",[e._v("radiation power")]),e._v(", the symbol is expressed as: Φe\\Phi_{e}Φe​, The unit is Watt (WWW). Radiant flux is emitted by a radiation source, transmitted through some propagation medium and received on some surface.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Luminous flux")]),e._v(": "),t("code",[e._v("Luminous flux")]),e._v(" refers to the derived amount of radiant flux evaluated according to the international standard human visual characteristics, that is, the photometric measure of "),t("code",[e._v("radiant flux")]),e._v(", and its symbol is: Φv\\Phi _{v}Φv​. The "),t("code",[e._v("luminous flux")]),e._v(" can be derived from the "),t("code",[e._v("radiant flux")]),e._v(".")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Light Amount")]),e._v(": "),t("code",[e._v("Light Amount")]),e._v(" is also called "),t("code",[e._v("Light Energy")]),e._v(", which refers to the time integral of "),t("code",[e._v("Luminous Flux")]),e._v(" within a given time interval (Δt\\Delta tΔt), and the symbol is expressed as: QvQ_ {v}Qv​, and Qv=∫ΔtΦv⋅dtQ_{v}=\\int_{\\Delta t}{\\Phi_{v} \\ \\cdot dt}Qv​=∫Δt​Φv​⋅dt.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Luminous intensity")]),e._v(": "),t("code",[e._v("Luminous intensity")]),e._v(" (IvI_{v}Iv​) is the luminous flux dΦvd \\Phi_ emitted by the light source and propagated within the solid angle element dΩd \\OmegadΩ in the specified direction The quotient of {v}dΦv​and the solid angle element dΩd \\OmegadΩ, namely: Iv=dΦvdΩI_{v}=\\frac{d \\Phi_{v}\\ }{d \\Omega}Iv​=dΩdΦv​​. The unit of "),t("code",[e._v("luminous intensity")]),e._v(" is "),t("code",[e._v("candela")]),e._v(" (cdcdcd). Candela is the luminous intensity in a specified direction of a light source emitting monochromatic radiation with a frequency of 540×1012Hz540 \\times 10^{12} Hz540×1012Hz, and the radiant intensity of the light source in this direction is 1683W⋅ sr\\frac{1}{683} W \\cdot sr6831​W⋅sr.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Luminance")]),e._v(": "),t("code",[e._v("Luminance")]),e._v(" (IvI_{v}Iv​) refers to the "),t("code",[e._v("luminous intensity")]),e._v(" of a surface light source per unit projected area in a certain direction along that direction. The area of the small surface element on the surface light source is dAdAdA, the angle between a certain direction and the surface element normal is θ\\thetaθ, and the projected area of the surface element along this direction is dA⋅cosθdA \\cdot cos \\thetadA⋅cosθ, Then the luminance of the surface element along this direction is: IvdA⋅cosθ\\frac{I_{v}}{dA \\cdot cos \\theta}dA⋅cosθIv​​, namely Iv\\ =dΦvdA⋅cosθ⋅dΩI_{v}=\\frac{d \\Phi_{v}}{dA \\cdot cos \\theta \\cdot d \\Omega\\ }Iv​=dA⋅cosθ⋅dΩdΦv​​. It can be seen from this that the unit of "),t("code",[e._v("brightness")]),e._v(" is cd/m2cd/m^{2}cd/m2 (candela/square meter), also known as "),t("code",[e._v("nit")]),e._v(". For example, the brightness of the screen of the iPhone 12 is: 625 nits maximum brightness (typical), 1200 nits maximum brightness (HDR).")])])]),e._v(" "),t("h2",{attrs:{id:"photography-hdr"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#photography-hdr"}},[e._v("#")]),e._v(" photography HDR")]),e._v(" "),t("p",[e._v("HDR photos can be generated by simultaneously capturing multiple images at different exposures. The camera quickly takes three or more photos with different exposure values, which are then processed to produce a single HDR photo.")]),e._v(" "),t("p",[e._v("HDR image processing is very useful when taking photos of high-contrast scenes.")]),e._v(" "),t("p",[e._v("As shown in the night scene below, for example, when the exposure is too low, it can make the scenery behind appear dark. If the exposure is adjusted to show what's behind, what's in front will again be too bright, and may even be completely white. By combining multiple exposures, these different exposures can be averaged so that the scenery behind can appear in the photo without overexposing the scenery in front. [^2]")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://s2.loli.net/2022/09/03/YKFekM89zTUp5Qa.png",alt:"EVDifferent.png"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://s2.loli.net/2022/09/03/Huf7VntP5U1Xq83.png",alt:"EVToneMapping.png"}})]),e._v(" "),t("p",[e._v("Currently, the cameras of many smartphones, such as the iPhone, have an HDR option. We can choose to have the camera:")]),e._v(" "),t("ul",[t("li",[e._v("Turn on HDR (ON)")]),e._v(" "),t("li",[e._v("Turn off HDR (OFF)")]),e._v(" "),t("li",[e._v("Automatically determine whether to enable HDR (Auto)")])]),e._v(" "),t("p",[e._v("With Auto mode, the camera only takes HDR photos in high-contrast situations. Most cameras will also capture normal photos while compositing HDR photos, allowing us to choose between HDR photos and normal photos.")]),e._v(" "),t("h3",{attrs:{id:"dodging-and-burning-and-tone-mapping"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dodging-and-burning-and-tone-mapping"}},[e._v("#")]),e._v(" dodging and burning and tone mapping")]),e._v(" "),t("p",[e._v("In fact, in the film era, photographers would take multiple negatives with different exposure levels, and in the [darkroom](https://baike.baidu.com/item/%E6%9A%97%E6%88%BF /62291) uses a method of **"),t("a",{attrs:{href:"https://en.wikipedia.org/wiki/Dodging_and_burning",target:"_blank",rel:"noopener noreferrer"}},[e._v("dodging and burning"),t("OutboundLink")],1),e._v("** to combine several negatives into one with high dynamic range Photo. With the emergence of digital cameras, HDR introduces the technology in the darkroom into the camera imaging sensor. By taking multiple different exposures of the same scene and then synthesizing them, it can finally enhance the dynamic range of the photo and achieve the purpose of increasing the layering of the photo. . In photo HDR, this compositing technique is called "),t("a",{attrs:{href:"https://en.wikipedia.org/wiki/Tone_mapping",target:"_blank",rel:"noopener noreferrer"}},[e._v("tone mapping ("),t("em",[e._v("tone mapping")]),e._v(")"),t("OutboundLink")],1),e._v(". Of course, the tone mapping technology will also be used in the next "),t("code",[e._v("video HDR")]),e._v(", the difference is that the tone mapping in "),t("code",[e._v("video HDR")]),e._v(" is used to map the HDR content to the SDR display device, so that the SDR device can also display HDR content.")]),e._v(" "),t("h2",{attrs:{id:"video-hdr"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#video-hdr"}},[e._v("#")]),e._v(" video HDR")]),e._v(" "),t("p",[e._v("HDR video is captured in a similar way to HDR photos. Record each shot simultaneously with a different exposure (or a different ISO configuration). These shots are then fused together to produce a single shot. This method often results in brighter video, although it produces an unreal-looking gray effect. Because of this, HDR videos are often post-processed before release.")]),e._v(" "),t("p",[e._v("In order to display HDR video correctly, the display device must also support HDR. For example, an HDR-capable TV must support a specific video output standard. These standards will involve: 10-bit color and DCI-P3 color space covering at least 90%. HDR monitors must also support a specific HDR format, such as HDR10, Dolby Vision, or Hybrid Log Gamma (HLG).")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://s2.loli.net/2022/09/03/zPTfvReNcH7Vb2g.jpg",alt:"color_space.jpg"}})]),e._v(" "),t("p",[e._v("Comparison of various color spaces in CIE 1931 xy")]),e._v(" "),t("p",[e._v("Take HDR10 as an example, because the HDR10 standard uses "),t("code",[e._v("Rec.2020")]),e._v(" wide color gamut, 10-bit color depth and "),t("code",[e._v("SMTPE ST 2084")]),e._v(" PQ conversion function, so when displaying HDR10 content on a non-HDR display device, it will appear Two cases [3],[4]:")]),e._v(" "),t("ul",[t("li",[e._v("Unable to decode resulting in blurred screen")]),e._v(" "),t("li",[e._v("It can be decoded, but because the highlighted part of the video will be displayed according to the highest brightness of the display device, the overall video will be grayed out")])]),e._v(" "),t("p",[e._v("This is the difference between HDR photos and HDR videos: In addition to video capture, HDR videos have a strong dependence on the capabilities of display devices.")]),e._v(" "),t("h4",{attrs:{id:"pq-hlg"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#pq-hlg"}},[e._v("#")]),e._v(" PQ & HLG")]),e._v(" "),t("p",[e._v("In order to display HDR images correctly, it is not enough to simply increase the brightness, it is crucial to display the color and tone in a way that matches human vision. Color and tone are affected by the gamma "),t("code",[e._v("input-output")]),e._v(" characteristic. In HDR, there are two gamma curves, PQ and HLG.")]),e._v(" "),t("ul",[t("li",[e._v("The PQ gamma curve is based on the characteristics of human visual perception, and is most suitable for producing movies or streaming video content on the Internet, where reproduction accuracy is key.")]),e._v(" "),t("li",[e._v("The HLG gamma curve is designed to allow display on existing SDR TVs without loss of position and is best suited for broadcast TV and live video feeds.")])]),e._v(" "),t("p",[e._v("The specific differences between PQ and HLG are shown in the following table:")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th"),e._v(" "),t("th",[e._v("PQ (Perceptual Quantization)")]),e._v(" "),t("th",[e._v("HLG (Hybrid Log Gamma)")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Goals")]),e._v(" "),t("td",[e._v("Internet Video Streaming, Movies")]),e._v(" "),t("td",[e._v("Broadcast TV, Live Video")])]),e._v(" "),t("tr",[t("td",[e._v("Advantages")]),e._v(" "),t("td",[e._v("Handles brightness up to 10,000 cd/m² in absolute value, new gamma curve based on human visual perception")]),e._v(" "),t("td",[e._v("Handles brightness as relative value (same as existing standard) Compatible with SDR TVs")])]),e._v(" "),t("tr",[t("td",[e._v("Maximum Luminance")]),e._v(" "),t("td",[e._v("1,000 cd/m² Absolute Same, regardless of Display")]),e._v(" "),t("td",[e._v("Relative, Varies by Display")])]),e._v(" "),t("tr",[t("td",[e._v("Black Level")]),e._v(" "),t("td",[e._v("0.005 cd/m2 or less")]),e._v(" "),t("td",[e._v("0.005 cd/m2 or less")])]),e._v(" "),t("tr",[t("td",[e._v("Proposer")]),e._v(" "),t("td",[e._v("Dolby")]),e._v(" "),t("td",[e._v("BBC and NHK")])]),e._v(" "),t("tr",[t("td",[e._v("Reference Standards")]),e._v(" "),t("td",[e._v("SMPTE ST 2084, ITU-R BT.2100")]),e._v(" "),t("td",[e._v("ITU-R BT.2100")])]),e._v(" "),t("tr",[t("td",[e._v("The effect on the SDR display device")]),e._v(" "),t("td",[e._v("Poor")]),e._v(" "),t("td",[e._v("Good")])]),e._v(" "),t("tr",[t("td",[e._v("Live Effect")]),e._v(" "),t("td",[e._v("Good")]),e._v(" "),t("td",[e._v("Excellent")])])])]),e._v(" "),t("h2",{attrs:{id:"hdr-standard"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#hdr-standard"}},[e._v("#")]),e._v(" HDR Standard")]),e._v(" "),t("h4",{attrs:{id:"hdr10"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#hdr10"}},[e._v("#")]),e._v(" HDR10")]),e._v(" "),t("h4",{attrs:{id:"dolby-vision"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dolby-vision"}},[e._v("#")]),e._v(" DOLBY VISION")]),e._v(" "),t("h4",{attrs:{id:"hdr10-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#hdr10-2"}},[e._v("#")]),e._v(" HDR10+")]),e._v(" "),t("h4",{attrs:{id:"uhd-premium"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#uhd-premium"}},[e._v("#")]),e._v(" UHD Premium")]),e._v(" "),t("h4",{attrs:{id:"displayhdr"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#displayhdr"}},[e._v("#")]),e._v(" DisPlayHDR")]),e._v(" "),t("h4",{attrs:{id:"mobile-hdr"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mobile-hdr"}},[e._v("#")]),e._v(" Mobile HDR")]),e._v(" "),t("h4",{attrs:{id:"cuva-hdr"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cuva-hdr"}},[e._v("#")]),e._v(" CUVA HDR")]),e._v(" "),t("h4",{attrs:{id:"vesa-hdr"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#vesa-hdr"}},[e._v("#")]),e._v(" VESA HDR")]),e._v(" "),t("h2",{attrs:{id:"mobile-phone-support-for-hdr"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mobile-phone-support-for-hdr"}},[e._v("#")]),e._v(" Mobile phone support for HDR")]),e._v(" "),t("h4",{attrs:{id:"iphone"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#iphone"}},[e._v("#")]),e._v(" iPhone")]),e._v(" "),t("p",[e._v("iPhone 12 Pro's all-round support for Dolby Vision can be said to rely on its own power to bring the most advanced and avant-garde HDR technology to the eyes of the general public. As the first mobile phone to support HDR shooting, how does iPhone 12's Dolby Vision HDR shooting come true? Is it really as good as advertised?")]),e._v(" "),t("p",[e._v("Perhaps we can learn from this expert Samuel Bilodeau, who has 5 years of HDR color correction and teaching experience.Find out in this review we wrote. The author said at the end of the review: I am particularly excited and happy to see the emergence of iPhone and other consumer cameras with HDR shooting capabilities, and push HDR a step forward.")]),e._v(" "),t("h2",{attrs:{id:"ffmpeg-analyzes-hdr-content"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ffmpeg-analyzes-hdr-content"}},[e._v("#")]),e._v(" FFmpeg analyzes HDR content")]),e._v(" "),t("h4",{attrs:{id:"ffprobe-analyzes-the-metadata-of-hdr-content"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ffprobe-analyzes-the-metadata-of-hdr-content"}},[e._v("#")]),e._v(" ffprobe analyzes the metadata of HDR content")]),e._v(" "),t("p",[e._v("Metadata for "),t("code",[e._v("Mastering Display")]),e._v(" and "),t("code",[e._v("Content Light Level")]),e._v(" can be extracted using the ffprobe command. We only need to extract the relevant data of the first frame, so when analyzing, we can use "),t("a",{attrs:{href:"http://ffmpeg.org/ffprobe.html#Main-options",target:"_blank",rel:"noopener noreferrer"}},[t("code",[e._v('-read_intervals "%+#1"')]),t("OutboundLink")],1),e._v(" option to have ffprobe only extract the metadata of the first frame. The specific analysis of fame and fortune is as follows:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('ffprobe \\\n-hide_banner\\\n-loglevel warning \\\n-select_streams v \\\n-print_format json \\\n-show_frames \\\n-read_intervals "%+#1" \\\n-show_entries "frame=color_space,color_primaries,color_transfer,side_data_list,pix_fmt" \\\nHDR.mp4\n')])])]),t("p",[e._v("The meaning of each option is as follows:")]),e._v(" "),t("ul",[t("li",[e._v("-hide_banner -loglevel warning: do not display information we do not need")]),e._v(" "),t("li",[e._v("-select_streams v: only select video streams to analyze")]),e._v(" "),t("li",[e._v("-print_format json: output analysis results in json format")]),e._v(" "),t("li",[e._v('-read_intervals "%+#1": only analyze the data in the first frame')]),e._v(" "),t("li",[e._v("-show_entries … : only output the data we specified")])]),e._v(" "),t("p",[e._v("After the command is executed, the following analysis results will be displayed:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('{\n     "frames": [\n         {\n             "pix_fmt": "yuv420p10le",\n             "color_space": "bt2020nc",\n             "color_primaries": "bt2020",\n             "color_transfer": "smpte2084",\n             "side_data_list": [\n                 {\n                     "side_data_type": "Mastering display metadata",\n                     "red_x": "34000/50000",\n                     "red_y": "16000/50000",\n                     "green_x": "13250/50000",\n                     "green_y": "34500/50000",\n                     "blue_x": "7500/50000",\n                     "blue_y": "3000/50000",\n                     "white_point_x": "15635/50000",\n                     "white_point_y": "16450/50000",\n                     "min_luminance": "40/10000",\n                     "max_luminance": "11000000/10000"\n                 }\n             ]\n         }\n     ]\n}\n')])])]),t("p",[e._v("As shown above, the color value and the maximum/minimum brightness value in the ffprobe analysis result are both a ratio. The final result of the color value will determine the color space used by the master content, and the maximum/minimum brightness value will determine the master content. Dynamic Range.")]),e._v(" "),t("p",[e._v("According to the above results, we can know that on the CIE coordinates, the coordinates of the red, green, blue and white points are:")]),e._v(" "),t("ul",[t("li",[e._v("red: (0.68, 0.32)")]),e._v(" "),t("li",[e._v("green: (0.265, 0.69)")]),e._v(" "),t("li",[e._v("blue: (0.15, 0.06)")]),e._v(" "),t("li",[e._v("white point: (0.3127, 0.329)")])]),e._v(" "),t("p",[e._v("From the relevant information of "),t("a",{attrs:{href:"https://en.wikipedia.org/wiki/DCI-P3",target:"_blank",rel:"noopener noreferrer"}},[e._v("DCI-P3"),t("OutboundLink")],1),e._v(", it can be known that the master content of this video was produced in "),t("code",[e._v("P3-D65 (Display)")]),e._v(" color Space, with a minimum brightness of 0.004 nits, a maximum brightness of 1100 nits, and a contrast ratio of 275,000.")]),e._v(" "),t("h4",{attrs:{id:"ffmpeg-transcoding-hdr-content"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ffmpeg-transcoding-hdr-content"}},[e._v("#")]),e._v(" FFmpeg transcoding HDR content")]),e._v(" "),t("p",[e._v("For the transcoding of HDR video, the content of "),t("code",[e._v("Mastering Display")]),e._v(" and "),t("code",[e._v("Content Light Level")]),e._v(" metadata needs to be passed to the encoder (note, not to FFmpeg), otherwise, these information will be lost during the transcoding process, This in turn leads to the impact of transcoding on the content of the screen. The specific way is as follows:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("ffmpeg -i hdr.mp4 \\\n-map 0 -c:v libx265\\\n-x265-params hdr-opt=1:repeat-headers=1:colorprim=bt2020:transfer=smpte2084:colormatrix=bt2020nc:master-display=G(13250,34500)B(7500,3000)R(34000,16000) WP(15635,16450)L(11000000,40)\\\n-crf 20 \\\n-preset veryfast \\\n-pix_fmt yuv420p10le\\\ntest.mp4\n")])])]),t("p",[e._v("in,")]),e._v(" "),t("ul",[t("li",[e._v("hdr-opt=1: means we want to enable HDR")]),e._v(" "),t("li",[e._v("repeat-headers=1: Indicates that each frame needs these data")]),e._v(" "),t("li",[e._v("colorprim, transfer and colormatrix: consistent with ffprobe")]),e._v(" "),t("li",[e._v("master-display: The color string constructed according to the result of ffprobe")])]),e._v(" "),t("h2",{attrs:{id:"related-terms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#related-terms"}},[e._v("#")]),e._v(" Related terms")]),e._v(" "),t("h4",{attrs:{id:"color-calibration-of-screens-screen-color-correction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#color-calibration-of-screens-screen-color-correction"}},[e._v("#")]),e._v(" Color Calibration of Screens (screen color correction)")]),e._v(" "),t("p",[e._v("A process used to ensure that colors are accurately displayed on the screen. A colorimeter is generally used to measure the native color response of a display, then calculate an index for correction to ensure that the color is displayed correctly on the display, and finally test the corrected response.")]),e._v(" "),t("h4",{attrs:{id:"color-spaces"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#color-spaces"}},[e._v("#")]),e._v(" Color Spaces")]),e._v(" "),t("p",[e._v("A color space can refer to an organization of colors, or it can refer to a specific range of colors. In the field of cinema and television, we generally use RGB (using red, green, and blue primary color components to represent a color) or YUV (using the brightness (black and white value) of a color, and its color value calculated based on the difference between color components. degrees) color space (Translator's Note: RGB and YUV are color models, not color spaces, there is an error in the original text here). These color spaces are generally based on specific display device characteristics, see D65-P3 entry. Other color spaces, such as XYZ or Lab, are more suitable for representing the human eye color vision model.")]),e._v(" "),t("h4",{attrs:{id:"contrast-ratio"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#contrast-ratio"}},[e._v("#")]),e._v(" Contrast Ratio")]),e._v(" "),t("p",[e._v("Contrast is the ratio of lightness between the lightest (white) part and the darkest (black) part that a system can produce, usually described by an n:1 ratio.")]),e._v(" "),t("h4",{attrs:{id:"cri-color-remapping-information-color-remapping-information"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cri-color-remapping-information-color-remapping-information"}},[e._v("#")]),e._v(" CRI Color Remapping Information (color remapping information)")]),e._v(" "),t("p",[e._v("A standard set of metadata produced by analyzing two different masters of the same content, such as an HDR and an SDR master. When a master (such as HDR) and CRI metadata are transmitted together, the decoder can only decode HDR content when facing an HDR screen, and can also convert HDR content when facing an SDR screen by using CRI metadata. The content is transformed into SDR content. The main advantage of using this approach is that the author's creative intent is preserved for both decoded versions. CRI is a standard component of MPEG (HEVC v2) and is an optional feature for Ultra HD Blu-ray production.")]),e._v(" "),t("h4",{attrs:{id:"dci-p3-d65-p3-st-428-1"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dci-p3-d65-p3-st-428-1"}},[e._v("#")]),e._v(" DCI-P3, D65-P3, ST 428-1")]),e._v(" "),t("p",[e._v("A digital cinema color space. The DCI-P3 color space is an RGB color space introduced by the Digital Cinema Initiatives in 2005 and standardized by SMPTE ST428-1 in 2006.")]),e._v(" "),t("p",[e._v("This color space has a much wider gamut than sRGB (see Rec.709).")]),e._v(" "),t("p",[e._v("All digital cinema projectors can fully display the DCI-P3 color space, D65 P3 refers to the color temperature of its white point changed from DCI white point to D65 white point.")]),e._v(" "),t("p",[e._v("The three triangles in the diagram represent: the largest is the Rec.2020 standard, the new standard for Ultra HD TV (currently only fully implemented by laser displays), the slightly smaller DCI-P3 for digital cinema, and traditional video surveillance The smallest Rec.709 color space used by TV receivers, HD broadcast TV, Blu-ray, OTT.")]),e._v(" "),t("h4",{attrs:{id:"edid-extended-display-identification-data"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#edid-extended-display-identification-data"}},[e._v("#")]),e._v(" EDID Extended Display Identification Data")]),e._v(" "),t("p",[e._v("EDID is an acronym for Extended Display Information Data, a standard developed by the Consumer Technology Association (CTA). This is provided by every DVI monitor, HDMI monitor, or other device that supports a DVI HDMI input (aka DVI/HDMI Sinks). Probably each DVI/HDMI input has its own EDID. EDID tells the device the performance characteristics of the display it is connected to.")]),e._v(" "),t("p",[e._v("The source device recognizes the presence of EDID memory on the display's DVI or HDMI interface and uses the information in it to optimize the output video (resolution, frame rate, color...) and/or audio format. All devices that support DVI/HDMI standard input, that is, TMDS sinks (DVI/HDMI Sinks) must implement EDID.")]),e._v(" "),t("h4",{attrs:{id:"eotf-electro-optic-transfer-function"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#eotf-electro-optic-transfer-function"}},[e._v("#")]),e._v(" EOTF Electro-optic transfer function")]),e._v(" "),t("p",[e._v("EOTF is an acronym for Electro-Optical Transfer Function. It is a mathematical function that maps code values to display brightness. In other words, an ETOF defines how code values in an image are displayed as visible light by a display or projector.")]),e._v(" "),t("p",[e._v("See OETF Photoelectric Transfer Function, ST2084.")]),e._v(" "),t("h4",{attrs:{id:"oetf-photoelectric-transfer-function"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#oetf-photoelectric-transfer-function"}},[e._v("#")]),e._v(" OETF Photoelectric Transfer Function")]),e._v(" "),t("p",[e._v("OETF is an acronym for Opto-Electronic Transfer Function. It is a mathematical function that maps scene luminosity (the light in a scene) to digitally encoded values that can be transmitted and compressed. This term is generally applied to devices that acquire images, such as digital cameras.")]),e._v(" "),t("p",[e._v("In post-production, content is typically graded on a screen with a specific EOTF, and historically, an inverse of the OETF was often used as the screen's EOTF.")]),e._v(" "),t("h4",{attrs:{id:"ootf-optical-to-optical-transfer-function"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ootf-optical-to-optical-transfer-function"}},[e._v("#")]),e._v(" OOTF Optical-to-optical transfer function")]),e._v(" "),t("p",[e._v("OOTF is an acronym for Optical-to-Optical Transfer Function. It is a mathematical function that maps the brightness of the scene captured by the camera to the brightness of the display.")]),e._v(" "),t("h4",{attrs:{id:"flicker-flicker"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flicker-flicker"}},[e._v("#")]),e._v(" Flicker Flicker")]),e._v(" "),t("p",[e._v("Characteristic of certain kinds of monitors, such as old cathode ray tube monitors (CRTs), or poorly tuned flat panel monitors, or even motion picture film projectors. This unwelcome change in brightness occurs mainly at frequencies below 50 Visible in frames per second. For brighter displays, the human eye can perceive higher frequency flicker.")]),e._v(" "),t("h4",{attrs:{id:"f-stop-of-dynamic-range-the-number-of-f-stops-of-the-dynamic-range"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#f-stop-of-dynamic-range-the-number-of-f-stops-of-the-dynamic-range"}},[e._v("#")]),e._v(" f-stop of Dynamic Range The number of f-stops of the dynamic range")]),e._v(" "),t("p",[e._v("In photography, a change of one f-stop corresponds to doubling (or halving) the light captured when capturing an image.")]),e._v(" "),t("p",[e._v("The number of f-stops contained in an image describes the contrast of the image (2^N notation). For example, a camera can output an image of 10 stops, which means that the contrast ratio can be as high as 2^10 (1024:1 ), that is, the white part will be 1024 times brighter than the black part. In comparison, the human eye can go up to 18-20 stops (this is a very high dynamic range HDR, a typical SDR video image is 6-7 stops)")]),e._v(" "),t("h4",{attrs:{id:"gamut-or-color-gamut"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gamut-or-color-gamut"}},[e._v("#")]),e._v(" Gamut or Color Gamut")]),e._v(" "),t("p",[e._v("In color reproduction, including but not limited to computer graphics and photography, a color gamut is a specific subset of colors.")]),e._v(" "),t("p",[e._v("color gamutThe most common use of the term is to refer to a subset of colors that can be accurately reflected within a certain range. This premise may be a specific display device or a given color space. The color gamut is generally represented by the area on the CIE 1931 chromaticity diagram, and the edges of the CIE 1931 curve represent the range of colors in the visible light spectrum.")]),e._v(" "),t("h4",{attrs:{id:"gamut-mapping-gamut-mapping"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gamut-mapping-gamut-mapping"}},[e._v("#")]),e._v(" Gamut Mapping Gamut Mapping")]),e._v(" "),t("p",[e._v("In almost all translation processes (referring to the representation of a specific color, the conversion process in different color spaces), we have to face the reality that the range covered by the color gamut of different devices is different, which means Making accurate color reproduction impossible.")]),e._v(" "),t("p",[e._v("Therefore, it is necessary to perform some processing on the parts near the edge of the color gamut. Some colors must be shifted into gamut, otherwise they cannot be displayed on the output device, and they will be roughly discarded (clipped). This is known as a gamut mismatch, for example when we convert from the wider RGB color space to the CMYK color space.")]),e._v(" "),t("p",[e._v("A color management system can use a variety of methods to achieve the desired result and give the experienced user the means to control gamut mapping.")]),e._v(" "),t("h4",{attrs:{id:"imf-interactive-master-file"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#imf-interactive-master-file"}},[e._v("#")]),e._v(" IMF Interactive Master File")]),e._v(" "),t("p",[e._v("The Interoperable Master Format (IMF) is a standard provided by SMPTE to implement a single-file, interchangeable master file format and structure for commercial content distribution worldwide. It evolved from the Digital Cinema Package (DCP) architecture, which provides a complete exchangeable unit of files for distribution channels. IMF can be said to be a revolutionary progress, IMF provides a real file-based final master. DCP is aimed at theatrical content distribution, while IMF provides a master format for commercial environments that can create multiple cut versions of the same content for different audiences.")]),e._v(" "),t("h4",{attrs:{id:"inverse-tone-mapping-itm-inverse-tone-mapping-reverse-tone-mapping"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#inverse-tone-mapping-itm-inverse-tone-mapping-reverse-tone-mapping"}},[e._v("#")]),e._v(" Inverse Tone Mapping (ITM) Inverse tone mapping / reverse tone mapping")]),e._v(" "),t("p",[e._v("Remastering of SDR content for HDR content.")]),e._v(" "),t("p",[e._v("ITM takes SDR content and expands its luminosity and color space to match the capabilities of HDR displays while preserving the original content creator's intent.")]),e._v(" "),t("h4",{attrs:{id:"lut-lookup-table"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#lut-lookup-table"}},[e._v("#")]),e._v(" LUT lookup table")]),e._v(" "),t("p",[e._v("It is an acronym for Look Up Table. LUTs provide an efficient way of applying complex mathematical operations to incoming data—which would be computationally expensive without LUTs. As such, they are an ideal implementation for mapping an image from one color space to zero one color space.")]),e._v(" "),t("p",[e._v("The following LUTs exist:")]),e._v(" "),t("p",[e._v("In 3D LUTs, the output R', G', B' value of each pixel is jointly calculated from the R, G, B value of the input pixel.")]),e._v(" "),t("p",[e._v("For 1D LUTs, the output R’ is only calculated from the input R value, and the same is true for G’ and B’. They are commonly used to apply corrections for gamma functions and other EOTFs, and are typically found in chipsets in consumer electronics devices.")]),e._v(" "),t("p",[e._v('3D LUTs support more powerful mathematical transformations than 1D LUTs, but are more complex and expensive to implement in chipsets. Typically 3D LUTs are used to apply a creative "look" and color space conversion during post-production.')]),e._v(" "),t("h4",{attrs:{id:"maxcll-metadata-maxcll-metadata"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#maxcll-metadata-maxcll-metadata"}},[e._v("#")]),e._v(" MaxCLL Metadata MaxCLL Metadata")]),e._v(" "),t("p",[e._v("Maximum Content Light Level (MaxCLL) is an integer-type metadata value used to define the highest brightness of any pixel in an encoded HDR video stream or file, in units of nits. MaxCLL can be measured during the mastering process or after the production process, but in order to ensure the color transition of HDR monitors in the MaxCLL HDR range, and to apply a hard clip outside the maximum brightness of the monitor, the MaxCLL of the monitor can also be used as the MaxCLL element data.")]),e._v(" "),t("h4",{attrs:{id:"maxfall-metadata-maxfall-metadata"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#maxfall-metadata-maxfall-metadata"}},[e._v("#")]),e._v(" MaxFALL Metadata MaxFALL Metadata")]),e._v(" "),t("p",[e._v("Maximum Frame Average Light Level (MaxFALL) is an integer-type metadata value used to define the highest average brightness of any frame in an encoded HDR video stream or file, in units of nits. MaxFALL is obtained by calculating the average brightness of all pixels decoded in each frame.")]),e._v(" "),t("h4",{attrs:{id:"peak-code-value-peak-code-value"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#peak-code-value-peak-code-value"}},[e._v("#")]),e._v(" Peak Code Value Peak code value")]),e._v(" "),t("p",[e._v("Peak Code Value refers to the maximum digital code value that passes through a system component without generating a clip.")]),e._v(" "),t("h4",{attrs:{id:"peak-display-luminance-peak-display-brightness"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#peak-display-luminance-peak-display-brightness"}},[e._v("#")]),e._v(" Peak Display Luminance peak display brightness")]),e._v(" "),t("p",[e._v("The highest brightness a display can produce.")]),e._v(" "),t("h4",{attrs:{id:"perceptual-quantizer-sensory-quantization-curve"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#perceptual-quantizer-sensory-quantization-curve"}},[e._v("#")]),e._v(" Perceptual Quantizer sensory quantization curve")]),e._v(" "),t("p",[e._v("Abbreviation for Perceptual Quantizer, which is an EOTF.")]),e._v(" "),t("p",[e._v("MovieLabs proposed a mathematical curve based on Barton Curve that can be used for high dynamic range (HDR). In 2014, SMPTE established the ST2084 standard.")]),e._v(" "),t("p",[e._v("Perceptual Quantization is an efficient way to encode HDR brightness information. Throughout the dynamic range, the difference between each pair of adjacent code values is slightly smaller than the perceivable difference, making the code value utilization rate extremely high.")]),e._v(" "),t("p",[e._v("However, this EOTF is not compatible with legacy displays, and PQ-encoded signals can only be decoded on newer, HDR-capable devices.")]),e._v(" "),t("p",[e._v("PQ is designed for 10bit or 12bit content, and according to SMPTE ST2084 standard, it is not recommended to use it for live broadcast.")]),e._v(" "),t("h4",{attrs:{id:"quantum-dot-qd-displays"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#quantum-dot-qd-displays"}},[e._v("#")]),e._v(" Quantum Dot (QD) Displays")]),e._v(" "),t("p",[e._v('Quantum dot displays use nanoscale (2nm-10nm) crystalline "dots". Each dot emits a different solid color, and the color they emit depends on its size.')]),e._v(" "),t("p",[e._v("By adding a film with quantum dots in front of the LCD backlight, the color reproduction and overall brightness of the image can be significantly improved.")]),e._v(" "),t("p",[e._v("These tiny nanocrystals can change the spectrum of the backlight before it reaches the red, green, and blue sub-pixels, achieving a color gamut improvement of up to 20-30%, making the color of the display device closer to the Rec.2020 target color gamut.")]),e._v(" "),t("h4",{attrs:{id:"st2084"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#st2084"}},[e._v("#")]),e._v(" ST2084")]),e._v(" "),t("p",[e._v("EOTF defined in the SMPTE standard for mastering HDR content. Also known as PQ, this EOTF is primarily used for mastering non-broadcast content.")]),e._v(" "),t("h4",{attrs:{id:"st2086"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#st2086"}},[e._v("#")]),e._v(" ST2086")]),e._v(" "),t("p",[e._v("Metadata (primary color coordinates, white point, luminance range) defined in the SMPTE standard to describe the absolute color space of the display used for video mastering.")]),e._v(" "),t("h4",{attrs:{id:"tone-mapping-tone-mapping-operator-tmo-tone-mapping-tone-mapping-operator"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#tone-mapping-tone-mapping-operator-tmo-tone-mapping-tone-mapping-operator"}},[e._v("#")]),e._v(" Tone Mapping/Tone Mapping Operator (TMO) tone mapping/tone mapping operator")]),e._v(" "),t("p",[e._v('Tone mapping is a technique used in image processing and computer graphics that is used to map one set of colors to another to achieve an approximation of a High Dynamic Range (HDR) image in a medium with a more limited dynamic range perception. Printouts, CRT or standard dynamic range (SDR) monitors, and projectors all have low dynamic range, not enough to reproduce the full light intensity present in an HDR image. Tone mapping resolves the dramatic drop in contrast from the recording range to the displayable range, while preserving image detail and color representation, which is important for enjoying original scene content and preserving creative intent. This tone mapping process is generally performed using a Tone Mapping Operator, usually an "S" shaped curve to achieve a soft transition of highlight and shadow detail. See Inverse Tone Mapping (ITM).')]),e._v(" "),t("h4",{attrs:{id:"wide-color-gamut-wcg-wide-color-gamut"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#wide-color-gamut-wcg-wide-color-gamut"}},[e._v("#")]),e._v(" Wide Color Gamut (WCG) wide color gamut")]),e._v(" "),t("p",[e._v("Abbreviation for Wide Color Gamut, which means wide color gamut. The wide color gamut includes more saturated colors than Recommendation ITU-R BT.709. For example, the color space defined by Rec.2020 belongs to the wide color gamut.")]),e._v(" "),t("h4",{attrs:{id:"white-point-white-point"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#white-point-white-point"}},[e._v("#")]),e._v(" White Point White Point")]),e._v(" "),t("p",[e._v('The white point (often referred to in technical documentation as reference white or target white) is a set of chromaticity coordinates used to define the "white" color in image acquisition, encoding or reproduction. Depending on the application, different definitions of "white" are required to provide acceptable results. For example, a photo taken indoors might be lit with incandescent lighting, which is relatively more orange than daylight. Therefore, most professional cameras have different settings for shooting under incandescent lighting versus daylight. Likewise, images produced for a D65 white point monitor will not display correctly on a monitor with a different white point.')]),e._v(" "),t("p",[e._v("CIE Definition D65 is often used to define the white point of video displays.")]),e._v(" "),t("p",[e._v("D55 was once the standard white point for film projection, and both the DCI white point and the D60 white point are widely used by digital cinema.")])])}),[],!1,null,null,null);t.default=i.exports}}]);