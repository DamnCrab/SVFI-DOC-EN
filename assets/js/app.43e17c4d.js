(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(e){function t(t){for(var o,a,s=t[0],l=t[1],c=t[2],u=0,p=[];u<s.length;u++)a=s[u],Object.prototype.hasOwnProperty.call(i,a)&&i[a]&&p.push(i[a][0]),i[a]=0;for(o in l)Object.prototype.hasOwnProperty.call(l,o)&&(e[o]=l[o]);for(d&&d(t);p.length;)p.shift()();return r.push.apply(r,c||[]),n()}function n(){for(var e,t=0;t<r.length;t++){for(var n=r[t],o=!0,s=1;s<n.length;s++){var l=n[s];0!==i[l]&&(o=!1)}o&&(r.splice(t--,1),e=a(a.s=n[0]))}return e}var o={},i={1:0},r=[];function a(t){if(o[t])return o[t].exports;var n=o[t]={i:t,l:!1,exports:{}};return e[t].call(n.exports,n,n.exports,a),n.l=!0,n.exports}a.e=function(e){var t=[],n=i[e];if(0!==n)if(n)t.push(n[2]);else{var o=new Promise((function(t,o){n=i[e]=[t,o]}));t.push(n[2]=o);var r,s=document.createElement("script");s.charset="utf-8",s.timeout=120,a.nc&&s.setAttribute("nonce",a.nc),s.src=function(e){return a.p+"assets/js/"+({}[e]||e)+"."+{2:"77811d9b",3:"2974fd54",4:"ed498d97",5:"00489729",6:"22336278",7:"289125a0",8:"1748afa9",9:"c2e9d181",10:"bbcbd3a2",11:"7a1a1c82",12:"b78a184b",13:"7620b6fa",14:"cb19f62f",15:"aafd71d0",16:"bc63a940",17:"81670ee6",18:"b6783edf",19:"e6db922b",20:"6ee99b42"}[e]+".js"}(e);var l=new Error;r=function(t){s.onerror=s.onload=null,clearTimeout(c);var n=i[e];if(0!==n){if(n){var o=t&&("load"===t.type?"missing":t.type),r=t&&t.target&&t.target.src;l.message="Loading chunk "+e+" failed.\n("+o+": "+r+")",l.name="ChunkLoadError",l.type=o,l.request=r,n[1](l)}i[e]=void 0}};var c=setTimeout((function(){r({type:"timeout",target:s})}),12e4);s.onerror=s.onload=r,document.head.appendChild(s)}return Promise.all(t)},a.m=e,a.c=o,a.d=function(e,t,n){a.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:n})},a.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},a.t=function(e,t){if(1&t&&(e=a(e)),8&t)return e;if(4&t&&"object"==typeof e&&e&&e.__esModule)return e;var n=Object.create(null);if(a.r(n),Object.defineProperty(n,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var o in e)a.d(n,o,function(t){return e[t]}.bind(null,o));return n},a.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return a.d(t,"a",t),t},a.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},a.p="/",a.oe=function(e){throw console.error(e),e};var s=window.webpackJsonp=window.webpackJsonp||[],l=s.push.bind(s);s.push=t,s=s.slice();for(var c=0;c<s.length;c++)t(s[c]);var d=l;r.push([104,0]),n()}([function(e,t,n){var o=n(55),i=o.all;e.exports=o.IS_HTMLDDA?function(e){return"function"==typeof e||e===i}:function(e){return"function"==typeof e}},function(e,t,n){var o=n(27),i=Function.prototype,r=i.call,a=o&&i.bind.bind(r,r);e.exports=o?a:function(e){return function(){return r.apply(e,arguments)}}},function(e,t){var n=function(e){return e&&e.Math==Math&&e};e.exports=n("object"==typeof globalThis&&globalThis)||n("object"==typeof window&&window)||n("object"==typeof self&&self)||n("object"==typeof global&&global)||function(){return this}()||this||Function("return this")()},function(e,t){e.exports=function(e){try{return!!e()}catch(e){return!0}}},function(e,t,n){var o=n(3);e.exports=!o((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(e,t){var n=Array.isArray;e.exports=n},function(e,t,n){var o=n(69),i="object"==typeof self&&self&&self.Object===Object&&self,r=o||i||Function("return this")();e.exports=r},function(e,t,n){"use strict";function o(e,t,n,o,i,r,a,s){var l,c="function"==typeof e?e.options:e;if(t&&(c.render=t,c.staticRenderFns=n,c._compiled=!0),o&&(c.functional=!0),r&&(c._scopeId="data-v-"+r),a?(l=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),i&&i.call(this,e),e&&e._registeredComponents&&e._registeredComponents.add(a)},c._ssrRegister=l):i&&(l=s?function(){i.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:i),l)if(c.functional){c._injectStyles=l;var d=c.render;c.render=function(e,t){return l.call(t),d(e,t)}}else{var u=c.beforeCreate;c.beforeCreate=u?[].concat(u,l):[l]}return{exports:e,options:c}}n.d(t,"a",(function(){return o}))},function(e,t,n){var o=n(1),i=n(32),r=o({}.hasOwnProperty);e.exports=Object.hasOwn||function(e,t){return r(i(e),t)}},function(e,t,n){var o=n(0),i=n(55),r=i.all;e.exports=i.IS_HTMLDDA?function(e){return"object"==typeof e?null!==e:o(e)||e===r}:function(e){return"object"==typeof e?null!==e:o(e)}},function(e,t,n){var o=n(164),i=n(167);e.exports=function(e,t){var n=i(e,t);return o(n)?n:void 0}},function(e,t,n){"use strict";n.d(t,"e",(function(){return o})),n.d(t,"b",(function(){return r})),n.d(t,"j",(function(){return a})),n.d(t,"g",(function(){return l})),n.d(t,"h",(function(){return c})),n.d(t,"i",(function(){return d})),n.d(t,"c",(function(){return u})),n.d(t,"f",(function(){return p})),n.d(t,"l",(function(){return h})),n.d(t,"m",(function(){return f})),n.d(t,"d",(function(){return g})),n.d(t,"k",(function(){return v})),n.d(t,"n",(function(){return y})),n.d(t,"a",(function(){return w}));n(13);const o=/#.*$/,i=/\.(md|html)$/,r=/\/$/,a=/^[a-z]+:/i;function s(e){return decodeURI(e).replace(o,"").replace(i,"")}function l(e){return a.test(e)}function c(e){return/^mailto:/.test(e)}function d(e){return/^tel:/.test(e)}function u(e){if(l(e))return e;if(!e)return"404";const t=e.match(o),n=t?t[0]:"",i=s(e);return r.test(i)?e:i+".html"+n}function p(e,t){const n=e.hash,i=function(e){const t=e&&e.match(o);if(t)return t[0]}(t);if(i&&n!==i)return!1;return s(e.path)===s(t)}function h(e,t,n){if(l(t))return{type:"external",path:t};n&&(t=function(e,t,n){const o=e.charAt(0);if("/"===o)return e;if("?"===o||"#"===o)return t+e;const i=t.split("/");n&&i[i.length-1]||i.pop();const r=e.replace(/^\//,"").split("/");for(let e=0;e<r.length;e++){const t=r[e];".."===t?i.pop():"."!==t&&i.push(t)}""!==i[0]&&i.unshift("");return i.join("/")}(t,n));const o=s(t);for(let t=0;t<e.length;t++)if(s(e[t].regularPath)===o)return Object.assign({},e[t],{type:"page",path:u(e[t].path)});return console.error(`[vuepress] No matching page found for sidebar item "${t}"`),{}}function f(e,t,n,o){const{pages:i,themeConfig:r}=n,a=o&&r.locales&&r.locales[o]||r;if("auto"===(e.frontmatter.sidebar||a.sidebar||r.sidebar))return m(e);const s=a.sidebar||r.sidebar;if(s){const{base:n,config:o}=function(e,t){if(Array.isArray(t))return{base:"/",config:t};for(const o in t)if(0===(n=e,/(\.html|\/)$/.test(n)?n:n+"/").indexOf(encodeURI(o)))return{base:o,config:t[o]};var n;return{}}(t,s);return"auto"===o?m(e):o?o.map(e=>function e(t,n,o,i=1){if("string"==typeof t)return h(n,t,o);if(Array.isArray(t))return Object.assign(h(n,t[0],o),{title:t[1]});{i>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const r=t.children||[];return 0===r.length&&t.path?Object.assign(h(n,t.path,o),{title:t.title}):{type:"group",path:t.path,title:t.title,sidebarDepth:t.sidebarDepth,initialOpenGroupIndex:t.initialOpenGroupIndex,children:r.map(t=>e(t,n,o,i+1)),collapsable:!1!==t.collapsable}}}(e,i,n)):[]}return[]}function m(e){const t=g(e.headers||[]);return[{type:"group",collapsable:!1,title:e.title,path:null,children:t.map(t=>({type:"auto",title:t.title,basePath:e.path,path:e.path+"#"+t.slug,children:t.children||[]}))}]}function g(e){let t;return(e=e.map(e=>Object.assign({},e))).forEach(e=>{2===e.level?t=e:t&&(t.children||(t.children=[])).push(e)}),e.filter(e=>2===e.level)}function v(e){return Object.assign(e,{type:e.items&&e.items.length?"links":"link"})}function y(e){return Object.prototype.toString.call(e).match(/\[object (.*?)\]/)[1].toLowerCase()}function b(e){let t=e.frontmatter.date||e.lastUpdated||new Date,n=new Date(t);return"Invalid Date"==n&&t&&(n=new Date(t.replace(/-/g,"/"))),n.getTime()}function w(e,t){return b(t)-b(e)}},function(e,t){e.exports=function(e){return null!=e&&"object"==typeof e}},function(e,t,n){"use strict";var o=n(26),i=n(32),r=n(33),a=n(128),s=n(130);o({target:"Array",proto:!0,arity:1,forced:n(3)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(e){return e instanceof TypeError}}()},{push:function(e){var t=i(this),n=r(t),o=arguments.length;s(n+o);for(var l=0;l<o;l++)t[n]=arguments[l],n++;return a(t,n),n}})},function(e,t,n){var o=n(16),i=n(149),r=n(150),a=o?o.toStringTag:void 0;e.exports=function(e){return null==e?void 0===e?"[object Undefined]":"[object Null]":a&&a in Object(e)?i(e):r(e)}},function(e,t,n){var o=n(4),i=n(17),r=n(35);e.exports=o?function(e,t,n){return i.f(e,t,r(1,n))}:function(e,t,n){return e[t]=n,e}},function(e,t,n){var o=n(6).Symbol;e.exports=o},function(e,t,n){var o=n(4),i=n(64),r=n(99),a=n(25),s=n(54),l=TypeError,c=Object.defineProperty,d=Object.getOwnPropertyDescriptor;t.f=o?r?function(e,t,n){if(a(e),t=s(t),a(n),"function"==typeof e&&"prototype"===t&&"value"in n&&"writable"in n&&!n.writable){var o=d(e,t);o&&o.writable&&(e[t]=n.value,n={configurable:"configurable"in n?n.configurable:o.configurable,enumerable:"enumerable"in n?n.enumerable:o.enumerable,writable:!1})}return c(e,t,n)}:c:function(e,t,n){if(a(e),t=s(t),a(n),i)try{return c(e,t,n)}catch(e){}if("get"in n||"set"in n)throw l("Accessors not supported");return"value"in n&&(e[t]=n.value),e}},function(e,t,n){var o=n(1),i=o({}.toString),r=o("".slice);e.exports=function(e){return r(i(e),8,-1)}},function(e,t,n){var o=n(154),i=n(155),r=n(156),a=n(157),s=n(158);function l(e){var t=-1,n=null==e?0:e.length;for(this.clear();++t<n;){var o=e[t];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=a,l.prototype.set=s,e.exports=l},function(e,t,n){var o=n(71);e.exports=function(e,t){for(var n=e.length;n--;)if(o(e[n][0],t))return n;return-1}},function(e,t,n){var o=n(10)(Object,"create");e.exports=o},function(e,t,n){var o=n(176);e.exports=function(e,t){var n=e.__data__;return o(t)?n["string"==typeof t?"string":"hash"]:n.map}},function(e,t,n){var o=n(45);e.exports=function(e){if("string"==typeof e||o(e))return e;var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(e,t,n){var o,i;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(i="function"==typeof(o=function(){var e,t,n={version:"0.2.0"},o=n.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function i(e,t,n){return e<t?t:e>n?n:e}function r(e){return 100*(-1+e)}n.configure=function(e){var t,n;for(t in e)void 0!==(n=e[t])&&e.hasOwnProperty(t)&&(o[t]=n);return this},n.status=null,n.set=function(e){var t=n.isStarted();e=i(e,o.minimum,1),n.status=1===e?null:e;var l=n.render(!t),c=l.querySelector(o.barSelector),d=o.speed,u=o.easing;return l.offsetWidth,a((function(t){""===o.positionUsing&&(o.positionUsing=n.getPositioningCSS()),s(c,function(e,t,n){var i;return(i="translate3d"===o.positionUsing?{transform:"translate3d("+r(e)+"%,0,0)"}:"translate"===o.positionUsing?{transform:"translate("+r(e)+"%,0)"}:{"margin-left":r(e)+"%"}).transition="all "+t+"ms "+n,i}(e,d,u)),1===e?(s(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){s(l,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){n.remove(),t()}),d)}),d)):setTimeout(t,d)})),this},n.isStarted=function(){return"number"==typeof n.status},n.start=function(){n.status||n.set(0);var e=function(){setTimeout((function(){n.status&&(n.trickle(),e())}),o.trickleSpeed)};return o.trickle&&e(),this},n.done=function(e){return e||n.status?n.inc(.3+.5*Math.random()).set(1):this},n.inc=function(e){var t=n.status;return t?("number"!=typeof e&&(e=(1-t)*i(Math.random()*t,.1,.95)),t=i(t+e,0,.994),n.set(t)):n.start()},n.trickle=function(){return n.inc(Math.random()*o.trickleRate)},e=0,t=0,n.promise=function(o){return o&&"resolved"!==o.state()?(0===t&&n.start(),e++,t++,o.always((function(){0==--t?(e=0,n.done()):n.set((e-t)/e)})),this):this},n.render=function(e){if(n.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var t=document.createElement("div");t.id="nprogress",t.innerHTML=o.template;var i,a=t.querySelector(o.barSelector),l=e?"-100":r(n.status||0),d=document.querySelector(o.parent);return s(a,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),o.showSpinner||(i=t.querySelector(o.spinnerSelector))&&p(i),d!=document.body&&c(d,"nprogress-custom-parent"),d.appendChild(t),t},n.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(o.parent),"nprogress-custom-parent");var e=document.getElementById("nprogress");e&&p(e)},n.isRendered=function(){return!!document.getElementById("nprogress")},n.getPositioningCSS=function(){var e=document.body.style,t="WebkitTransform"in e?"Webkit":"MozTransform"in e?"Moz":"msTransform"in e?"ms":"OTransform"in e?"O":"";return t+"Perspective"in e?"translate3d":t+"Transform"in e?"translate":"margin"};var a=function(){var e=[];function t(){var n=e.shift();n&&n(t)}return function(n){e.push(n),1==e.length&&t()}}(),s=function(){var e=["Webkit","O","Moz","ms"],t={};function n(n){return n=n.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(e,t){return t.toUpperCase()})),t[n]||(t[n]=function(t){var n=document.body.style;if(t in n)return t;for(var o,i=e.length,r=t.charAt(0).toUpperCase()+t.slice(1);i--;)if((o=e[i]+r)in n)return o;return t}(n))}function o(e,t,o){t=n(t),e.style[t]=o}return function(e,t){var n,i,r=arguments;if(2==r.length)for(n in t)void 0!==(i=t[n])&&t.hasOwnProperty(n)&&o(e,n,i);else o(e,r[1],r[2])}}();function l(e,t){return("string"==typeof e?e:u(e)).indexOf(" "+t+" ")>=0}function c(e,t){var n=u(e),o=n+t;l(n,t)||(e.className=o.substring(1))}function d(e,t){var n,o=u(e);l(e,t)&&(n=o.replace(" "+t+" "," "),e.className=n.substring(1,n.length-1))}function u(e){return(" "+(e.className||"")+" ").replace(/\s+/gi," ")}function p(e){e&&e.parentNode&&e.parentNode.removeChild(e)}return n})?o.call(t,n,t,e):o)||(e.exports=i)},function(e,t,n){var o=n(9),i=String,r=TypeError;e.exports=function(e){if(o(e))return e;throw r(i(e)+" is not an object")}},function(e,t,n){var o=n(2),i=n(51).f,r=n(15),a=n(112),s=n(37),l=n(65),c=n(124);e.exports=function(e,t){var n,d,u,p,h,f=e.target,m=e.global,g=e.stat;if(n=m?o:g?o[f]||s(f,{}):(o[f]||{}).prototype)for(d in t){if(p=t[d],u=e.dontCallGetSet?(h=i(n,d))&&h.value:n[d],!c(m?d:f+(g?".":"#")+d,e.forced)&&void 0!==u){if(typeof p==typeof u)continue;l(p,u)}(e.sham||u&&u.sham)&&r(p,"sham",!0),a(n,d,p,e)}}},function(e,t,n){var o=n(3);e.exports=!o((function(){var e=function(){}.bind();return"function"!=typeof e||e.hasOwnProperty("prototype")}))},function(e,t,n){var o=n(47),i=n(52);e.exports=function(e){return o(i(e))}},function(e,t,n){var o=n(2),i=n(0),r=function(e){return i(e)?e:void 0};e.exports=function(e,t){return arguments.length<2?r(o[e]):o[e]&&o[e][t]}},function(e,t,n){var o=n(0),i=n(110),r=TypeError;e.exports=function(e){if(o(e))return e;throw r(i(e)+" is not a function")}},function(e,t,n){var o=n(2),i=n(61),r=n(8),a=n(63),s=n(59),l=n(58),c=o.Symbol,d=i("wks"),u=l?c.for||c:c&&c.withoutSetter||a;e.exports=function(e){return r(d,e)||(d[e]=s&&r(c,e)?c[e]:u("Symbol."+e)),d[e]}},function(e,t,n){var o=n(52),i=Object;e.exports=function(e){return i(o(e))}},function(e,t,n){var o=n(122);e.exports=function(e){return o(e.length)}},function(e,t,n){var o=n(27),i=Function.prototype.call;e.exports=o?i.bind(i):function(){return i.apply(i,arguments)}},function(e,t){e.exports=function(e,t){return{enumerable:!(1&e),configurable:!(2&e),writable:!(4&e),value:t}}},function(e,t,n){var o=n(2),i=n(37),r=o["__core-js_shared__"]||i("__core-js_shared__",{});e.exports=r},function(e,t,n){var o=n(2),i=Object.defineProperty;e.exports=function(e,t){try{i(o,e,{value:t,configurable:!0,writable:!0})}catch(n){o[e]=t}return t}},function(e,t,n){var o=n(148),i=n(12),r=Object.prototype,a=r.hasOwnProperty,s=r.propertyIsEnumerable,l=o(function(){return arguments}())?o:function(e){return i(e)&&a.call(e,"callee")&&!s.call(e,"callee")};e.exports=l},function(e,t,n){var o=n(10)(n(6),"Map");e.exports=o},function(e,t){e.exports=function(e){var t=typeof e;return null!=e&&("object"==t||"function"==t)}},function(e,t,n){var o=n(168),i=n(175),r=n(177),a=n(178),s=n(179);function l(e){var t=-1,n=null==e?0:e.length;for(this.clear();++t<n;){var o=e[t];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=a,l.prototype.set=s,e.exports=l},function(e,t){e.exports=function(e){var t=-1,n=Array(e.size);return e.forEach((function(e){n[++t]=e})),n}},function(e,t){e.exports=function(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=9007199254740991}},function(e,t,n){var o=n(5),i=n(45),r=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,a=/^\w*$/;e.exports=function(e,t){if(o(e))return!1;var n=typeof e;return!("number"!=n&&"symbol"!=n&&"boolean"!=n&&null!=e&&!i(e))||(a.test(e)||!r.test(e)||null!=t&&e in Object(t))}},function(e,t,n){var o=n(14),i=n(12);e.exports=function(e){return"symbol"==typeof e||i(e)&&"[object Symbol]"==o(e)}},function(e,t){e.exports=function(e){return e}},function(e,t,n){var o=n(1),i=n(3),r=n(18),a=Object,s=o("".split);e.exports=i((function(){return!a("z").propertyIsEnumerable(0)}))?function(e){return"String"==r(e)?s(e,""):a(e)}:a},function(e,t){e.exports={}},function(e,t){e.exports=function(e){return e.webpackPolyfill||(e.deprecate=function(){},e.paths=[],e.children||(e.children=[]),Object.defineProperty(e,"loaded",{enumerable:!0,get:function(){return e.l}}),Object.defineProperty(e,"id",{enumerable:!0,get:function(){return e.i}}),e.webpackPolyfill=1),e}},function(e,t){var n=/^\s+|\s+$/g,o=/^[-+]0x[0-9a-f]+$/i,i=/^0b[01]+$/i,r=/^0o[0-7]+$/i,a=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=s||l||Function("return this")(),d=Object.prototype.toString,u=Math.max,p=Math.min,h=function(){return c.Date.now()};function f(e){var t=typeof e;return!!e&&("object"==t||"function"==t)}function m(e){if("number"==typeof e)return e;if(function(e){return"symbol"==typeof e||function(e){return!!e&&"object"==typeof e}(e)&&"[object Symbol]"==d.call(e)}(e))return NaN;if(f(e)){var t="function"==typeof e.valueOf?e.valueOf():e;e=f(t)?t+"":t}if("string"!=typeof e)return 0===e?e:+e;e=e.replace(n,"");var s=i.test(e);return s||r.test(e)?a(e.slice(2),s?2:8):o.test(e)?NaN:+e}e.exports=function(e,t,n){var o,i,r,a,s,l,c=0,d=!1,g=!1,v=!0;if("function"!=typeof e)throw new TypeError("Expected a function");function y(t){var n=o,r=i;return o=i=void 0,c=t,a=e.apply(r,n)}function b(e){return c=e,s=setTimeout(x,t),d?y(e):a}function w(e){var n=e-l;return void 0===l||n>=t||n<0||g&&e-c>=r}function x(){var e=h();if(w(e))return k(e);s=setTimeout(x,function(e){var n=t-(e-l);return g?p(n,r-(e-c)):n}(e))}function k(e){return s=void 0,v&&o?y(e):(o=i=void 0,a)}function _(){var e=h(),n=w(e);if(o=arguments,i=this,l=e,n){if(void 0===s)return b(l);if(g)return s=setTimeout(x,t),y(l)}return void 0===s&&(s=setTimeout(x,t)),a}return t=m(t)||0,f(n)&&(d=!!n.leading,r=(g="maxWait"in n)?u(m(n.maxWait)||0,t):r,v="trailing"in n?!!n.trailing:v),_.cancel=function(){void 0!==s&&clearTimeout(s),c=0,o=l=i=s=void 0},_.flush=function(){return void 0===s?a:k(h())},_}},function(e,t,n){var o=n(4),i=n(34),r=n(106),a=n(35),s=n(28),l=n(54),c=n(8),d=n(64),u=Object.getOwnPropertyDescriptor;t.f=o?u:function(e,t){if(e=s(e),t=l(t),d)try{return u(e,t)}catch(e){}if(c(e,t))return a(!i(r.f,e,t),e[t])}},function(e,t,n){var o=n(53),i=TypeError;e.exports=function(e){if(o(e))throw i("Can't call method on "+e);return e}},function(e,t){e.exports=function(e){return null==e}},function(e,t,n){var o=n(107),i=n(56);e.exports=function(e){var t=o(e,"string");return i(t)?t:t+""}},function(e,t){var n="object"==typeof document&&document.all,o=void 0===n&&void 0!==n;e.exports={all:n,IS_HTMLDDA:o}},function(e,t,n){var o=n(29),i=n(0),r=n(57),a=n(58),s=Object;e.exports=a?function(e){return"symbol"==typeof e}:function(e){var t=o("Symbol");return i(t)&&r(t.prototype,s(e))}},function(e,t,n){var o=n(1);e.exports=o({}.isPrototypeOf)},function(e,t,n){var o=n(59);e.exports=o&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(e,t,n){var o=n(60),i=n(3),r=n(2).String;e.exports=!!Object.getOwnPropertySymbols&&!i((function(){var e=Symbol();return!r(e)||!(Object(e)instanceof Symbol)||!Symbol.sham&&o&&o<41}))},function(e,t,n){var o,i,r=n(2),a=n(108),s=r.process,l=r.Deno,c=s&&s.versions||l&&l.version,d=c&&c.v8;d&&(i=(o=d.split("."))[0]>0&&o[0]<4?1:+(o[0]+o[1])),!i&&a&&(!(o=a.match(/Edge\/(\d+)/))||o[1]>=74)&&(o=a.match(/Chrome\/(\d+)/))&&(i=+o[1]),e.exports=i},function(e,t,n){var o=n(62),i=n(36);(e.exports=function(e,t){return i[e]||(i[e]=void 0!==t?t:{})})("versions",[]).push({version:"3.31.0",mode:o?"pure":"global",copyright:"© 2014-2023 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.31.0/LICENSE",source:"https://github.com/zloirock/core-js"})},function(e,t){e.exports=!1},function(e,t,n){var o=n(1),i=0,r=Math.random(),a=o(1..toString);e.exports=function(e){return"Symbol("+(void 0===e?"":e)+")_"+a(++i+r,36)}},function(e,t,n){var o=n(4),i=n(3),r=n(98);e.exports=!o&&!i((function(){return 7!=Object.defineProperty(r("div"),"a",{get:function(){return 7}}).a}))},function(e,t,n){var o=n(8),i=n(117),r=n(51),a=n(17);e.exports=function(e,t,n){for(var s=i(t),l=a.f,c=r.f,d=0;d<s.length;d++){var u=s[d];o(e,u)||n&&o(n,u)||l(e,u,c(t,u))}}},function(e,t,n){var o=n(121);e.exports=function(e){var t=+e;return t!=t||0===t?0:o(t)}},function(e,t,n){var o=n(134),i=n(25),r=n(135);e.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var e,t=!1,n={};try{(e=o(Object.prototype,"__proto__","set"))(n,[]),t=n instanceof Array}catch(e){}return function(n,o){return i(n),r(o),t?e(n,o):n.__proto__=o,n}}():void 0)},function(e,t){e.exports=function(e,t){for(var n=-1,o=t.length,i=e.length;++n<o;)e[i+n]=t[n];return e}},function(e,t){var n="object"==typeof global&&global&&global.Object===Object&&global;e.exports=n},function(e,t,n){var o=n(19),i=n(159),r=n(160),a=n(161),s=n(162),l=n(163);function c(e){var t=this.__data__=new o(e);this.size=t.size}c.prototype.clear=i,c.prototype.delete=r,c.prototype.get=a,c.prototype.has=s,c.prototype.set=l,e.exports=c},function(e,t){e.exports=function(e,t){return e===t||e!=e&&t!=t}},function(e,t,n){var o=n(14),i=n(40);e.exports=function(e){if(!i(e))return!1;var t=o(e);return"[object Function]"==t||"[object GeneratorFunction]"==t||"[object AsyncFunction]"==t||"[object Proxy]"==t}},function(e,t){var n=Function.prototype.toString;e.exports=function(e){if(null!=e){try{return n.call(e)}catch(e){}try{return e+""}catch(e){}}return""}},function(e,t,n){var o=n(180),i=n(12);e.exports=function e(t,n,r,a,s){return t===n||(null==t||null==n||!i(t)&&!i(n)?t!=t&&n!=n:o(t,n,r,a,e,s))}},function(e,t,n){var o=n(76),i=n(183),r=n(77);e.exports=function(e,t,n,a,s,l){var c=1&n,d=e.length,u=t.length;if(d!=u&&!(c&&u>d))return!1;var p=l.get(e),h=l.get(t);if(p&&h)return p==t&&h==e;var f=-1,m=!0,g=2&n?new o:void 0;for(l.set(e,t),l.set(t,e);++f<d;){var v=e[f],y=t[f];if(a)var b=c?a(y,v,f,t,e,l):a(v,y,f,e,t,l);if(void 0!==b){if(b)continue;m=!1;break}if(g){if(!i(t,(function(e,t){if(!r(g,t)&&(v===e||s(v,e,n,a,l)))return g.push(t)}))){m=!1;break}}else if(v!==y&&!s(v,y,n,a,l)){m=!1;break}}return l.delete(e),l.delete(t),m}},function(e,t,n){var o=n(41),i=n(181),r=n(182);function a(e){var t=-1,n=null==e?0:e.length;for(this.__data__=new o;++t<n;)this.add(e[t])}a.prototype.add=a.prototype.push=i,a.prototype.has=r,e.exports=a},function(e,t){e.exports=function(e,t){return e.has(t)}},function(e,t,n){var o=n(193),i=n(199),r=n(82);e.exports=function(e){return r(e)?o(e):i(e)}},function(e,t,n){(function(e){var o=n(6),i=n(195),r=t&&!t.nodeType&&t,a=r&&"object"==typeof e&&e&&!e.nodeType&&e,s=a&&a.exports===r?o.Buffer:void 0,l=(s?s.isBuffer:void 0)||i;e.exports=l}).call(this,n(49)(e))},function(e,t){var n=/^(?:0|[1-9]\d*)$/;e.exports=function(e,t){var o=typeof e;return!!(t=null==t?9007199254740991:t)&&("number"==o||"symbol"!=o&&n.test(e))&&e>-1&&e%1==0&&e<t}},function(e,t,n){var o=n(196),i=n(197),r=n(198),a=r&&r.isTypedArray,s=a?i(a):o;e.exports=s},function(e,t,n){var o=n(72),i=n(43);e.exports=function(e){return null!=e&&i(e.length)&&!o(e)}},function(e,t,n){var o=n(10)(n(6),"Set");e.exports=o},function(e,t,n){var o=n(40);e.exports=function(e){return e==e&&!o(e)}},function(e,t){e.exports=function(e,t){return function(n){return null!=n&&(n[e]===t&&(void 0!==t||e in Object(n)))}}},function(e,t,n){var o=n(87),i=n(23);e.exports=function(e,t){for(var n=0,r=(t=o(t,e)).length;null!=e&&n<r;)e=e[i(t[n++])];return n&&n==r?e:void 0}},function(e,t,n){var o=n(5),i=n(44),r=n(210),a=n(213);e.exports=function(e,t){return o(e)?e:i(e,t)?[e]:r(a(e))}},function(e,t){},function(e,t,n){},function(e,t,n){},function(e,t,n){},function(e,t,n){},function(e,t,n){var o=n(146),i=n(151),r=n(222),a=n(230),s=n(239),l=n(103),c=r((function(e){var t=l(e);return s(t)&&(t=void 0),a(o(e,1,s,!0),i(t,2))}));e.exports=c},function(e,t,n){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var o=/["'&<>]/;e.exports=function(e){var t,n=""+e,i=o.exec(n);if(!i)return n;var r="",a=0,s=0;for(a=i.index;a<n.length;a++){switch(n.charCodeAt(a)){case 34:t="&quot;";break;case 38:t="&amp;";break;case 39:t="&#39;";break;case 60:t="&lt;";break;case 62:t="&gt;";break;default:continue}s!==a&&(r+=n.substring(s,a)),s=a+1,r+=t}return s!==a?r+n.substring(s,a):r}},function(e,t,n){"use strict";n.r(t);var o={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},i=(n(242),n(7)),r=Object(i.a)(o,(function(){return(0,this._self._c)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);t.default=r.exports},function(e,t,n){"use strict";n.r(t);var o={name:"CodeGroup",data:()=>({codeTabs:[],activeCodeTabIndex:-1}),watch:{activeCodeTabIndex(e){this.codeTabs.forEach(e=>{e.elm.classList.remove("theme-code-block__active")}),this.codeTabs[e].elm.classList.add("theme-code-block__active")}},mounted(){this.codeTabs=(this.$slots.default||[]).filter(e=>Boolean(e.componentOptions)).map((e,t)=>(""===e.componentOptions.propsData.active&&(this.activeCodeTabIndex=t),{title:e.componentOptions.propsData.title,elm:e.elm})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab(e){this.activeCodeTabIndex=e}}},i=(n(243),n(7)),r=Object(i.a)(o,(function(){var e=this,t=e._self._c;return t("div",{staticClass:"theme-code-group"},[t("div",{staticClass:"theme-code-group__nav"},[t("ul",{staticClass:"theme-code-group__ul"},e._l(e.codeTabs,(function(n,o){return t("li",{key:n.title,staticClass:"theme-code-group__li"},[t("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":o===e.activeCodeTabIndex},on:{click:function(t){return e.changeCodeTab(o)}}},[e._v("\n            "+e._s(n.title)+"\n          ")])])})),0)]),e._v(" "),e._t("default"),e._v(" "),e.codeTabs.length<1?t("pre",{staticClass:"pre-blank"},[e._v("// Make sure to add code blocks to your code group")]):e._e()],2)}),[],!1,null,"2f5f1757",null);t.default=r.exports},function(e,t){e.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(e,t,n){var o=n(2),i=n(9),r=o.document,a=i(r)&&i(r.createElement);e.exports=function(e){return a?r.createElement(e):{}}},function(e,t,n){var o=n(4),i=n(3);e.exports=o&&i((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(e,t,n){var o=n(1),i=n(3),r=n(0),a=n(8),s=n(4),l=n(113).CONFIGURABLE,c=n(114),d=n(115),u=d.enforce,p=d.get,h=String,f=Object.defineProperty,m=o("".slice),g=o("".replace),v=o([].join),y=s&&!i((function(){return 8!==f((function(){}),"length",{value:8}).length})),b=String(String).split("String"),w=e.exports=function(e,t,n){"Symbol("===m(h(t),0,7)&&(t="["+g(h(t),/^Symbol\(([^)]*)\)/,"$1")+"]"),n&&n.getter&&(t="get "+t),n&&n.setter&&(t="set "+t),(!a(e,"name")||l&&e.name!==t)&&(s?f(e,"name",{value:t,configurable:!0}):e.name=t),y&&n&&a(n,"arity")&&e.length!==n.arity&&f(e,"length",{value:n.arity});try{n&&a(n,"constructor")&&n.constructor?s&&f(e,"prototype",{writable:!1}):e.prototype&&(e.prototype=void 0)}catch(e){}var o=u(e);return a(o,"source")||(o.source=v(b,"string"==typeof t?t:"")),e};Function.prototype.toString=w((function(){return r(this)&&p(this).source||c(this)}),"toString")},function(e,t,n){var o=n(61),i=n(63),r=o("keys");e.exports=function(e){return r[e]||(r[e]=i(e))}},function(e,t,n){var o=n(1),i=n(8),r=n(28),a=n(119).indexOf,s=n(48),l=o([].push);e.exports=function(e,t){var n,o=r(e),c=0,d=[];for(n in o)!i(s,n)&&i(o,n)&&l(d,n);for(;t.length>c;)i(o,n=t[c++])&&(~a(d,n)||l(d,n));return d}},function(e,t){e.exports=function(e){var t=null==e?0:e.length;return t?e[t-1]:void 0}},function(e,t,n){e.exports=n(250)},function(e,t,n){"use strict";var o=n(26),i=n(125).left,r=n(126),a=n(60);o({target:"Array",proto:!0,forced:!n(127)&&a>79&&a<83||!r("reduce")},{reduce:function(e){var t=arguments.length;return i(this,e,t,t>1?arguments[1]:void 0)}})},function(e,t,n){"use strict";var o={}.propertyIsEnumerable,i=Object.getOwnPropertyDescriptor,r=i&&!o.call({1:2},1);t.f=r?function(e){var t=i(this,e);return!!t&&t.enumerable}:o},function(e,t,n){var o=n(34),i=n(9),r=n(56),a=n(109),s=n(111),l=n(31),c=TypeError,d=l("toPrimitive");e.exports=function(e,t){if(!i(e)||r(e))return e;var n,l=a(e,d);if(l){if(void 0===t&&(t="default"),n=o(l,e,t),!i(n)||r(n))return n;throw c("Can't convert object to primitive value")}return void 0===t&&(t="number"),s(e,t)}},function(e,t){e.exports="undefined"!=typeof navigator&&String(navigator.userAgent)||""},function(e,t,n){var o=n(30),i=n(53);e.exports=function(e,t){var n=e[t];return i(n)?void 0:o(n)}},function(e,t){var n=String;e.exports=function(e){try{return n(e)}catch(e){return"Object"}}},function(e,t,n){var o=n(34),i=n(0),r=n(9),a=TypeError;e.exports=function(e,t){var n,s;if("string"===t&&i(n=e.toString)&&!r(s=o(n,e)))return s;if(i(n=e.valueOf)&&!r(s=o(n,e)))return s;if("string"!==t&&i(n=e.toString)&&!r(s=o(n,e)))return s;throw a("Can't convert object to primitive value")}},function(e,t,n){var o=n(0),i=n(17),r=n(100),a=n(37);e.exports=function(e,t,n,s){s||(s={});var l=s.enumerable,c=void 0!==s.name?s.name:t;if(o(n)&&r(n,c,s),s.global)l?e[t]=n:a(t,n);else{try{s.unsafe?e[t]&&(l=!0):delete e[t]}catch(e){}l?e[t]=n:i.f(e,t,{value:n,enumerable:!1,configurable:!s.nonConfigurable,writable:!s.nonWritable})}return e}},function(e,t,n){var o=n(4),i=n(8),r=Function.prototype,a=o&&Object.getOwnPropertyDescriptor,s=i(r,"name"),l=s&&"something"===function(){}.name,c=s&&(!o||o&&a(r,"name").configurable);e.exports={EXISTS:s,PROPER:l,CONFIGURABLE:c}},function(e,t,n){var o=n(1),i=n(0),r=n(36),a=o(Function.toString);i(r.inspectSource)||(r.inspectSource=function(e){return a(e)}),e.exports=r.inspectSource},function(e,t,n){var o,i,r,a=n(116),s=n(2),l=n(9),c=n(15),d=n(8),u=n(36),p=n(101),h=n(48),f=s.TypeError,m=s.WeakMap;if(a||u.state){var g=u.state||(u.state=new m);g.get=g.get,g.has=g.has,g.set=g.set,o=function(e,t){if(g.has(e))throw f("Object already initialized");return t.facade=e,g.set(e,t),t},i=function(e){return g.get(e)||{}},r=function(e){return g.has(e)}}else{var v=p("state");h[v]=!0,o=function(e,t){if(d(e,v))throw f("Object already initialized");return t.facade=e,c(e,v,t),t},i=function(e){return d(e,v)?e[v]:{}},r=function(e){return d(e,v)}}e.exports={set:o,get:i,has:r,enforce:function(e){return r(e)?i(e):o(e,{})},getterFor:function(e){return function(t){var n;if(!l(t)||(n=i(t)).type!==e)throw f("Incompatible receiver, "+e+" required");return n}}}},function(e,t,n){var o=n(2),i=n(0),r=o.WeakMap;e.exports=i(r)&&/native code/.test(String(r))},function(e,t,n){var o=n(29),i=n(1),r=n(118),a=n(123),s=n(25),l=i([].concat);e.exports=o("Reflect","ownKeys")||function(e){var t=r.f(s(e)),n=a.f;return n?l(t,n(e)):t}},function(e,t,n){var o=n(102),i=n(97).concat("length","prototype");t.f=Object.getOwnPropertyNames||function(e){return o(e,i)}},function(e,t,n){var o=n(28),i=n(120),r=n(33),a=function(e){return function(t,n,a){var s,l=o(t),c=r(l),d=i(a,c);if(e&&n!=n){for(;c>d;)if((s=l[d++])!=s)return!0}else for(;c>d;d++)if((e||d in l)&&l[d]===n)return e||d||0;return!e&&-1}};e.exports={includes:a(!0),indexOf:a(!1)}},function(e,t,n){var o=n(66),i=Math.max,r=Math.min;e.exports=function(e,t){var n=o(e);return n<0?i(n+t,0):r(n,t)}},function(e,t){var n=Math.ceil,o=Math.floor;e.exports=Math.trunc||function(e){var t=+e;return(t>0?o:n)(t)}},function(e,t,n){var o=n(66),i=Math.min;e.exports=function(e){return e>0?i(o(e),9007199254740991):0}},function(e,t){t.f=Object.getOwnPropertySymbols},function(e,t,n){var o=n(3),i=n(0),r=/#|\.prototype\./,a=function(e,t){var n=l[s(e)];return n==d||n!=c&&(i(t)?o(t):!!t)},s=a.normalize=function(e){return String(e).replace(r,".").toLowerCase()},l=a.data={},c=a.NATIVE="N",d=a.POLYFILL="P";e.exports=a},function(e,t,n){var o=n(30),i=n(32),r=n(47),a=n(33),s=TypeError,l=function(e){return function(t,n,l,c){o(n);var d=i(t),u=r(d),p=a(d),h=e?p-1:0,f=e?-1:1;if(l<2)for(;;){if(h in u){c=u[h],h+=f;break}if(h+=f,e?h<0:p<=h)throw s("Reduce of empty array with no initial value")}for(;e?h>=0:p>h;h+=f)h in u&&(c=n(c,u[h],h,d));return c}};e.exports={left:l(!1),right:l(!0)}},function(e,t,n){"use strict";var o=n(3);e.exports=function(e,t){var n=[][e];return!!n&&o((function(){n.call(null,t||function(){return 1},1)}))}},function(e,t,n){var o=n(18);e.exports="undefined"!=typeof process&&"process"==o(process)},function(e,t,n){"use strict";var o=n(4),i=n(129),r=TypeError,a=Object.getOwnPropertyDescriptor,s=o&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(e){return e instanceof TypeError}}();e.exports=s?function(e,t){if(i(e)&&!a(e,"length").writable)throw r("Cannot set read only .length");return e.length=t}:function(e,t){return e.length=t}},function(e,t,n){var o=n(18);e.exports=Array.isArray||function(e){return"Array"==o(e)}},function(e,t){var n=TypeError;e.exports=function(e){if(e>9007199254740991)throw n("Maximum allowed index exceeded");return e}},function(e,t,n){var o=n(26),i=n(2),r=n(132),a=n(133),s=i.WebAssembly,l=7!==Error("e",{cause:7}).cause,c=function(e,t){var n={};n[e]=a(e,t,l),o({global:!0,constructor:!0,arity:1,forced:l},n)},d=function(e,t){if(s&&s[e]){var n={};n[e]=a("WebAssembly."+e,t,l),o({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:l},n)}};c("Error",(function(e){return function(t){return r(e,this,arguments)}})),c("EvalError",(function(e){return function(t){return r(e,this,arguments)}})),c("RangeError",(function(e){return function(t){return r(e,this,arguments)}})),c("ReferenceError",(function(e){return function(t){return r(e,this,arguments)}})),c("SyntaxError",(function(e){return function(t){return r(e,this,arguments)}})),c("TypeError",(function(e){return function(t){return r(e,this,arguments)}})),c("URIError",(function(e){return function(t){return r(e,this,arguments)}})),d("CompileError",(function(e){return function(t){return r(e,this,arguments)}})),d("LinkError",(function(e){return function(t){return r(e,this,arguments)}})),d("RuntimeError",(function(e){return function(t){return r(e,this,arguments)}}))},function(e,t,n){var o=n(27),i=Function.prototype,r=i.apply,a=i.call;e.exports="object"==typeof Reflect&&Reflect.apply||(o?a.bind(r):function(){return a.apply(r,arguments)})},function(e,t,n){"use strict";var o=n(29),i=n(8),r=n(15),a=n(57),s=n(67),l=n(65),c=n(136),d=n(137),u=n(138),p=n(142),h=n(143),f=n(4),m=n(62);e.exports=function(e,t,n,g){var v=g?2:1,y=e.split("."),b=y[y.length-1],w=o.apply(null,y);if(w){var x=w.prototype;if(!m&&i(x,"cause")&&delete x.cause,!n)return w;var k=o("Error"),_=t((function(e,t){var n=u(g?t:e,void 0),o=g?new w(e):new w;return void 0!==n&&r(o,"message",n),h(o,_,o.stack,2),this&&a(x,this)&&d(o,this,_),arguments.length>v&&p(o,arguments[v]),o}));if(_.prototype=x,"Error"!==b?s?s(_,k):l(_,k,{name:!0}):f&&"stackTraceLimit"in w&&(c(_,w,"stackTraceLimit"),c(_,w,"prepareStackTrace")),l(_,w),!m)try{x.name!==b&&r(x,"name",b),x.constructor=_}catch(e){}return _}}},function(e,t,n){var o=n(1),i=n(30);e.exports=function(e,t,n){try{return o(i(Object.getOwnPropertyDescriptor(e,t)[n]))}catch(e){}}},function(e,t,n){var o=n(0),i=String,r=TypeError;e.exports=function(e){if("object"==typeof e||o(e))return e;throw r("Can't set "+i(e)+" as a prototype")}},function(e,t,n){var o=n(17).f;e.exports=function(e,t,n){n in e||o(e,n,{configurable:!0,get:function(){return t[n]},set:function(e){t[n]=e}})}},function(e,t,n){var o=n(0),i=n(9),r=n(67);e.exports=function(e,t,n){var a,s;return r&&o(a=t.constructor)&&a!==n&&i(s=a.prototype)&&s!==n.prototype&&r(e,s),e}},function(e,t,n){var o=n(139);e.exports=function(e,t){return void 0===e?arguments.length<2?"":t:o(e)}},function(e,t,n){var o=n(140),i=String;e.exports=function(e){if("Symbol"===o(e))throw TypeError("Cannot convert a Symbol value to a string");return i(e)}},function(e,t,n){var o=n(141),i=n(0),r=n(18),a=n(31)("toStringTag"),s=Object,l="Arguments"==r(function(){return arguments}());e.exports=o?r:function(e){var t,n,o;return void 0===e?"Undefined":null===e?"Null":"string"==typeof(n=function(e,t){try{return e[t]}catch(e){}}(t=s(e),a))?n:l?r(t):"Object"==(o=r(t))&&i(t.callee)?"Arguments":o}},function(e,t,n){var o={};o[n(31)("toStringTag")]="z",e.exports="[object z]"===String(o)},function(e,t,n){var o=n(9),i=n(15);e.exports=function(e,t){o(t)&&"cause"in t&&i(e,"cause",t.cause)}},function(e,t,n){var o=n(15),i=n(144),r=n(145),a=Error.captureStackTrace;e.exports=function(e,t,n,s){r&&(a?a(e,t):o(e,"stack",i(n,s)))}},function(e,t,n){var o=n(1),i=Error,r=o("".replace),a=String(i("zxcasd").stack),s=/\n\s*at [^:]*:[^\n]*/,l=s.test(a);e.exports=function(e,t){if(l&&"string"==typeof e&&!i.prepareStackTrace)for(;t--;)e=r(e,s,"");return e}},function(e,t,n){var o=n(3),i=n(35);e.exports=!o((function(){var e=Error("a");return!("stack"in e)||(Object.defineProperty(e,"stack",i(1,7)),7!==e.stack)}))},function(e,t,n){var o=n(68),i=n(147);e.exports=function e(t,n,r,a,s){var l=-1,c=t.length;for(r||(r=i),s||(s=[]);++l<c;){var d=t[l];n>0&&r(d)?n>1?e(d,n-1,r,a,s):o(s,d):a||(s[s.length]=d)}return s}},function(e,t,n){var o=n(16),i=n(38),r=n(5),a=o?o.isConcatSpreadable:void 0;e.exports=function(e){return r(e)||i(e)||!!(a&&e&&e[a])}},function(e,t,n){var o=n(14),i=n(12);e.exports=function(e){return i(e)&&"[object Arguments]"==o(e)}},function(e,t,n){var o=n(16),i=Object.prototype,r=i.hasOwnProperty,a=i.toString,s=o?o.toStringTag:void 0;e.exports=function(e){var t=r.call(e,s),n=e[s];try{e[s]=void 0;var o=!0}catch(e){}var i=a.call(e);return o&&(t?e[s]=n:delete e[s]),i}},function(e,t){var n=Object.prototype.toString;e.exports=function(e){return n.call(e)}},function(e,t,n){var o=n(152),i=n(208),r=n(46),a=n(5),s=n(219);e.exports=function(e){return"function"==typeof e?e:null==e?r:"object"==typeof e?a(e)?i(e[0],e[1]):o(e):s(e)}},function(e,t,n){var o=n(153),i=n(207),r=n(85);e.exports=function(e){var t=i(e);return 1==t.length&&t[0][2]?r(t[0][0],t[0][1]):function(n){return n===e||o(n,e,t)}}},function(e,t,n){var o=n(70),i=n(74);e.exports=function(e,t,n,r){var a=n.length,s=a,l=!r;if(null==e)return!s;for(e=Object(e);a--;){var c=n[a];if(l&&c[2]?c[1]!==e[c[0]]:!(c[0]in e))return!1}for(;++a<s;){var d=(c=n[a])[0],u=e[d],p=c[1];if(l&&c[2]){if(void 0===u&&!(d in e))return!1}else{var h=new o;if(r)var f=r(u,p,d,e,t,h);if(!(void 0===f?i(p,u,3,r,h):f))return!1}}return!0}},function(e,t){e.exports=function(){this.__data__=[],this.size=0}},function(e,t,n){var o=n(20),i=Array.prototype.splice;e.exports=function(e){var t=this.__data__,n=o(t,e);return!(n<0)&&(n==t.length-1?t.pop():i.call(t,n,1),--this.size,!0)}},function(e,t,n){var o=n(20);e.exports=function(e){var t=this.__data__,n=o(t,e);return n<0?void 0:t[n][1]}},function(e,t,n){var o=n(20);e.exports=function(e){return o(this.__data__,e)>-1}},function(e,t,n){var o=n(20);e.exports=function(e,t){var n=this.__data__,i=o(n,e);return i<0?(++this.size,n.push([e,t])):n[i][1]=t,this}},function(e,t,n){var o=n(19);e.exports=function(){this.__data__=new o,this.size=0}},function(e,t){e.exports=function(e){var t=this.__data__,n=t.delete(e);return this.size=t.size,n}},function(e,t){e.exports=function(e){return this.__data__.get(e)}},function(e,t){e.exports=function(e){return this.__data__.has(e)}},function(e,t,n){var o=n(19),i=n(39),r=n(41);e.exports=function(e,t){var n=this.__data__;if(n instanceof o){var a=n.__data__;if(!i||a.length<199)return a.push([e,t]),this.size=++n.size,this;n=this.__data__=new r(a)}return n.set(e,t),this.size=n.size,this}},function(e,t,n){var o=n(72),i=n(165),r=n(40),a=n(73),s=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,d=l.toString,u=c.hasOwnProperty,p=RegExp("^"+d.call(u).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");e.exports=function(e){return!(!r(e)||i(e))&&(o(e)?p:s).test(a(e))}},function(e,t,n){var o,i=n(166),r=(o=/[^.]+$/.exec(i&&i.keys&&i.keys.IE_PROTO||""))?"Symbol(src)_1."+o:"";e.exports=function(e){return!!r&&r in e}},function(e,t,n){var o=n(6)["__core-js_shared__"];e.exports=o},function(e,t){e.exports=function(e,t){return null==e?void 0:e[t]}},function(e,t,n){var o=n(169),i=n(19),r=n(39);e.exports=function(){this.size=0,this.__data__={hash:new o,map:new(r||i),string:new o}}},function(e,t,n){var o=n(170),i=n(171),r=n(172),a=n(173),s=n(174);function l(e){var t=-1,n=null==e?0:e.length;for(this.clear();++t<n;){var o=e[t];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=a,l.prototype.set=s,e.exports=l},function(e,t,n){var o=n(21);e.exports=function(){this.__data__=o?o(null):{},this.size=0}},function(e,t){e.exports=function(e){var t=this.has(e)&&delete this.__data__[e];return this.size-=t?1:0,t}},function(e,t,n){var o=n(21),i=Object.prototype.hasOwnProperty;e.exports=function(e){var t=this.__data__;if(o){var n=t[e];return"__lodash_hash_undefined__"===n?void 0:n}return i.call(t,e)?t[e]:void 0}},function(e,t,n){var o=n(21),i=Object.prototype.hasOwnProperty;e.exports=function(e){var t=this.__data__;return o?void 0!==t[e]:i.call(t,e)}},function(e,t,n){var o=n(21);e.exports=function(e,t){var n=this.__data__;return this.size+=this.has(e)?0:1,n[e]=o&&void 0===t?"__lodash_hash_undefined__":t,this}},function(e,t,n){var o=n(22);e.exports=function(e){var t=o(this,e).delete(e);return this.size-=t?1:0,t}},function(e,t){e.exports=function(e){var t=typeof e;return"string"==t||"number"==t||"symbol"==t||"boolean"==t?"__proto__"!==e:null===e}},function(e,t,n){var o=n(22);e.exports=function(e){return o(this,e).get(e)}},function(e,t,n){var o=n(22);e.exports=function(e){return o(this,e).has(e)}},function(e,t,n){var o=n(22);e.exports=function(e,t){var n=o(this,e),i=n.size;return n.set(e,t),this.size+=n.size==i?0:1,this}},function(e,t,n){var o=n(70),i=n(75),r=n(184),a=n(187),s=n(203),l=n(5),c=n(79),d=n(81),u="[object Object]",p=Object.prototype.hasOwnProperty;e.exports=function(e,t,n,h,f,m){var g=l(e),v=l(t),y=g?"[object Array]":s(e),b=v?"[object Array]":s(t),w=(y="[object Arguments]"==y?u:y)==u,x=(b="[object Arguments]"==b?u:b)==u,k=y==b;if(k&&c(e)){if(!c(t))return!1;g=!0,w=!1}if(k&&!w)return m||(m=new o),g||d(e)?i(e,t,n,h,f,m):r(e,t,y,n,h,f,m);if(!(1&n)){var _=w&&p.call(e,"__wrapped__"),T=x&&p.call(t,"__wrapped__");if(_||T){var C=_?e.value():e,S=T?t.value():t;return m||(m=new o),f(C,S,n,h,m)}}return!!k&&(m||(m=new o),a(e,t,n,h,f,m))}},function(e,t){e.exports=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this}},function(e,t){e.exports=function(e){return this.__data__.has(e)}},function(e,t){e.exports=function(e,t){for(var n=-1,o=null==e?0:e.length;++n<o;)if(t(e[n],n,e))return!0;return!1}},function(e,t,n){var o=n(16),i=n(185),r=n(71),a=n(75),s=n(186),l=n(42),c=o?o.prototype:void 0,d=c?c.valueOf:void 0;e.exports=function(e,t,n,o,c,u,p){switch(n){case"[object DataView]":if(e.byteLength!=t.byteLength||e.byteOffset!=t.byteOffset)return!1;e=e.buffer,t=t.buffer;case"[object ArrayBuffer]":return!(e.byteLength!=t.byteLength||!u(new i(e),new i(t)));case"[object Boolean]":case"[object Date]":case"[object Number]":return r(+e,+t);case"[object Error]":return e.name==t.name&&e.message==t.message;case"[object RegExp]":case"[object String]":return e==t+"";case"[object Map]":var h=s;case"[object Set]":var f=1&o;if(h||(h=l),e.size!=t.size&&!f)return!1;var m=p.get(e);if(m)return m==t;o|=2,p.set(e,t);var g=a(h(e),h(t),o,c,u,p);return p.delete(e),g;case"[object Symbol]":if(d)return d.call(e)==d.call(t)}return!1}},function(e,t,n){var o=n(6).Uint8Array;e.exports=o},function(e,t){e.exports=function(e){var t=-1,n=Array(e.size);return e.forEach((function(e,o){n[++t]=[o,e]})),n}},function(e,t,n){var o=n(188),i=Object.prototype.hasOwnProperty;e.exports=function(e,t,n,r,a,s){var l=1&n,c=o(e),d=c.length;if(d!=o(t).length&&!l)return!1;for(var u=d;u--;){var p=c[u];if(!(l?p in t:i.call(t,p)))return!1}var h=s.get(e),f=s.get(t);if(h&&f)return h==t&&f==e;var m=!0;s.set(e,t),s.set(t,e);for(var g=l;++u<d;){var v=e[p=c[u]],y=t[p];if(r)var b=l?r(y,v,p,t,e,s):r(v,y,p,e,t,s);if(!(void 0===b?v===y||a(v,y,n,r,s):b)){m=!1;break}g||(g="constructor"==p)}if(m&&!g){var w=e.constructor,x=t.constructor;w==x||!("constructor"in e)||!("constructor"in t)||"function"==typeof w&&w instanceof w&&"function"==typeof x&&x instanceof x||(m=!1)}return s.delete(e),s.delete(t),m}},function(e,t,n){var o=n(189),i=n(190),r=n(78);e.exports=function(e){return o(e,r,i)}},function(e,t,n){var o=n(68),i=n(5);e.exports=function(e,t,n){var r=t(e);return i(e)?r:o(r,n(e))}},function(e,t,n){var o=n(191),i=n(192),r=Object.prototype.propertyIsEnumerable,a=Object.getOwnPropertySymbols,s=a?function(e){return null==e?[]:(e=Object(e),o(a(e),(function(t){return r.call(e,t)})))}:i;e.exports=s},function(e,t){e.exports=function(e,t){for(var n=-1,o=null==e?0:e.length,i=0,r=[];++n<o;){var a=e[n];t(a,n,e)&&(r[i++]=a)}return r}},function(e,t){e.exports=function(){return[]}},function(e,t,n){var o=n(194),i=n(38),r=n(5),a=n(79),s=n(80),l=n(81),c=Object.prototype.hasOwnProperty;e.exports=function(e,t){var n=r(e),d=!n&&i(e),u=!n&&!d&&a(e),p=!n&&!d&&!u&&l(e),h=n||d||u||p,f=h?o(e.length,String):[],m=f.length;for(var g in e)!t&&!c.call(e,g)||h&&("length"==g||u&&("offset"==g||"parent"==g)||p&&("buffer"==g||"byteLength"==g||"byteOffset"==g)||s(g,m))||f.push(g);return f}},function(e,t){e.exports=function(e,t){for(var n=-1,o=Array(e);++n<e;)o[n]=t(n);return o}},function(e,t){e.exports=function(){return!1}},function(e,t,n){var o=n(14),i=n(43),r=n(12),a={};a["[object Float32Array]"]=a["[object Float64Array]"]=a["[object Int8Array]"]=a["[object Int16Array]"]=a["[object Int32Array]"]=a["[object Uint8Array]"]=a["[object Uint8ClampedArray]"]=a["[object Uint16Array]"]=a["[object Uint32Array]"]=!0,a["[object Arguments]"]=a["[object Array]"]=a["[object ArrayBuffer]"]=a["[object Boolean]"]=a["[object DataView]"]=a["[object Date]"]=a["[object Error]"]=a["[object Function]"]=a["[object Map]"]=a["[object Number]"]=a["[object Object]"]=a["[object RegExp]"]=a["[object Set]"]=a["[object String]"]=a["[object WeakMap]"]=!1,e.exports=function(e){return r(e)&&i(e.length)&&!!a[o(e)]}},function(e,t){e.exports=function(e){return function(t){return e(t)}}},function(e,t,n){(function(e){var o=n(69),i=t&&!t.nodeType&&t,r=i&&"object"==typeof e&&e&&!e.nodeType&&e,a=r&&r.exports===i&&o.process,s=function(){try{var e=r&&r.require&&r.require("util").types;return e||a&&a.binding&&a.binding("util")}catch(e){}}();e.exports=s}).call(this,n(49)(e))},function(e,t,n){var o=n(200),i=n(201),r=Object.prototype.hasOwnProperty;e.exports=function(e){if(!o(e))return i(e);var t=[];for(var n in Object(e))r.call(e,n)&&"constructor"!=n&&t.push(n);return t}},function(e,t){var n=Object.prototype;e.exports=function(e){var t=e&&e.constructor;return e===("function"==typeof t&&t.prototype||n)}},function(e,t,n){var o=n(202)(Object.keys,Object);e.exports=o},function(e,t){e.exports=function(e,t){return function(n){return e(t(n))}}},function(e,t,n){var o=n(204),i=n(39),r=n(205),a=n(83),s=n(206),l=n(14),c=n(73),d=c(o),u=c(i),p=c(r),h=c(a),f=c(s),m=l;(o&&"[object DataView]"!=m(new o(new ArrayBuffer(1)))||i&&"[object Map]"!=m(new i)||r&&"[object Promise]"!=m(r.resolve())||a&&"[object Set]"!=m(new a)||s&&"[object WeakMap]"!=m(new s))&&(m=function(e){var t=l(e),n="[object Object]"==t?e.constructor:void 0,o=n?c(n):"";if(o)switch(o){case d:return"[object DataView]";case u:return"[object Map]";case p:return"[object Promise]";case h:return"[object Set]";case f:return"[object WeakMap]"}return t}),e.exports=m},function(e,t,n){var o=n(10)(n(6),"DataView");e.exports=o},function(e,t,n){var o=n(10)(n(6),"Promise");e.exports=o},function(e,t,n){var o=n(10)(n(6),"WeakMap");e.exports=o},function(e,t,n){var o=n(84),i=n(78);e.exports=function(e){for(var t=i(e),n=t.length;n--;){var r=t[n],a=e[r];t[n]=[r,a,o(a)]}return t}},function(e,t,n){var o=n(74),i=n(209),r=n(216),a=n(44),s=n(84),l=n(85),c=n(23);e.exports=function(e,t){return a(e)&&s(t)?l(c(e),t):function(n){var a=i(n,e);return void 0===a&&a===t?r(n,e):o(t,a,3)}}},function(e,t,n){var o=n(86);e.exports=function(e,t,n){var i=null==e?void 0:o(e,t);return void 0===i?n:i}},function(e,t,n){var o=n(211),i=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,r=/\\(\\)?/g,a=o((function(e){var t=[];return 46===e.charCodeAt(0)&&t.push(""),e.replace(i,(function(e,n,o,i){t.push(o?i.replace(r,"$1"):n||e)})),t}));e.exports=a},function(e,t,n){var o=n(212);e.exports=function(e){var t=o(e,(function(e){return 500===n.size&&n.clear(),e})),n=t.cache;return t}},function(e,t,n){var o=n(41);function i(e,t){if("function"!=typeof e||null!=t&&"function"!=typeof t)throw new TypeError("Expected a function");var n=function(){var o=arguments,i=t?t.apply(this,o):o[0],r=n.cache;if(r.has(i))return r.get(i);var a=e.apply(this,o);return n.cache=r.set(i,a)||r,a};return n.cache=new(i.Cache||o),n}i.Cache=o,e.exports=i},function(e,t,n){var o=n(214);e.exports=function(e){return null==e?"":o(e)}},function(e,t,n){var o=n(16),i=n(215),r=n(5),a=n(45),s=o?o.prototype:void 0,l=s?s.toString:void 0;e.exports=function e(t){if("string"==typeof t)return t;if(r(t))return i(t,e)+"";if(a(t))return l?l.call(t):"";var n=t+"";return"0"==n&&1/t==-1/0?"-0":n}},function(e,t){e.exports=function(e,t){for(var n=-1,o=null==e?0:e.length,i=Array(o);++n<o;)i[n]=t(e[n],n,e);return i}},function(e,t,n){var o=n(217),i=n(218);e.exports=function(e,t){return null!=e&&i(e,t,o)}},function(e,t){e.exports=function(e,t){return null!=e&&t in Object(e)}},function(e,t,n){var o=n(87),i=n(38),r=n(5),a=n(80),s=n(43),l=n(23);e.exports=function(e,t,n){for(var c=-1,d=(t=o(t,e)).length,u=!1;++c<d;){var p=l(t[c]);if(!(u=null!=e&&n(e,p)))break;e=e[p]}return u||++c!=d?u:!!(d=null==e?0:e.length)&&s(d)&&a(p,d)&&(r(e)||i(e))}},function(e,t,n){var o=n(220),i=n(221),r=n(44),a=n(23);e.exports=function(e){return r(e)?o(a(e)):i(e)}},function(e,t){e.exports=function(e){return function(t){return null==t?void 0:t[e]}}},function(e,t,n){var o=n(86);e.exports=function(e){return function(t){return o(t,e)}}},function(e,t,n){var o=n(46),i=n(223),r=n(225);e.exports=function(e,t){return r(i(e,t,o),e+"")}},function(e,t,n){var o=n(224),i=Math.max;e.exports=function(e,t,n){return t=i(void 0===t?e.length-1:t,0),function(){for(var r=arguments,a=-1,s=i(r.length-t,0),l=Array(s);++a<s;)l[a]=r[t+a];a=-1;for(var c=Array(t+1);++a<t;)c[a]=r[a];return c[t]=n(l),o(e,this,c)}}},function(e,t){e.exports=function(e,t,n){switch(n.length){case 0:return e.call(t);case 1:return e.call(t,n[0]);case 2:return e.call(t,n[0],n[1]);case 3:return e.call(t,n[0],n[1],n[2])}return e.apply(t,n)}},function(e,t,n){var o=n(226),i=n(229)(o);e.exports=i},function(e,t,n){var o=n(227),i=n(228),r=n(46),a=i?function(e,t){return i(e,"toString",{configurable:!0,enumerable:!1,value:o(t),writable:!0})}:r;e.exports=a},function(e,t){e.exports=function(e){return function(){return e}}},function(e,t,n){var o=n(10),i=function(){try{var e=o(Object,"defineProperty");return e({},"",{}),e}catch(e){}}();e.exports=i},function(e,t){var n=Date.now;e.exports=function(e){var t=0,o=0;return function(){var i=n(),r=16-(i-o);if(o=i,r>0){if(++t>=800)return arguments[0]}else t=0;return e.apply(void 0,arguments)}}},function(e,t,n){var o=n(76),i=n(231),r=n(236),a=n(77),s=n(237),l=n(42);e.exports=function(e,t,n){var c=-1,d=i,u=e.length,p=!0,h=[],f=h;if(n)p=!1,d=r;else if(u>=200){var m=t?null:s(e);if(m)return l(m);p=!1,d=a,f=new o}else f=t?[]:h;e:for(;++c<u;){var g=e[c],v=t?t(g):g;if(g=n||0!==g?g:0,p&&v==v){for(var y=f.length;y--;)if(f[y]===v)continue e;t&&f.push(v),h.push(g)}else d(f,v,n)||(f!==h&&f.push(v),h.push(g))}return h}},function(e,t,n){var o=n(232);e.exports=function(e,t){return!!(null==e?0:e.length)&&o(e,t,0)>-1}},function(e,t,n){var o=n(233),i=n(234),r=n(235);e.exports=function(e,t,n){return t==t?r(e,t,n):o(e,i,n)}},function(e,t){e.exports=function(e,t,n,o){for(var i=e.length,r=n+(o?1:-1);o?r--:++r<i;)if(t(e[r],r,e))return r;return-1}},function(e,t){e.exports=function(e){return e!=e}},function(e,t){e.exports=function(e,t,n){for(var o=n-1,i=e.length;++o<i;)if(e[o]===t)return o;return-1}},function(e,t){e.exports=function(e,t,n){for(var o=-1,i=null==e?0:e.length;++o<i;)if(n(t,e[o]))return!0;return!1}},function(e,t,n){var o=n(83),i=n(238),r=n(42),a=o&&1/r(new o([,-0]))[1]==1/0?function(e){return new o(e)}:i;e.exports=a},function(e,t){e.exports=function(){}},function(e,t,n){var o=n(82),i=n(12);e.exports=function(e){return i(e)&&o(e)}},function(e,t,n){},function(e,t,n){},function(e,t,n){"use strict";n(89)},function(e,t,n){"use strict";n(90)},function(e,t,n){},function(e,t,n){},function(e,t,n){},function(e,t,n){},function(e,t,n){"use strict";n(91)},function(e,t,n){"use strict";n(92)},function(e,t,n){"use strict";n.r(t);
/*!
 * Vue.js v2.7.14
 * (c) 2014-2022 Evan You
 * Released under the MIT License.
 */
var o=Object.freeze({}),i=Array.isArray;function r(e){return null==e}function a(e){return null!=e}function s(e){return!0===e}function l(e){return"string"==typeof e||"number"==typeof e||"symbol"==typeof e||"boolean"==typeof e}function c(e){return"function"==typeof e}function d(e){return null!==e&&"object"==typeof e}var u=Object.prototype.toString;function p(e){return"[object Object]"===u.call(e)}function h(e){return"[object RegExp]"===u.call(e)}function f(e){var t=parseFloat(String(e));return t>=0&&Math.floor(t)===t&&isFinite(e)}function m(e){return a(e)&&"function"==typeof e.then&&"function"==typeof e.catch}function g(e){return null==e?"":Array.isArray(e)||p(e)&&e.toString===u?JSON.stringify(e,null,2):String(e)}function v(e){var t=parseFloat(e);return isNaN(t)?e:t}function y(e,t){for(var n=Object.create(null),o=e.split(","),i=0;i<o.length;i++)n[o[i]]=!0;return t?function(e){return n[e.toLowerCase()]}:function(e){return n[e]}}y("slot,component",!0);var b=y("key,ref,slot,slot-scope,is");function w(e,t){var n=e.length;if(n){if(t===e[n-1])return void(e.length=n-1);var o=e.indexOf(t);if(o>-1)return e.splice(o,1)}}var x=Object.prototype.hasOwnProperty;function k(e,t){return x.call(e,t)}function _(e){var t=Object.create(null);return function(n){return t[n]||(t[n]=e(n))}}var T=/-(\w)/g,C=_((function(e){return e.replace(T,(function(e,t){return t?t.toUpperCase():""}))})),S=_((function(e){return e.charAt(0).toUpperCase()+e.slice(1)})),I=/\B([A-Z])/g,A=_((function(e){return e.replace(I,"-$1").toLowerCase()}));var E=Function.prototype.bind?function(e,t){return e.bind(t)}:function(e,t){function n(n){var o=arguments.length;return o?o>1?e.apply(t,arguments):e.call(t,n):e.call(t)}return n._length=e.length,n};function R(e,t){t=t||0;for(var n=e.length-t,o=new Array(n);n--;)o[n]=e[n+t];return o}function D(e,t){for(var n in t)e[n]=t[n];return e}function z(e){for(var t={},n=0;n<e.length;n++)e[n]&&D(t,e[n]);return t}function O(e,t,n){}var P=function(e,t,n){return!1},q=function(e){return e};function j(e,t){if(e===t)return!0;var n=d(e),o=d(t);if(!n||!o)return!n&&!o&&String(e)===String(t);try{var i=Array.isArray(e),r=Array.isArray(t);if(i&&r)return e.length===t.length&&e.every((function(e,n){return j(e,t[n])}));if(e instanceof Date&&t instanceof Date)return e.getTime()===t.getTime();if(i||r)return!1;var a=Object.keys(e),s=Object.keys(t);return a.length===s.length&&a.every((function(n){return j(e[n],t[n])}))}catch(e){return!1}}function F(e,t){for(var n=0;n<e.length;n++)if(j(e[n],t))return n;return-1}function V(e){var t=!1;return function(){t||(t=!0,e.apply(this,arguments))}}function M(e,t){return e===t?0===e&&1/e!=1/t:e==e||t==t}var L=["component","directive","filter"],H=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],N={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:P,isReservedAttr:P,isUnknownElement:P,getTagNamespace:O,parsePlatformTagName:q,mustUseProp:P,async:!0,_lifecycleHooks:H},$=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function U(e){var t=(e+"").charCodeAt(0);return 36===t||95===t}function B(e,t,n,o){Object.defineProperty(e,t,{value:n,enumerable:!!o,writable:!0,configurable:!0})}var G=new RegExp("[^".concat($.source,".$_\\d]"));var W="__proto__"in{},Q="undefined"!=typeof window,Y=Q&&window.navigator.userAgent.toLowerCase(),K=Y&&/msie|trident/.test(Y),X=Y&&Y.indexOf("msie 9.0")>0,Z=Y&&Y.indexOf("edge/")>0;Y&&Y.indexOf("android");var J=Y&&/iphone|ipad|ipod|ios/.test(Y);Y&&/chrome\/\d+/.test(Y),Y&&/phantomjs/.test(Y);var ee,te=Y&&Y.match(/firefox\/(\d+)/),ne={}.watch,oe=!1;if(Q)try{var ie={};Object.defineProperty(ie,"passive",{get:function(){oe=!0}}),window.addEventListener("test-passive",null,ie)}catch(e){}var re=function(){return void 0===ee&&(ee=!Q&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),ee},ae=Q&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function se(e){return"function"==typeof e&&/native code/.test(e.toString())}var le,ce="undefined"!=typeof Symbol&&se(Symbol)&&"undefined"!=typeof Reflect&&se(Reflect.ownKeys);le="undefined"!=typeof Set&&se(Set)?Set:function(){function e(){this.set=Object.create(null)}return e.prototype.has=function(e){return!0===this.set[e]},e.prototype.add=function(e){this.set[e]=!0},e.prototype.clear=function(){this.set=Object.create(null)},e}();var de=null;function ue(e){void 0===e&&(e=null),e||de&&de._scope.off(),de=e,e&&e._scope.on()}var pe=function(){function e(e,t,n,o,i,r,a,s){this.tag=e,this.data=t,this.children=n,this.text=o,this.elm=i,this.ns=void 0,this.context=r,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=t&&t.key,this.componentOptions=a,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(e.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),e}(),he=function(e){void 0===e&&(e="");var t=new pe;return t.text=e,t.isComment=!0,t};function fe(e){return new pe(void 0,void 0,void 0,String(e))}function me(e){var t=new pe(e.tag,e.data,e.children&&e.children.slice(),e.text,e.elm,e.context,e.componentOptions,e.asyncFactory);return t.ns=e.ns,t.isStatic=e.isStatic,t.key=e.key,t.isComment=e.isComment,t.fnContext=e.fnContext,t.fnOptions=e.fnOptions,t.fnScopeId=e.fnScopeId,t.asyncMeta=e.asyncMeta,t.isCloned=!0,t}var ge=0,ve=[],ye=function(){function e(){this._pending=!1,this.id=ge++,this.subs=[]}return e.prototype.addSub=function(e){this.subs.push(e)},e.prototype.removeSub=function(e){this.subs[this.subs.indexOf(e)]=null,this._pending||(this._pending=!0,ve.push(this))},e.prototype.depend=function(t){e.target&&e.target.addDep(this)},e.prototype.notify=function(e){var t=this.subs.filter((function(e){return e}));for(var n=0,o=t.length;n<o;n++){0,t[n].update()}},e}();ye.target=null;var be=[];function we(e){be.push(e),ye.target=e}function xe(){be.pop(),ye.target=be[be.length-1]}var ke=Array.prototype,_e=Object.create(ke);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(e){var t=ke[e];B(_e,e,(function(){for(var n=[],o=0;o<arguments.length;o++)n[o]=arguments[o];var i,r=t.apply(this,n),a=this.__ob__;switch(e){case"push":case"unshift":i=n;break;case"splice":i=n.slice(2)}return i&&a.observeArray(i),a.dep.notify(),r}))}));var Te=Object.getOwnPropertyNames(_e),Ce={},Se=!0;function Ie(e){Se=e}var Ae={notify:O,depend:O,addSub:O,removeSub:O},Ee=function(){function e(e,t,n){if(void 0===t&&(t=!1),void 0===n&&(n=!1),this.value=e,this.shallow=t,this.mock=n,this.dep=n?Ae:new ye,this.vmCount=0,B(e,"__ob__",this),i(e)){if(!n)if(W)e.__proto__=_e;else for(var o=0,r=Te.length;o<r;o++){B(e,s=Te[o],_e[s])}t||this.observeArray(e)}else{var a=Object.keys(e);for(o=0;o<a.length;o++){var s;De(e,s=a[o],Ce,void 0,t,n)}}}return e.prototype.observeArray=function(e){for(var t=0,n=e.length;t<n;t++)Re(e[t],!1,this.mock)},e}();function Re(e,t,n){return e&&k(e,"__ob__")&&e.__ob__ instanceof Ee?e.__ob__:!Se||!n&&re()||!i(e)&&!p(e)||!Object.isExtensible(e)||e.__v_skip||Ve(e)||e instanceof pe?void 0:new Ee(e,t,n)}function De(e,t,n,o,r,a){var s=new ye,l=Object.getOwnPropertyDescriptor(e,t);if(!l||!1!==l.configurable){var c=l&&l.get,d=l&&l.set;c&&!d||n!==Ce&&2!==arguments.length||(n=e[t]);var u=!r&&Re(n,!1,a);return Object.defineProperty(e,t,{enumerable:!0,configurable:!0,get:function(){var t=c?c.call(e):n;return ye.target&&(s.depend(),u&&(u.dep.depend(),i(t)&&Pe(t))),Ve(t)&&!r?t.value:t},set:function(t){var o=c?c.call(e):n;if(M(o,t)){if(d)d.call(e,t);else{if(c)return;if(!r&&Ve(o)&&!Ve(t))return void(o.value=t);n=t}u=!r&&Re(t,!1,a),s.notify()}}}),s}}function ze(e,t,n){if(!Fe(e)){var o=e.__ob__;return i(e)&&f(t)?(e.length=Math.max(e.length,t),e.splice(t,1,n),o&&!o.shallow&&o.mock&&Re(n,!1,!0),n):t in e&&!(t in Object.prototype)?(e[t]=n,n):e._isVue||o&&o.vmCount?n:o?(De(o.value,t,n,void 0,o.shallow,o.mock),o.dep.notify(),n):(e[t]=n,n)}}function Oe(e,t){if(i(e)&&f(t))e.splice(t,1);else{var n=e.__ob__;e._isVue||n&&n.vmCount||Fe(e)||k(e,t)&&(delete e[t],n&&n.dep.notify())}}function Pe(e){for(var t=void 0,n=0,o=e.length;n<o;n++)(t=e[n])&&t.__ob__&&t.__ob__.dep.depend(),i(t)&&Pe(t)}function qe(e){return je(e,!0),B(e,"__v_isShallow",!0),e}function je(e,t){if(!Fe(e)){Re(e,t,re());0}}function Fe(e){return!(!e||!e.__v_isReadonly)}function Ve(e){return!(!e||!0!==e.__v_isRef)}function Me(e,t,n){Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){var e=t[n];if(Ve(e))return e.value;var o=e&&e.__ob__;return o&&o.dep.depend(),e},set:function(e){var o=t[n];Ve(o)&&!Ve(e)?o.value=e:t[n]=e}})}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var Le;var He=function(){function e(e){void 0===e&&(e=!1),this.detached=e,this.active=!0,this.effects=[],this.cleanups=[],this.parent=Le,!e&&Le&&(this.index=(Le.scopes||(Le.scopes=[])).push(this)-1)}return e.prototype.run=function(e){if(this.active){var t=Le;try{return Le=this,e()}finally{Le=t}}else 0},e.prototype.on=function(){Le=this},e.prototype.off=function(){Le=this.parent},e.prototype.stop=function(e){if(this.active){var t=void 0,n=void 0;for(t=0,n=this.effects.length;t<n;t++)this.effects[t].teardown();for(t=0,n=this.cleanups.length;t<n;t++)this.cleanups[t]();if(this.scopes)for(t=0,n=this.scopes.length;t<n;t++)this.scopes[t].stop(!0);if(!this.detached&&this.parent&&!e){var o=this.parent.scopes.pop();o&&o!==this&&(this.parent.scopes[this.index]=o,o.index=this.index)}this.parent=void 0,this.active=!1}},e}();function Ne(e){var t=e._provided,n=e.$parent&&e.$parent._provided;return n===t?e._provided=Object.create(n):t}var $e=_((function(e){var t="&"===e.charAt(0),n="~"===(e=t?e.slice(1):e).charAt(0),o="!"===(e=n?e.slice(1):e).charAt(0);return{name:e=o?e.slice(1):e,once:n,capture:o,passive:t}}));function Ue(e,t){function n(){var e=n.fns;if(!i(e))return St(e,null,arguments,t,"v-on handler");for(var o=e.slice(),r=0;r<o.length;r++)St(o[r],null,arguments,t,"v-on handler")}return n.fns=e,n}function Be(e,t,n,o,i,a){var l,c,d,u;for(l in e)c=e[l],d=t[l],u=$e(l),r(c)||(r(d)?(r(c.fns)&&(c=e[l]=Ue(c,a)),s(u.once)&&(c=e[l]=i(u.name,c,u.capture)),n(u.name,c,u.capture,u.passive,u.params)):c!==d&&(d.fns=c,e[l]=d));for(l in t)r(e[l])&&o((u=$e(l)).name,t[l],u.capture)}function Ge(e,t,n){var o;e instanceof pe&&(e=e.data.hook||(e.data.hook={}));var i=e[t];function l(){n.apply(this,arguments),w(o.fns,l)}r(i)?o=Ue([l]):a(i.fns)&&s(i.merged)?(o=i).fns.push(l):o=Ue([i,l]),o.merged=!0,e[t]=o}function We(e,t,n,o,i){if(a(t)){if(k(t,n))return e[n]=t[n],i||delete t[n],!0;if(k(t,o))return e[n]=t[o],i||delete t[o],!0}return!1}function Qe(e){return l(e)?[fe(e)]:i(e)?function e(t,n){var o,c,d,u,p=[];for(o=0;o<t.length;o++)r(c=t[o])||"boolean"==typeof c||(d=p.length-1,u=p[d],i(c)?c.length>0&&(Ye((c=e(c,"".concat(n||"","_").concat(o)))[0])&&Ye(u)&&(p[d]=fe(u.text+c[0].text),c.shift()),p.push.apply(p,c)):l(c)?Ye(u)?p[d]=fe(u.text+c):""!==c&&p.push(fe(c)):Ye(c)&&Ye(u)?p[d]=fe(u.text+c.text):(s(t._isVList)&&a(c.tag)&&r(c.key)&&a(n)&&(c.key="__vlist".concat(n,"_").concat(o,"__")),p.push(c)));return p}(e):void 0}function Ye(e){return a(e)&&a(e.text)&&!1===e.isComment}function Ke(e,t){var n,o,r,s,l=null;if(i(e)||"string"==typeof e)for(l=new Array(e.length),n=0,o=e.length;n<o;n++)l[n]=t(e[n],n);else if("number"==typeof e)for(l=new Array(e),n=0;n<e;n++)l[n]=t(n+1,n);else if(d(e))if(ce&&e[Symbol.iterator]){l=[];for(var c=e[Symbol.iterator](),u=c.next();!u.done;)l.push(t(u.value,l.length)),u=c.next()}else for(r=Object.keys(e),l=new Array(r.length),n=0,o=r.length;n<o;n++)s=r[n],l[n]=t(e[s],s,n);return a(l)||(l=[]),l._isVList=!0,l}function Xe(e,t,n,o){var i,r=this.$scopedSlots[e];r?(n=n||{},o&&(n=D(D({},o),n)),i=r(n)||(c(t)?t():t)):i=this.$slots[e]||(c(t)?t():t);var a=n&&n.slot;return a?this.$createElement("template",{slot:a},i):i}function Ze(e){return Rn(this.$options,"filters",e,!0)||q}function Je(e,t){return i(e)?-1===e.indexOf(t):e!==t}function et(e,t,n,o,i){var r=N.keyCodes[t]||n;return i&&o&&!N.keyCodes[t]?Je(i,o):r?Je(r,e):o?A(o)!==t:void 0===e}function tt(e,t,n,o,r){if(n)if(d(n)){i(n)&&(n=z(n));var a=void 0,s=function(i){if("class"===i||"style"===i||b(i))a=e;else{var s=e.attrs&&e.attrs.type;a=o||N.mustUseProp(t,s,i)?e.domProps||(e.domProps={}):e.attrs||(e.attrs={})}var l=C(i),c=A(i);l in a||c in a||(a[i]=n[i],r&&((e.on||(e.on={}))["update:".concat(i)]=function(e){n[i]=e}))};for(var l in n)s(l)}else;return e}function nt(e,t){var n=this._staticTrees||(this._staticTrees=[]),o=n[e];return o&&!t||it(o=n[e]=this.$options.staticRenderFns[e].call(this._renderProxy,this._c,this),"__static__".concat(e),!1),o}function ot(e,t,n){return it(e,"__once__".concat(t).concat(n?"_".concat(n):""),!0),e}function it(e,t,n){if(i(e))for(var o=0;o<e.length;o++)e[o]&&"string"!=typeof e[o]&&rt(e[o],"".concat(t,"_").concat(o),n);else rt(e,t,n)}function rt(e,t,n){e.isStatic=!0,e.key=t,e.isOnce=n}function at(e,t){if(t)if(p(t)){var n=e.on=e.on?D({},e.on):{};for(var o in t){var i=n[o],r=t[o];n[o]=i?[].concat(i,r):r}}else;return e}function st(e,t,n,o){t=t||{$stable:!n};for(var r=0;r<e.length;r++){var a=e[r];i(a)?st(a,t,n):a&&(a.proxy&&(a.fn.proxy=!0),t[a.key]=a.fn)}return o&&(t.$key=o),t}function lt(e,t){for(var n=0;n<t.length;n+=2){var o=t[n];"string"==typeof o&&o&&(e[t[n]]=t[n+1])}return e}function ct(e,t){return"string"==typeof e?t+e:e}function dt(e){e._o=ot,e._n=v,e._s=g,e._l=Ke,e._t=Xe,e._q=j,e._i=F,e._m=nt,e._f=Ze,e._k=et,e._b=tt,e._v=fe,e._e=he,e._u=st,e._g=at,e._d=lt,e._p=ct}function ut(e,t){if(!e||!e.length)return{};for(var n={},o=0,i=e.length;o<i;o++){var r=e[o],a=r.data;if(a&&a.attrs&&a.attrs.slot&&delete a.attrs.slot,r.context!==t&&r.fnContext!==t||!a||null==a.slot)(n.default||(n.default=[])).push(r);else{var s=a.slot,l=n[s]||(n[s]=[]);"template"===r.tag?l.push.apply(l,r.children||[]):l.push(r)}}for(var c in n)n[c].every(pt)&&delete n[c];return n}function pt(e){return e.isComment&&!e.asyncFactory||" "===e.text}function ht(e){return e.isComment&&e.asyncFactory}function ft(e,t,n,i){var r,a=Object.keys(n).length>0,s=t?!!t.$stable:!a,l=t&&t.$key;if(t){if(t._normalized)return t._normalized;if(s&&i&&i!==o&&l===i.$key&&!a&&!i.$hasNormal)return i;for(var c in r={},t)t[c]&&"$"!==c[0]&&(r[c]=mt(e,n,c,t[c]))}else r={};for(var d in n)d in r||(r[d]=gt(n,d));return t&&Object.isExtensible(t)&&(t._normalized=r),B(r,"$stable",s),B(r,"$key",l),B(r,"$hasNormal",a),r}function mt(e,t,n,o){var r=function(){var t=de;ue(e);var n=arguments.length?o.apply(null,arguments):o({}),r=(n=n&&"object"==typeof n&&!i(n)?[n]:Qe(n))&&n[0];return ue(t),n&&(!r||1===n.length&&r.isComment&&!ht(r))?void 0:n};return o.proxy&&Object.defineProperty(t,n,{get:r,enumerable:!0,configurable:!0}),r}function gt(e,t){return function(){return e[t]}}function vt(e){return{get attrs(){if(!e._attrsProxy){var t=e._attrsProxy={};B(t,"_v_attr_proxy",!0),yt(t,e.$attrs,o,e,"$attrs")}return e._attrsProxy},get listeners(){e._listenersProxy||yt(e._listenersProxy={},e.$listeners,o,e,"$listeners");return e._listenersProxy},get slots(){return function(e){e._slotsProxy||wt(e._slotsProxy={},e.$scopedSlots);return e._slotsProxy}(e)},emit:E(e.$emit,e),expose:function(t){t&&Object.keys(t).forEach((function(n){return Me(e,t,n)}))}}}function yt(e,t,n,o,i){var r=!1;for(var a in t)a in e?t[a]!==n[a]&&(r=!0):(r=!0,bt(e,a,o,i));for(var a in e)a in t||(r=!0,delete e[a]);return r}function bt(e,t,n,o){Object.defineProperty(e,t,{enumerable:!0,configurable:!0,get:function(){return n[o][t]}})}function wt(e,t){for(var n in t)e[n]=t[n];for(var n in e)n in t||delete e[n]}var xt=null;function kt(e,t){return(e.__esModule||ce&&"Module"===e[Symbol.toStringTag])&&(e=e.default),d(e)?t.extend(e):e}function _t(e){if(i(e))for(var t=0;t<e.length;t++){var n=e[t];if(a(n)&&(a(n.componentOptions)||ht(n)))return n}}function Tt(e,t,n,o,u,p){return(i(n)||l(n))&&(u=o,o=n,n=void 0),s(p)&&(u=2),function(e,t,n,o,l){if(a(n)&&a(n.__ob__))return he();a(n)&&a(n.is)&&(t=n.is);if(!t)return he();0;i(o)&&c(o[0])&&((n=n||{}).scopedSlots={default:o[0]},o.length=0);2===l?o=Qe(o):1===l&&(o=function(e){for(var t=0;t<e.length;t++)if(i(e[t]))return Array.prototype.concat.apply([],e);return e}(o));var u,p;if("string"==typeof t){var h=void 0;p=e.$vnode&&e.$vnode.ns||N.getTagNamespace(t),u=N.isReservedTag(t)?new pe(N.parsePlatformTagName(t),n,o,void 0,void 0,e):n&&n.pre||!a(h=Rn(e.$options,"components",t))?new pe(t,n,o,void 0,void 0,e):wn(h,n,e,o,t)}else u=wn(t,n,e,o);return i(u)?u:a(u)?(a(p)&&function e(t,n,o){t.ns=n,"foreignObject"===t.tag&&(n=void 0,o=!0);if(a(t.children))for(var i=0,l=t.children.length;i<l;i++){var c=t.children[i];a(c.tag)&&(r(c.ns)||s(o)&&"svg"!==c.tag)&&e(c,n,o)}}(u,p),a(n)&&function(e){d(e.style)&&Ht(e.style);d(e.class)&&Ht(e.class)}(n),u):he()}(e,t,n,o,u)}function Ct(e,t,n){we();try{if(t)for(var o=t;o=o.$parent;){var i=o.$options.errorCaptured;if(i)for(var r=0;r<i.length;r++)try{if(!1===i[r].call(o,e,t,n))return}catch(e){It(e,o,"errorCaptured hook")}}It(e,t,n)}finally{xe()}}function St(e,t,n,o,i){var r;try{(r=n?e.apply(t,n):e.call(t))&&!r._isVue&&m(r)&&!r._handled&&(r.catch((function(e){return Ct(e,o,i+" (Promise/async)")})),r._handled=!0)}catch(e){Ct(e,o,i)}return r}function It(e,t,n){if(N.errorHandler)try{return N.errorHandler.call(null,e,t,n)}catch(t){t!==e&&At(t,null,"config.errorHandler")}At(e,t,n)}function At(e,t,n){if(!Q||"undefined"==typeof console)throw e;console.error(e)}var Et,Rt=!1,Dt=[],zt=!1;function Ot(){zt=!1;var e=Dt.slice(0);Dt.length=0;for(var t=0;t<e.length;t++)e[t]()}if("undefined"!=typeof Promise&&se(Promise)){var Pt=Promise.resolve();Et=function(){Pt.then(Ot),J&&setTimeout(O)},Rt=!0}else if(K||"undefined"==typeof MutationObserver||!se(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Et="undefined"!=typeof setImmediate&&se(setImmediate)?function(){setImmediate(Ot)}:function(){setTimeout(Ot,0)};else{var qt=1,jt=new MutationObserver(Ot),Ft=document.createTextNode(String(qt));jt.observe(Ft,{characterData:!0}),Et=function(){qt=(qt+1)%2,Ft.data=String(qt)},Rt=!0}function Vt(e,t){var n;if(Dt.push((function(){if(e)try{e.call(t)}catch(e){Ct(e,t,"nextTick")}else n&&n(t)})),zt||(zt=!0,Et()),!e&&"undefined"!=typeof Promise)return new Promise((function(e){n=e}))}function Mt(e){return function(t,n){if(void 0===n&&(n=de),n)return function(e,t,n){var o=e.$options;o[t]=Sn(o[t],n)}(n,e,t)}}Mt("beforeMount"),Mt("mounted"),Mt("beforeUpdate"),Mt("updated"),Mt("beforeDestroy"),Mt("destroyed"),Mt("activated"),Mt("deactivated"),Mt("serverPrefetch"),Mt("renderTracked"),Mt("renderTriggered"),Mt("errorCaptured");var Lt=new le;function Ht(e){return function e(t,n){var o,r,a=i(t);if(!a&&!d(t)||t.__v_skip||Object.isFrozen(t)||t instanceof pe)return;if(t.__ob__){var s=t.__ob__.dep.id;if(n.has(s))return;n.add(s)}if(a)for(o=t.length;o--;)e(t[o],n);else if(Ve(t))e(t.value,n);else for(r=Object.keys(t),o=r.length;o--;)e(t[r[o]],n)}(e,Lt),Lt.clear(),e}var Nt,$t=0,Ut=function(){function e(e,t,n,o,i){var r,a;r=this,void 0===(a=Le&&!Le._vm?Le:e?e._scope:void 0)&&(a=Le),a&&a.active&&a.effects.push(r),(this.vm=e)&&i&&(e._watcher=this),o?(this.deep=!!o.deep,this.user=!!o.user,this.lazy=!!o.lazy,this.sync=!!o.sync,this.before=o.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=n,this.id=++$t,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new le,this.newDepIds=new le,this.expression="",c(t)?this.getter=t:(this.getter=function(e){if(!G.test(e)){var t=e.split(".");return function(e){for(var n=0;n<t.length;n++){if(!e)return;e=e[t[n]]}return e}}}(t),this.getter||(this.getter=O)),this.value=this.lazy?void 0:this.get()}return e.prototype.get=function(){var e;we(this);var t=this.vm;try{e=this.getter.call(t,t)}catch(e){if(!this.user)throw e;Ct(e,t,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&Ht(e),xe(),this.cleanupDeps()}return e},e.prototype.addDep=function(e){var t=e.id;this.newDepIds.has(t)||(this.newDepIds.add(t),this.newDeps.push(e),this.depIds.has(t)||e.addSub(this))},e.prototype.cleanupDeps=function(){for(var e=this.deps.length;e--;){var t=this.deps[e];this.newDepIds.has(t.id)||t.removeSub(this)}var n=this.depIds;this.depIds=this.newDepIds,this.newDepIds=n,this.newDepIds.clear(),n=this.deps,this.deps=this.newDeps,this.newDeps=n,this.newDeps.length=0},e.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():pn(this)},e.prototype.run=function(){if(this.active){var e=this.get();if(e!==this.value||d(e)||this.deep){var t=this.value;if(this.value=e,this.user){var n='callback for watcher "'.concat(this.expression,'"');St(this.cb,this.vm,[e,t],this.vm,n)}else this.cb.call(this.vm,e,t)}}},e.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},e.prototype.depend=function(){for(var e=this.deps.length;e--;)this.deps[e].depend()},e.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&w(this.vm._scope.effects,this),this.active){for(var e=this.deps.length;e--;)this.deps[e].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},e}();function Bt(e,t){Nt.$on(e,t)}function Gt(e,t){Nt.$off(e,t)}function Wt(e,t){var n=Nt;return function o(){var i=t.apply(null,arguments);null!==i&&n.$off(e,o)}}function Qt(e,t,n){Nt=e,Be(t,n||{},Bt,Gt,Wt,e),Nt=void 0}var Yt=null;function Kt(e){var t=Yt;return Yt=e,function(){Yt=t}}function Xt(e){for(;e&&(e=e.$parent);)if(e._inactive)return!0;return!1}function Zt(e,t){if(t){if(e._directInactive=!1,Xt(e))return}else if(e._directInactive)return;if(e._inactive||null===e._inactive){e._inactive=!1;for(var n=0;n<e.$children.length;n++)Zt(e.$children[n]);Jt(e,"activated")}}function Jt(e,t,n,o){void 0===o&&(o=!0),we();var i=de;o&&ue(e);var r=e.$options[t],a="".concat(t," hook");if(r)for(var s=0,l=r.length;s<l;s++)St(r[s],e,n||null,e,a);e._hasHookEvent&&e.$emit("hook:"+t),o&&ue(i),xe()}var en=[],tn=[],nn={},on=!1,rn=!1,an=0;var sn=0,ln=Date.now;if(Q&&!K){var cn=window.performance;cn&&"function"==typeof cn.now&&ln()>document.createEvent("Event").timeStamp&&(ln=function(){return cn.now()})}var dn=function(e,t){if(e.post){if(!t.post)return 1}else if(t.post)return-1;return e.id-t.id};function un(){var e,t;for(sn=ln(),rn=!0,en.sort(dn),an=0;an<en.length;an++)(e=en[an]).before&&e.before(),t=e.id,nn[t]=null,e.run();var n=tn.slice(),o=en.slice();an=en.length=tn.length=0,nn={},on=rn=!1,function(e){for(var t=0;t<e.length;t++)e[t]._inactive=!0,Zt(e[t],!0)}(n),function(e){var t=e.length;for(;t--;){var n=e[t],o=n.vm;o&&o._watcher===n&&o._isMounted&&!o._isDestroyed&&Jt(o,"updated")}}(o),function(){for(var e=0;e<ve.length;e++){var t=ve[e];t.subs=t.subs.filter((function(e){return e})),t._pending=!1}ve.length=0}(),ae&&N.devtools&&ae.emit("flush")}function pn(e){var t=e.id;if(null==nn[t]&&(e!==ye.target||!e.noRecurse)){if(nn[t]=!0,rn){for(var n=en.length-1;n>an&&en[n].id>e.id;)n--;en.splice(n+1,0,e)}else en.push(e);on||(on=!0,Vt(un))}}function hn(e,t){if(e){for(var n=Object.create(null),o=ce?Reflect.ownKeys(e):Object.keys(e),i=0;i<o.length;i++){var r=o[i];if("__ob__"!==r){var a=e[r].from;if(a in t._provided)n[r]=t._provided[a];else if("default"in e[r]){var s=e[r].default;n[r]=c(s)?s.call(t):s}else 0}}return n}}function fn(e,t,n,r,a){var l,c=this,d=a.options;k(r,"_uid")?(l=Object.create(r))._original=r:(l=r,r=r._original);var u=s(d._compiled),p=!u;this.data=e,this.props=t,this.children=n,this.parent=r,this.listeners=e.on||o,this.injections=hn(d.inject,r),this.slots=function(){return c.$slots||ft(r,e.scopedSlots,c.$slots=ut(n,r)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return ft(r,e.scopedSlots,this.slots())}}),u&&(this.$options=d,this.$slots=this.slots(),this.$scopedSlots=ft(r,e.scopedSlots,this.$slots)),d._scopeId?this._c=function(e,t,n,o){var a=Tt(l,e,t,n,o,p);return a&&!i(a)&&(a.fnScopeId=d._scopeId,a.fnContext=r),a}:this._c=function(e,t,n,o){return Tt(l,e,t,n,o,p)}}function mn(e,t,n,o,i){var r=me(e);return r.fnContext=n,r.fnOptions=o,t.slot&&((r.data||(r.data={})).slot=t.slot),r}function gn(e,t){for(var n in t)e[C(n)]=t[n]}function vn(e){return e.name||e.__name||e._componentTag}dt(fn.prototype);var yn={init:function(e,t){if(e.componentInstance&&!e.componentInstance._isDestroyed&&e.data.keepAlive){var n=e;yn.prepatch(n,n)}else{(e.componentInstance=function(e,t){var n={_isComponent:!0,_parentVnode:e,parent:t},o=e.data.inlineTemplate;a(o)&&(n.render=o.render,n.staticRenderFns=o.staticRenderFns);return new e.componentOptions.Ctor(n)}(e,Yt)).$mount(t?e.elm:void 0,t)}},prepatch:function(e,t){var n=t.componentOptions;!function(e,t,n,i,r){var a=i.data.scopedSlots,s=e.$scopedSlots,l=!!(a&&!a.$stable||s!==o&&!s.$stable||a&&e.$scopedSlots.$key!==a.$key||!a&&e.$scopedSlots.$key),c=!!(r||e.$options._renderChildren||l),d=e.$vnode;e.$options._parentVnode=i,e.$vnode=i,e._vnode&&(e._vnode.parent=i),e.$options._renderChildren=r;var u=i.data.attrs||o;e._attrsProxy&&yt(e._attrsProxy,u,d.data&&d.data.attrs||o,e,"$attrs")&&(c=!0),e.$attrs=u,n=n||o;var p=e.$options._parentListeners;if(e._listenersProxy&&yt(e._listenersProxy,n,p||o,e,"$listeners"),e.$listeners=e.$options._parentListeners=n,Qt(e,n,p),t&&e.$options.props){Ie(!1);for(var h=e._props,f=e.$options._propKeys||[],m=0;m<f.length;m++){var g=f[m],v=e.$options.props;h[g]=Dn(g,v,t,e)}Ie(!0),e.$options.propsData=t}c&&(e.$slots=ut(r,i.context),e.$forceUpdate())}(t.componentInstance=e.componentInstance,n.propsData,n.listeners,t,n.children)},insert:function(e){var t,n=e.context,o=e.componentInstance;o._isMounted||(o._isMounted=!0,Jt(o,"mounted")),e.data.keepAlive&&(n._isMounted?((t=o)._inactive=!1,tn.push(t)):Zt(o,!0))},destroy:function(e){var t=e.componentInstance;t._isDestroyed||(e.data.keepAlive?function e(t,n){if(!(n&&(t._directInactive=!0,Xt(t))||t._inactive)){t._inactive=!0;for(var o=0;o<t.$children.length;o++)e(t.$children[o]);Jt(t,"deactivated")}}(t,!0):t.$destroy())}},bn=Object.keys(yn);function wn(e,t,n,l,c){if(!r(e)){var u=n.$options._base;if(d(e)&&(e=u.extend(e)),"function"==typeof e){var p;if(r(e.cid)&&void 0===(e=function(e,t){if(s(e.error)&&a(e.errorComp))return e.errorComp;if(a(e.resolved))return e.resolved;var n=xt;if(n&&a(e.owners)&&-1===e.owners.indexOf(n)&&e.owners.push(n),s(e.loading)&&a(e.loadingComp))return e.loadingComp;if(n&&!a(e.owners)){var o=e.owners=[n],i=!0,l=null,c=null;n.$on("hook:destroyed",(function(){return w(o,n)}));var u=function(e){for(var t=0,n=o.length;t<n;t++)o[t].$forceUpdate();e&&(o.length=0,null!==l&&(clearTimeout(l),l=null),null!==c&&(clearTimeout(c),c=null))},p=V((function(n){e.resolved=kt(n,t),i?o.length=0:u(!0)})),h=V((function(t){a(e.errorComp)&&(e.error=!0,u(!0))})),f=e(p,h);return d(f)&&(m(f)?r(e.resolved)&&f.then(p,h):m(f.component)&&(f.component.then(p,h),a(f.error)&&(e.errorComp=kt(f.error,t)),a(f.loading)&&(e.loadingComp=kt(f.loading,t),0===f.delay?e.loading=!0:l=setTimeout((function(){l=null,r(e.resolved)&&r(e.error)&&(e.loading=!0,u(!1))}),f.delay||200)),a(f.timeout)&&(c=setTimeout((function(){c=null,r(e.resolved)&&h(null)}),f.timeout)))),i=!1,e.loading?e.loadingComp:e.resolved}}(p=e,u)))return function(e,t,n,o,i){var r=he();return r.asyncFactory=e,r.asyncMeta={data:t,context:n,children:o,tag:i},r}(p,t,n,l,c);t=t||{},Bn(e),a(t.model)&&function(e,t){var n=e.model&&e.model.prop||"value",o=e.model&&e.model.event||"input";(t.attrs||(t.attrs={}))[n]=t.model.value;var r=t.on||(t.on={}),s=r[o],l=t.model.callback;a(s)?(i(s)?-1===s.indexOf(l):s!==l)&&(r[o]=[l].concat(s)):r[o]=l}(e.options,t);var h=function(e,t,n){var o=t.options.props;if(!r(o)){var i={},s=e.attrs,l=e.props;if(a(s)||a(l))for(var c in o){var d=A(c);We(i,l,c,d,!0)||We(i,s,c,d,!1)}return i}}(t,e);if(s(e.options.functional))return function(e,t,n,r,s){var l=e.options,c={},d=l.props;if(a(d))for(var u in d)c[u]=Dn(u,d,t||o);else a(n.attrs)&&gn(c,n.attrs),a(n.props)&&gn(c,n.props);var p=new fn(n,c,s,r,e),h=l.render.call(null,p._c,p);if(h instanceof pe)return mn(h,n,p.parent,l,p);if(i(h)){for(var f=Qe(h)||[],m=new Array(f.length),g=0;g<f.length;g++)m[g]=mn(f[g],n,p.parent,l,p);return m}}(e,h,t,n,l);var f=t.on;if(t.on=t.nativeOn,s(e.options.abstract)){var g=t.slot;t={},g&&(t.slot=g)}!function(e){for(var t=e.hook||(e.hook={}),n=0;n<bn.length;n++){var o=bn[n],i=t[o],r=yn[o];i===r||i&&i._merged||(t[o]=i?xn(r,i):r)}}(t);var v=vn(e.options)||c;return new pe("vue-component-".concat(e.cid).concat(v?"-".concat(v):""),t,void 0,void 0,void 0,n,{Ctor:e,propsData:h,listeners:f,tag:c,children:l},p)}}}function xn(e,t){var n=function(n,o){e(n,o),t(n,o)};return n._merged=!0,n}var kn=O,_n=N.optionMergeStrategies;function Tn(e,t,n){if(void 0===n&&(n=!0),!t)return e;for(var o,i,r,a=ce?Reflect.ownKeys(t):Object.keys(t),s=0;s<a.length;s++)"__ob__"!==(o=a[s])&&(i=e[o],r=t[o],n&&k(e,o)?i!==r&&p(i)&&p(r)&&Tn(i,r):ze(e,o,r));return e}function Cn(e,t,n){return n?function(){var o=c(t)?t.call(n,n):t,i=c(e)?e.call(n,n):e;return o?Tn(o,i):i}:t?e?function(){return Tn(c(t)?t.call(this,this):t,c(e)?e.call(this,this):e)}:t:e}function Sn(e,t){var n=t?e?e.concat(t):i(t)?t:[t]:e;return n?function(e){for(var t=[],n=0;n<e.length;n++)-1===t.indexOf(e[n])&&t.push(e[n]);return t}(n):n}function In(e,t,n,o){var i=Object.create(e||null);return t?D(i,t):i}_n.data=function(e,t,n){return n?Cn(e,t,n):t&&"function"!=typeof t?e:Cn(e,t)},H.forEach((function(e){_n[e]=Sn})),L.forEach((function(e){_n[e+"s"]=In})),_n.watch=function(e,t,n,o){if(e===ne&&(e=void 0),t===ne&&(t=void 0),!t)return Object.create(e||null);if(!e)return t;var r={};for(var a in D(r,e),t){var s=r[a],l=t[a];s&&!i(s)&&(s=[s]),r[a]=s?s.concat(l):i(l)?l:[l]}return r},_n.props=_n.methods=_n.inject=_n.computed=function(e,t,n,o){if(!e)return t;var i=Object.create(null);return D(i,e),t&&D(i,t),i},_n.provide=function(e,t){return e?function(){var n=Object.create(null);return Tn(n,c(e)?e.call(this):e),t&&Tn(n,c(t)?t.call(this):t,!1),n}:t};var An=function(e,t){return void 0===t?e:t};function En(e,t,n){if(c(t)&&(t=t.options),function(e,t){var n=e.props;if(n){var o,r,a={};if(i(n))for(o=n.length;o--;)"string"==typeof(r=n[o])&&(a[C(r)]={type:null});else if(p(n))for(var s in n)r=n[s],a[C(s)]=p(r)?r:{type:r};else 0;e.props=a}}(t),function(e,t){var n=e.inject;if(n){var o=e.inject={};if(i(n))for(var r=0;r<n.length;r++)o[n[r]]={from:n[r]};else if(p(n))for(var a in n){var s=n[a];o[a]=p(s)?D({from:a},s):{from:s}}else 0}}(t),function(e){var t=e.directives;if(t)for(var n in t){var o=t[n];c(o)&&(t[n]={bind:o,update:o})}}(t),!t._base&&(t.extends&&(e=En(e,t.extends,n)),t.mixins))for(var o=0,r=t.mixins.length;o<r;o++)e=En(e,t.mixins[o],n);var a,s={};for(a in e)l(a);for(a in t)k(e,a)||l(a);function l(o){var i=_n[o]||An;s[o]=i(e[o],t[o],n,o)}return s}function Rn(e,t,n,o){if("string"==typeof n){var i=e[t];if(k(i,n))return i[n];var r=C(n);if(k(i,r))return i[r];var a=S(r);return k(i,a)?i[a]:i[n]||i[r]||i[a]}}function Dn(e,t,n,o){var i=t[e],r=!k(n,e),a=n[e],s=qn(Boolean,i.type);if(s>-1)if(r&&!k(i,"default"))a=!1;else if(""===a||a===A(e)){var l=qn(String,i.type);(l<0||s<l)&&(a=!0)}if(void 0===a){a=function(e,t,n){if(!k(t,"default"))return;var o=t.default;0;if(e&&e.$options.propsData&&void 0===e.$options.propsData[n]&&void 0!==e._props[n])return e._props[n];return c(o)&&"Function"!==On(t.type)?o.call(e):o}(o,i,e);var d=Se;Ie(!0),Re(a),Ie(d)}return a}var zn=/^\s*function (\w+)/;function On(e){var t=e&&e.toString().match(zn);return t?t[1]:""}function Pn(e,t){return On(e)===On(t)}function qn(e,t){if(!i(t))return Pn(t,e)?0:-1;for(var n=0,o=t.length;n<o;n++)if(Pn(t[n],e))return n;return-1}var jn={enumerable:!0,configurable:!0,get:O,set:O};function Fn(e,t,n){jn.get=function(){return this[t][n]},jn.set=function(e){this[t][n]=e},Object.defineProperty(e,n,jn)}function Vn(e){var t=e.$options;if(t.props&&function(e,t){var n=e.$options.propsData||{},o=e._props=qe({}),i=e.$options._propKeys=[];e.$parent&&Ie(!1);var r=function(r){i.push(r);var a=Dn(r,t,n,e);De(o,r,a),r in e||Fn(e,"_props",r)};for(var a in t)r(a);Ie(!0)}(e,t.props),function(e){var t=e.$options,n=t.setup;if(n){var o=e._setupContext=vt(e);ue(e),we();var i=St(n,null,[e._props||qe({}),o],e,"setup");if(xe(),ue(),c(i))t.render=i;else if(d(i))if(e._setupState=i,i.__sfc){var r=e._setupProxy={};for(var a in i)"__sfc"!==a&&Me(r,i,a)}else for(var a in i)U(a)||Me(e,i,a);else 0}}(e),t.methods&&function(e,t){e.$options.props;for(var n in t)e[n]="function"!=typeof t[n]?O:E(t[n],e)}(e,t.methods),t.data)!function(e){var t=e.$options.data;p(t=e._data=c(t)?function(e,t){we();try{return e.call(t,t)}catch(e){return Ct(e,t,"data()"),{}}finally{xe()}}(t,e):t||{})||(t={});var n=Object.keys(t),o=e.$options.props,i=(e.$options.methods,n.length);for(;i--;){var r=n[i];0,o&&k(o,r)||U(r)||Fn(e,"_data",r)}var a=Re(t);a&&a.vmCount++}(e);else{var n=Re(e._data={});n&&n.vmCount++}t.computed&&function(e,t){var n=e._computedWatchers=Object.create(null),o=re();for(var i in t){var r=t[i],a=c(r)?r:r.get;0,o||(n[i]=new Ut(e,a||O,O,Mn)),i in e||Ln(e,i,r)}}(e,t.computed),t.watch&&t.watch!==ne&&function(e,t){for(var n in t){var o=t[n];if(i(o))for(var r=0;r<o.length;r++)$n(e,n,o[r]);else $n(e,n,o)}}(e,t.watch)}var Mn={lazy:!0};function Ln(e,t,n){var o=!re();c(n)?(jn.get=o?Hn(t):Nn(n),jn.set=O):(jn.get=n.get?o&&!1!==n.cache?Hn(t):Nn(n.get):O,jn.set=n.set||O),Object.defineProperty(e,t,jn)}function Hn(e){return function(){var t=this._computedWatchers&&this._computedWatchers[e];if(t)return t.dirty&&t.evaluate(),ye.target&&t.depend(),t.value}}function Nn(e){return function(){return e.call(this,this)}}function $n(e,t,n,o){return p(n)&&(o=n,n=n.handler),"string"==typeof n&&(n=e[n]),e.$watch(t,n,o)}var Un=0;function Bn(e){var t=e.options;if(e.super){var n=Bn(e.super);if(n!==e.superOptions){e.superOptions=n;var o=function(e){var t,n=e.options,o=e.sealedOptions;for(var i in n)n[i]!==o[i]&&(t||(t={}),t[i]=n[i]);return t}(e);o&&D(e.extendOptions,o),(t=e.options=En(n,e.extendOptions)).name&&(t.components[t.name]=e)}}return t}function Gn(e){this._init(e)}function Wn(e){e.cid=0;var t=1;e.extend=function(e){e=e||{};var n=this,o=n.cid,i=e._Ctor||(e._Ctor={});if(i[o])return i[o];var r=vn(e)||vn(n.options);var a=function(e){this._init(e)};return(a.prototype=Object.create(n.prototype)).constructor=a,a.cid=t++,a.options=En(n.options,e),a.super=n,a.options.props&&function(e){var t=e.options.props;for(var n in t)Fn(e.prototype,"_props",n)}(a),a.options.computed&&function(e){var t=e.options.computed;for(var n in t)Ln(e.prototype,n,t[n])}(a),a.extend=n.extend,a.mixin=n.mixin,a.use=n.use,L.forEach((function(e){a[e]=n[e]})),r&&(a.options.components[r]=a),a.superOptions=n.options,a.extendOptions=e,a.sealedOptions=D({},a.options),i[o]=a,a}}function Qn(e){return e&&(vn(e.Ctor.options)||e.tag)}function Yn(e,t){return i(e)?e.indexOf(t)>-1:"string"==typeof e?e.split(",").indexOf(t)>-1:!!h(e)&&e.test(t)}function Kn(e,t){var n=e.cache,o=e.keys,i=e._vnode;for(var r in n){var a=n[r];if(a){var s=a.name;s&&!t(s)&&Xn(n,r,o,i)}}}function Xn(e,t,n,o){var i=e[t];!i||o&&i.tag===o.tag||i.componentInstance.$destroy(),e[t]=null,w(n,t)}!function(e){e.prototype._init=function(e){var t=this;t._uid=Un++,t._isVue=!0,t.__v_skip=!0,t._scope=new He(!0),t._scope._vm=!0,e&&e._isComponent?function(e,t){var n=e.$options=Object.create(e.constructor.options),o=t._parentVnode;n.parent=t.parent,n._parentVnode=o;var i=o.componentOptions;n.propsData=i.propsData,n._parentListeners=i.listeners,n._renderChildren=i.children,n._componentTag=i.tag,t.render&&(n.render=t.render,n.staticRenderFns=t.staticRenderFns)}(t,e):t.$options=En(Bn(t.constructor),e||{},t),t._renderProxy=t,t._self=t,function(e){var t=e.$options,n=t.parent;if(n&&!t.abstract){for(;n.$options.abstract&&n.$parent;)n=n.$parent;n.$children.push(e)}e.$parent=n,e.$root=n?n.$root:e,e.$children=[],e.$refs={},e._provided=n?n._provided:Object.create(null),e._watcher=null,e._inactive=null,e._directInactive=!1,e._isMounted=!1,e._isDestroyed=!1,e._isBeingDestroyed=!1}(t),function(e){e._events=Object.create(null),e._hasHookEvent=!1;var t=e.$options._parentListeners;t&&Qt(e,t)}(t),function(e){e._vnode=null,e._staticTrees=null;var t=e.$options,n=e.$vnode=t._parentVnode,i=n&&n.context;e.$slots=ut(t._renderChildren,i),e.$scopedSlots=n?ft(e.$parent,n.data.scopedSlots,e.$slots):o,e._c=function(t,n,o,i){return Tt(e,t,n,o,i,!1)},e.$createElement=function(t,n,o,i){return Tt(e,t,n,o,i,!0)};var r=n&&n.data;De(e,"$attrs",r&&r.attrs||o,null,!0),De(e,"$listeners",t._parentListeners||o,null,!0)}(t),Jt(t,"beforeCreate",void 0,!1),function(e){var t=hn(e.$options.inject,e);t&&(Ie(!1),Object.keys(t).forEach((function(n){De(e,n,t[n])})),Ie(!0))}(t),Vn(t),function(e){var t=e.$options.provide;if(t){var n=c(t)?t.call(e):t;if(!d(n))return;for(var o=Ne(e),i=ce?Reflect.ownKeys(n):Object.keys(n),r=0;r<i.length;r++){var a=i[r];Object.defineProperty(o,a,Object.getOwnPropertyDescriptor(n,a))}}}(t),Jt(t,"created"),t.$options.el&&t.$mount(t.$options.el)}}(Gn),function(e){var t={get:function(){return this._data}},n={get:function(){return this._props}};Object.defineProperty(e.prototype,"$data",t),Object.defineProperty(e.prototype,"$props",n),e.prototype.$set=ze,e.prototype.$delete=Oe,e.prototype.$watch=function(e,t,n){if(p(t))return $n(this,e,t,n);(n=n||{}).user=!0;var o=new Ut(this,e,t,n);if(n.immediate){var i='callback for immediate watcher "'.concat(o.expression,'"');we(),St(t,this,[o.value],this,i),xe()}return function(){o.teardown()}}}(Gn),function(e){var t=/^hook:/;e.prototype.$on=function(e,n){var o=this;if(i(e))for(var r=0,a=e.length;r<a;r++)o.$on(e[r],n);else(o._events[e]||(o._events[e]=[])).push(n),t.test(e)&&(o._hasHookEvent=!0);return o},e.prototype.$once=function(e,t){var n=this;function o(){n.$off(e,o),t.apply(n,arguments)}return o.fn=t,n.$on(e,o),n},e.prototype.$off=function(e,t){var n=this;if(!arguments.length)return n._events=Object.create(null),n;if(i(e)){for(var o=0,r=e.length;o<r;o++)n.$off(e[o],t);return n}var a,s=n._events[e];if(!s)return n;if(!t)return n._events[e]=null,n;for(var l=s.length;l--;)if((a=s[l])===t||a.fn===t){s.splice(l,1);break}return n},e.prototype.$emit=function(e){var t=this,n=t._events[e];if(n){n=n.length>1?R(n):n;for(var o=R(arguments,1),i='event handler for "'.concat(e,'"'),r=0,a=n.length;r<a;r++)St(n[r],t,o,t,i)}return t}}(Gn),function(e){e.prototype._update=function(e,t){var n=this,o=n.$el,i=n._vnode,r=Kt(n);n._vnode=e,n.$el=i?n.__patch__(i,e):n.__patch__(n.$el,e,t,!1),r(),o&&(o.__vue__=null),n.$el&&(n.$el.__vue__=n);for(var a=n;a&&a.$vnode&&a.$parent&&a.$vnode===a.$parent._vnode;)a.$parent.$el=a.$el,a=a.$parent},e.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},e.prototype.$destroy=function(){var e=this;if(!e._isBeingDestroyed){Jt(e,"beforeDestroy"),e._isBeingDestroyed=!0;var t=e.$parent;!t||t._isBeingDestroyed||e.$options.abstract||w(t.$children,e),e._scope.stop(),e._data.__ob__&&e._data.__ob__.vmCount--,e._isDestroyed=!0,e.__patch__(e._vnode,null),Jt(e,"destroyed"),e.$off(),e.$el&&(e.$el.__vue__=null),e.$vnode&&(e.$vnode.parent=null)}}}(Gn),function(e){dt(e.prototype),e.prototype.$nextTick=function(e){return Vt(e,this)},e.prototype._render=function(){var e,t=this,n=t.$options,o=n.render,r=n._parentVnode;r&&t._isMounted&&(t.$scopedSlots=ft(t.$parent,r.data.scopedSlots,t.$slots,t.$scopedSlots),t._slotsProxy&&wt(t._slotsProxy,t.$scopedSlots)),t.$vnode=r;try{ue(t),xt=t,e=o.call(t._renderProxy,t.$createElement)}catch(n){Ct(n,t,"render"),e=t._vnode}finally{xt=null,ue()}return i(e)&&1===e.length&&(e=e[0]),e instanceof pe||(e=he()),e.parent=r,e}}(Gn);var Zn=[String,RegExp,Array],Jn={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Zn,exclude:Zn,max:[String,Number]},methods:{cacheVNode:function(){var e=this.cache,t=this.keys,n=this.vnodeToCache,o=this.keyToCache;if(n){var i=n.tag,r=n.componentInstance,a=n.componentOptions;e[o]={name:Qn(a),tag:i,componentInstance:r},t.push(o),this.max&&t.length>parseInt(this.max)&&Xn(e,t[0],t,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var e in this.cache)Xn(this.cache,e,this.keys)},mounted:function(){var e=this;this.cacheVNode(),this.$watch("include",(function(t){Kn(e,(function(e){return Yn(t,e)}))})),this.$watch("exclude",(function(t){Kn(e,(function(e){return!Yn(t,e)}))}))},updated:function(){this.cacheVNode()},render:function(){var e=this.$slots.default,t=_t(e),n=t&&t.componentOptions;if(n){var o=Qn(n),i=this.include,r=this.exclude;if(i&&(!o||!Yn(i,o))||r&&o&&Yn(r,o))return t;var a=this.cache,s=this.keys,l=null==t.key?n.Ctor.cid+(n.tag?"::".concat(n.tag):""):t.key;a[l]?(t.componentInstance=a[l].componentInstance,w(s,l),s.push(l)):(this.vnodeToCache=t,this.keyToCache=l),t.data.keepAlive=!0}return t||e&&e[0]}}};!function(e){var t={get:function(){return N}};Object.defineProperty(e,"config",t),e.util={warn:kn,extend:D,mergeOptions:En,defineReactive:De},e.set=ze,e.delete=Oe,e.nextTick=Vt,e.observable=function(e){return Re(e),e},e.options=Object.create(null),L.forEach((function(t){e.options[t+"s"]=Object.create(null)})),e.options._base=e,D(e.options.components,Jn),function(e){e.use=function(e){var t=this._installedPlugins||(this._installedPlugins=[]);if(t.indexOf(e)>-1)return this;var n=R(arguments,1);return n.unshift(this),c(e.install)?e.install.apply(e,n):c(e)&&e.apply(null,n),t.push(e),this}}(e),function(e){e.mixin=function(e){return this.options=En(this.options,e),this}}(e),Wn(e),function(e){L.forEach((function(t){e[t]=function(e,n){return n?("component"===t&&p(n)&&(n.name=n.name||e,n=this.options._base.extend(n)),"directive"===t&&c(n)&&(n={bind:n,update:n}),this.options[t+"s"][e]=n,n):this.options[t+"s"][e]}}))}(e)}(Gn),Object.defineProperty(Gn.prototype,"$isServer",{get:re}),Object.defineProperty(Gn.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Gn,"FunctionalRenderContext",{value:fn}),Gn.version="2.7.14";var eo=y("style,class"),to=y("input,textarea,option,select,progress"),no=y("contenteditable,draggable,spellcheck"),oo=y("events,caret,typing,plaintext-only"),io=y("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),ro="http://www.w3.org/1999/xlink",ao=function(e){return":"===e.charAt(5)&&"xlink"===e.slice(0,5)},so=function(e){return ao(e)?e.slice(6,e.length):""},lo=function(e){return null==e||!1===e};function co(e){for(var t=e.data,n=e,o=e;a(o.componentInstance);)(o=o.componentInstance._vnode)&&o.data&&(t=uo(o.data,t));for(;a(n=n.parent);)n&&n.data&&(t=uo(t,n.data));return function(e,t){if(a(e)||a(t))return po(e,ho(t));return""}(t.staticClass,t.class)}function uo(e,t){return{staticClass:po(e.staticClass,t.staticClass),class:a(e.class)?[e.class,t.class]:t.class}}function po(e,t){return e?t?e+" "+t:e:t||""}function ho(e){return Array.isArray(e)?function(e){for(var t,n="",o=0,i=e.length;o<i;o++)a(t=ho(e[o]))&&""!==t&&(n&&(n+=" "),n+=t);return n}(e):d(e)?function(e){var t="";for(var n in e)e[n]&&(t&&(t+=" "),t+=n);return t}(e):"string"==typeof e?e:""}var fo={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},mo=y("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),go=y("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),vo=function(e){return mo(e)||go(e)};var yo=Object.create(null);var bo=y("text,number,password,search,email,tel,url");var wo=Object.freeze({__proto__:null,createElement:function(e,t){var n=document.createElement(e);return"select"!==e||t.data&&t.data.attrs&&void 0!==t.data.attrs.multiple&&n.setAttribute("multiple","multiple"),n},createElementNS:function(e,t){return document.createElementNS(fo[e],t)},createTextNode:function(e){return document.createTextNode(e)},createComment:function(e){return document.createComment(e)},insertBefore:function(e,t,n){e.insertBefore(t,n)},removeChild:function(e,t){e.removeChild(t)},appendChild:function(e,t){e.appendChild(t)},parentNode:function(e){return e.parentNode},nextSibling:function(e){return e.nextSibling},tagName:function(e){return e.tagName},setTextContent:function(e,t){e.textContent=t},setStyleScope:function(e,t){e.setAttribute(t,"")}}),xo={create:function(e,t){ko(t)},update:function(e,t){e.data.ref!==t.data.ref&&(ko(e,!0),ko(t))},destroy:function(e){ko(e,!0)}};function ko(e,t){var n=e.data.ref;if(a(n)){var o=e.context,r=e.componentInstance||e.elm,s=t?null:r,l=t?void 0:r;if(c(n))St(n,o,[s],o,"template ref function");else{var d=e.data.refInFor,u="string"==typeof n||"number"==typeof n,p=Ve(n),h=o.$refs;if(u||p)if(d){var f=u?h[n]:n.value;t?i(f)&&w(f,r):i(f)?f.includes(r)||f.push(r):u?(h[n]=[r],_o(o,n,h[n])):n.value=[r]}else if(u){if(t&&h[n]!==r)return;h[n]=l,_o(o,n,s)}else if(p){if(t&&n.value!==r)return;n.value=s}else 0}}}function _o(e,t,n){var o=e._setupState;o&&k(o,t)&&(Ve(o[t])?o[t].value=n:o[t]=n)}var To=new pe("",{},[]),Co=["create","activate","update","remove","destroy"];function So(e,t){return e.key===t.key&&e.asyncFactory===t.asyncFactory&&(e.tag===t.tag&&e.isComment===t.isComment&&a(e.data)===a(t.data)&&function(e,t){if("input"!==e.tag)return!0;var n,o=a(n=e.data)&&a(n=n.attrs)&&n.type,i=a(n=t.data)&&a(n=n.attrs)&&n.type;return o===i||bo(o)&&bo(i)}(e,t)||s(e.isAsyncPlaceholder)&&r(t.asyncFactory.error))}function Io(e,t,n){var o,i,r={};for(o=t;o<=n;++o)a(i=e[o].key)&&(r[i]=o);return r}var Ao={create:Eo,update:Eo,destroy:function(e){Eo(e,To)}};function Eo(e,t){(e.data.directives||t.data.directives)&&function(e,t){var n,o,i,r=e===To,a=t===To,s=Do(e.data.directives,e.context),l=Do(t.data.directives,t.context),c=[],d=[];for(n in l)o=s[n],i=l[n],o?(i.oldValue=o.value,i.oldArg=o.arg,Oo(i,"update",t,e),i.def&&i.def.componentUpdated&&d.push(i)):(Oo(i,"bind",t,e),i.def&&i.def.inserted&&c.push(i));if(c.length){var u=function(){for(var n=0;n<c.length;n++)Oo(c[n],"inserted",t,e)};r?Ge(t,"insert",u):u()}d.length&&Ge(t,"postpatch",(function(){for(var n=0;n<d.length;n++)Oo(d[n],"componentUpdated",t,e)}));if(!r)for(n in s)l[n]||Oo(s[n],"unbind",e,e,a)}(e,t)}var Ro=Object.create(null);function Do(e,t){var n,o,i=Object.create(null);if(!e)return i;for(n=0;n<e.length;n++){if((o=e[n]).modifiers||(o.modifiers=Ro),i[zo(o)]=o,t._setupState&&t._setupState.__sfc){var r=o.def||Rn(t,"_setupState","v-"+o.name);o.def="function"==typeof r?{bind:r,update:r}:r}o.def=o.def||Rn(t.$options,"directives",o.name)}return i}function zo(e){return e.rawName||"".concat(e.name,".").concat(Object.keys(e.modifiers||{}).join("."))}function Oo(e,t,n,o,i){var r=e.def&&e.def[t];if(r)try{r(n.elm,e,n,o,i)}catch(o){Ct(o,n.context,"directive ".concat(e.name," ").concat(t," hook"))}}var Po=[xo,Ao];function qo(e,t){var n=t.componentOptions;if(!(a(n)&&!1===n.Ctor.options.inheritAttrs||r(e.data.attrs)&&r(t.data.attrs))){var o,i,l=t.elm,c=e.data.attrs||{},d=t.data.attrs||{};for(o in(a(d.__ob__)||s(d._v_attr_proxy))&&(d=t.data.attrs=D({},d)),d)i=d[o],c[o]!==i&&jo(l,o,i,t.data.pre);for(o in(K||Z)&&d.value!==c.value&&jo(l,"value",d.value),c)r(d[o])&&(ao(o)?l.removeAttributeNS(ro,so(o)):no(o)||l.removeAttribute(o))}}function jo(e,t,n,o){o||e.tagName.indexOf("-")>-1?Fo(e,t,n):io(t)?lo(n)?e.removeAttribute(t):(n="allowfullscreen"===t&&"EMBED"===e.tagName?"true":t,e.setAttribute(t,n)):no(t)?e.setAttribute(t,function(e,t){return lo(t)||"false"===t?"false":"contenteditable"===e&&oo(t)?t:"true"}(t,n)):ao(t)?lo(n)?e.removeAttributeNS(ro,so(t)):e.setAttributeNS(ro,t,n):Fo(e,t,n)}function Fo(e,t,n){if(lo(n))e.removeAttribute(t);else{if(K&&!X&&"TEXTAREA"===e.tagName&&"placeholder"===t&&""!==n&&!e.__ieph){var o=function(t){t.stopImmediatePropagation(),e.removeEventListener("input",o)};e.addEventListener("input",o),e.__ieph=!0}e.setAttribute(t,n)}}var Vo={create:qo,update:qo};function Mo(e,t){var n=t.elm,o=t.data,i=e.data;if(!(r(o.staticClass)&&r(o.class)&&(r(i)||r(i.staticClass)&&r(i.class)))){var s=co(t),l=n._transitionClasses;a(l)&&(s=po(s,ho(l))),s!==n._prevClass&&(n.setAttribute("class",s),n._prevClass=s)}}var Lo,Ho={create:Mo,update:Mo};function No(e,t,n){var o=Lo;return function i(){var r=t.apply(null,arguments);null!==r&&Bo(e,i,n,o)}}var $o=Rt&&!(te&&Number(te[1])<=53);function Uo(e,t,n,o){if($o){var i=sn,r=t;t=r._wrapper=function(e){if(e.target===e.currentTarget||e.timeStamp>=i||e.timeStamp<=0||e.target.ownerDocument!==document)return r.apply(this,arguments)}}Lo.addEventListener(e,t,oe?{capture:n,passive:o}:n)}function Bo(e,t,n,o){(o||Lo).removeEventListener(e,t._wrapper||t,n)}function Go(e,t){if(!r(e.data.on)||!r(t.data.on)){var n=t.data.on||{},o=e.data.on||{};Lo=t.elm||e.elm,function(e){if(a(e.__r)){var t=K?"change":"input";e[t]=[].concat(e.__r,e[t]||[]),delete e.__r}a(e.__c)&&(e.change=[].concat(e.__c,e.change||[]),delete e.__c)}(n),Be(n,o,Uo,Bo,No,t.context),Lo=void 0}}var Wo,Qo={create:Go,update:Go,destroy:function(e){return Go(e,To)}};function Yo(e,t){if(!r(e.data.domProps)||!r(t.data.domProps)){var n,o,i=t.elm,l=e.data.domProps||{},c=t.data.domProps||{};for(n in(a(c.__ob__)||s(c._v_attr_proxy))&&(c=t.data.domProps=D({},c)),l)n in c||(i[n]="");for(n in c){if(o=c[n],"textContent"===n||"innerHTML"===n){if(t.children&&(t.children.length=0),o===l[n])continue;1===i.childNodes.length&&i.removeChild(i.childNodes[0])}if("value"===n&&"PROGRESS"!==i.tagName){i._value=o;var d=r(o)?"":String(o);Ko(i,d)&&(i.value=d)}else if("innerHTML"===n&&go(i.tagName)&&r(i.innerHTML)){(Wo=Wo||document.createElement("div")).innerHTML="<svg>".concat(o,"</svg>");for(var u=Wo.firstChild;i.firstChild;)i.removeChild(i.firstChild);for(;u.firstChild;)i.appendChild(u.firstChild)}else if(o!==l[n])try{i[n]=o}catch(e){}}}}function Ko(e,t){return!e.composing&&("OPTION"===e.tagName||function(e,t){var n=!0;try{n=document.activeElement!==e}catch(e){}return n&&e.value!==t}(e,t)||function(e,t){var n=e.value,o=e._vModifiers;if(a(o)){if(o.number)return v(n)!==v(t);if(o.trim)return n.trim()!==t.trim()}return n!==t}(e,t))}var Xo={create:Yo,update:Yo},Zo=_((function(e){var t={},n=/:(.+)/;return e.split(/;(?![^(]*\))/g).forEach((function(e){if(e){var o=e.split(n);o.length>1&&(t[o[0].trim()]=o[1].trim())}})),t}));function Jo(e){var t=ei(e.style);return e.staticStyle?D(e.staticStyle,t):t}function ei(e){return Array.isArray(e)?z(e):"string"==typeof e?Zo(e):e}var ti,ni=/^--/,oi=/\s*!important$/,ii=function(e,t,n){if(ni.test(t))e.style.setProperty(t,n);else if(oi.test(n))e.style.setProperty(A(t),n.replace(oi,""),"important");else{var o=ai(t);if(Array.isArray(n))for(var i=0,r=n.length;i<r;i++)e.style[o]=n[i];else e.style[o]=n}},ri=["Webkit","Moz","ms"],ai=_((function(e){if(ti=ti||document.createElement("div").style,"filter"!==(e=C(e))&&e in ti)return e;for(var t=e.charAt(0).toUpperCase()+e.slice(1),n=0;n<ri.length;n++){var o=ri[n]+t;if(o in ti)return o}}));function si(e,t){var n=t.data,o=e.data;if(!(r(n.staticStyle)&&r(n.style)&&r(o.staticStyle)&&r(o.style))){var i,s,l=t.elm,c=o.staticStyle,d=o.normalizedStyle||o.style||{},u=c||d,p=ei(t.data.style)||{};t.data.normalizedStyle=a(p.__ob__)?D({},p):p;var h=function(e,t){var n,o={};if(t)for(var i=e;i.componentInstance;)(i=i.componentInstance._vnode)&&i.data&&(n=Jo(i.data))&&D(o,n);(n=Jo(e.data))&&D(o,n);for(var r=e;r=r.parent;)r.data&&(n=Jo(r.data))&&D(o,n);return o}(t,!0);for(s in u)r(h[s])&&ii(l,s,"");for(s in h)(i=h[s])!==u[s]&&ii(l,s,null==i?"":i)}}var li={create:si,update:si},ci=/\s+/;function di(e,t){if(t&&(t=t.trim()))if(e.classList)t.indexOf(" ")>-1?t.split(ci).forEach((function(t){return e.classList.add(t)})):e.classList.add(t);else{var n=" ".concat(e.getAttribute("class")||""," ");n.indexOf(" "+t+" ")<0&&e.setAttribute("class",(n+t).trim())}}function ui(e,t){if(t&&(t=t.trim()))if(e.classList)t.indexOf(" ")>-1?t.split(ci).forEach((function(t){return e.classList.remove(t)})):e.classList.remove(t),e.classList.length||e.removeAttribute("class");else{for(var n=" ".concat(e.getAttribute("class")||""," "),o=" "+t+" ";n.indexOf(o)>=0;)n=n.replace(o," ");(n=n.trim())?e.setAttribute("class",n):e.removeAttribute("class")}}function pi(e){if(e){if("object"==typeof e){var t={};return!1!==e.css&&D(t,hi(e.name||"v")),D(t,e),t}return"string"==typeof e?hi(e):void 0}}var hi=_((function(e){return{enterClass:"".concat(e,"-enter"),enterToClass:"".concat(e,"-enter-to"),enterActiveClass:"".concat(e,"-enter-active"),leaveClass:"".concat(e,"-leave"),leaveToClass:"".concat(e,"-leave-to"),leaveActiveClass:"".concat(e,"-leave-active")}})),fi=Q&&!X,mi="transition",gi="transitionend",vi="animation",yi="animationend";fi&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(mi="WebkitTransition",gi="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(vi="WebkitAnimation",yi="webkitAnimationEnd"));var bi=Q?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(e){return e()};function wi(e){bi((function(){bi(e)}))}function xi(e,t){var n=e._transitionClasses||(e._transitionClasses=[]);n.indexOf(t)<0&&(n.push(t),di(e,t))}function ki(e,t){e._transitionClasses&&w(e._transitionClasses,t),ui(e,t)}function _i(e,t,n){var o=Ci(e,t),i=o.type,r=o.timeout,a=o.propCount;if(!i)return n();var s="transition"===i?gi:yi,l=0,c=function(){e.removeEventListener(s,d),n()},d=function(t){t.target===e&&++l>=a&&c()};setTimeout((function(){l<a&&c()}),r+1),e.addEventListener(s,d)}var Ti=/\b(transform|all)(,|$)/;function Ci(e,t){var n,o=window.getComputedStyle(e),i=(o[mi+"Delay"]||"").split(", "),r=(o[mi+"Duration"]||"").split(", "),a=Si(i,r),s=(o[vi+"Delay"]||"").split(", "),l=(o[vi+"Duration"]||"").split(", "),c=Si(s,l),d=0,u=0;return"transition"===t?a>0&&(n="transition",d=a,u=r.length):"animation"===t?c>0&&(n="animation",d=c,u=l.length):u=(n=(d=Math.max(a,c))>0?a>c?"transition":"animation":null)?"transition"===n?r.length:l.length:0,{type:n,timeout:d,propCount:u,hasTransform:"transition"===n&&Ti.test(o[mi+"Property"])}}function Si(e,t){for(;e.length<t.length;)e=e.concat(e);return Math.max.apply(null,t.map((function(t,n){return Ii(t)+Ii(e[n])})))}function Ii(e){return 1e3*Number(e.slice(0,-1).replace(",","."))}function Ai(e,t){var n=e.elm;a(n._leaveCb)&&(n._leaveCb.cancelled=!0,n._leaveCb());var o=pi(e.data.transition);if(!r(o)&&!a(n._enterCb)&&1===n.nodeType){for(var i=o.css,s=o.type,l=o.enterClass,u=o.enterToClass,p=o.enterActiveClass,h=o.appearClass,f=o.appearToClass,m=o.appearActiveClass,g=o.beforeEnter,y=o.enter,b=o.afterEnter,w=o.enterCancelled,x=o.beforeAppear,k=o.appear,_=o.afterAppear,T=o.appearCancelled,C=o.duration,S=Yt,I=Yt.$vnode;I&&I.parent;)S=I.context,I=I.parent;var A=!S._isMounted||!e.isRootInsert;if(!A||k||""===k){var E=A&&h?h:l,R=A&&m?m:p,D=A&&f?f:u,z=A&&x||g,O=A&&c(k)?k:y,P=A&&_||b,q=A&&T||w,j=v(d(C)?C.enter:C);0;var F=!1!==i&&!X,M=Di(O),L=n._enterCb=V((function(){F&&(ki(n,D),ki(n,R)),L.cancelled?(F&&ki(n,E),q&&q(n)):P&&P(n),n._enterCb=null}));e.data.show||Ge(e,"insert",(function(){var t=n.parentNode,o=t&&t._pending&&t._pending[e.key];o&&o.tag===e.tag&&o.elm._leaveCb&&o.elm._leaveCb(),O&&O(n,L)})),z&&z(n),F&&(xi(n,E),xi(n,R),wi((function(){ki(n,E),L.cancelled||(xi(n,D),M||(Ri(j)?setTimeout(L,j):_i(n,s,L)))}))),e.data.show&&(t&&t(),O&&O(n,L)),F||M||L()}}}function Ei(e,t){var n=e.elm;a(n._enterCb)&&(n._enterCb.cancelled=!0,n._enterCb());var o=pi(e.data.transition);if(r(o)||1!==n.nodeType)return t();if(!a(n._leaveCb)){var i=o.css,s=o.type,l=o.leaveClass,c=o.leaveToClass,u=o.leaveActiveClass,p=o.beforeLeave,h=o.leave,f=o.afterLeave,m=o.leaveCancelled,g=o.delayLeave,y=o.duration,b=!1!==i&&!X,w=Di(h),x=v(d(y)?y.leave:y);0;var k=n._leaveCb=V((function(){n.parentNode&&n.parentNode._pending&&(n.parentNode._pending[e.key]=null),b&&(ki(n,c),ki(n,u)),k.cancelled?(b&&ki(n,l),m&&m(n)):(t(),f&&f(n)),n._leaveCb=null}));g?g(_):_()}function _(){k.cancelled||(!e.data.show&&n.parentNode&&((n.parentNode._pending||(n.parentNode._pending={}))[e.key]=e),p&&p(n),b&&(xi(n,l),xi(n,u),wi((function(){ki(n,l),k.cancelled||(xi(n,c),w||(Ri(x)?setTimeout(k,x):_i(n,s,k)))}))),h&&h(n,k),b||w||k())}}function Ri(e){return"number"==typeof e&&!isNaN(e)}function Di(e){if(r(e))return!1;var t=e.fns;return a(t)?Di(Array.isArray(t)?t[0]:t):(e._length||e.length)>1}function zi(e,t){!0!==t.data.show&&Ai(t)}var Oi=function(e){var t,n,o={},c=e.modules,d=e.nodeOps;for(t=0;t<Co.length;++t)for(o[Co[t]]=[],n=0;n<c.length;++n)a(c[n][Co[t]])&&o[Co[t]].push(c[n][Co[t]]);function u(e){var t=d.parentNode(e);a(t)&&d.removeChild(t,e)}function p(e,t,n,i,r,l,c){if(a(e.elm)&&a(l)&&(e=l[c]=me(e)),e.isRootInsert=!r,!function(e,t,n,i){var r=e.data;if(a(r)){var l=a(e.componentInstance)&&r.keepAlive;if(a(r=r.hook)&&a(r=r.init)&&r(e,!1),a(e.componentInstance))return h(e,t),f(n,e.elm,i),s(l)&&function(e,t,n,i){var r,s=e;for(;s.componentInstance;)if(s=s.componentInstance._vnode,a(r=s.data)&&a(r=r.transition)){for(r=0;r<o.activate.length;++r)o.activate[r](To,s);t.push(s);break}f(n,e.elm,i)}(e,t,n,i),!0}}(e,t,n,i)){var u=e.data,p=e.children,g=e.tag;a(g)?(e.elm=e.ns?d.createElementNS(e.ns,g):d.createElement(g,e),b(e),m(e,p,t),a(u)&&v(e,t),f(n,e.elm,i)):s(e.isComment)?(e.elm=d.createComment(e.text),f(n,e.elm,i)):(e.elm=d.createTextNode(e.text),f(n,e.elm,i))}}function h(e,t){a(e.data.pendingInsert)&&(t.push.apply(t,e.data.pendingInsert),e.data.pendingInsert=null),e.elm=e.componentInstance.$el,g(e)?(v(e,t),b(e)):(ko(e),t.push(e))}function f(e,t,n){a(e)&&(a(n)?d.parentNode(n)===e&&d.insertBefore(e,t,n):d.appendChild(e,t))}function m(e,t,n){if(i(t)){0;for(var o=0;o<t.length;++o)p(t[o],n,e.elm,null,!0,t,o)}else l(e.text)&&d.appendChild(e.elm,d.createTextNode(String(e.text)))}function g(e){for(;e.componentInstance;)e=e.componentInstance._vnode;return a(e.tag)}function v(e,n){for(var i=0;i<o.create.length;++i)o.create[i](To,e);a(t=e.data.hook)&&(a(t.create)&&t.create(To,e),a(t.insert)&&n.push(e))}function b(e){var t;if(a(t=e.fnScopeId))d.setStyleScope(e.elm,t);else for(var n=e;n;)a(t=n.context)&&a(t=t.$options._scopeId)&&d.setStyleScope(e.elm,t),n=n.parent;a(t=Yt)&&t!==e.context&&t!==e.fnContext&&a(t=t.$options._scopeId)&&d.setStyleScope(e.elm,t)}function w(e,t,n,o,i,r){for(;o<=i;++o)p(n[o],r,e,t,!1,n,o)}function x(e){var t,n,i=e.data;if(a(i))for(a(t=i.hook)&&a(t=t.destroy)&&t(e),t=0;t<o.destroy.length;++t)o.destroy[t](e);if(a(t=e.children))for(n=0;n<e.children.length;++n)x(e.children[n])}function k(e,t,n){for(;t<=n;++t){var o=e[t];a(o)&&(a(o.tag)?(_(o),x(o)):u(o.elm))}}function _(e,t){if(a(t)||a(e.data)){var n,i=o.remove.length+1;for(a(t)?t.listeners+=i:t=function(e,t){function n(){0==--n.listeners&&u(e)}return n.listeners=t,n}(e.elm,i),a(n=e.componentInstance)&&a(n=n._vnode)&&a(n.data)&&_(n,t),n=0;n<o.remove.length;++n)o.remove[n](e,t);a(n=e.data.hook)&&a(n=n.remove)?n(e,t):t()}else u(e.elm)}function T(e,t,n,o){for(var i=n;i<o;i++){var r=t[i];if(a(r)&&So(e,r))return i}}function C(e,t,n,i,l,c){if(e!==t){a(t.elm)&&a(i)&&(t=i[l]=me(t));var u=t.elm=e.elm;if(s(e.isAsyncPlaceholder))a(t.asyncFactory.resolved)?A(e.elm,t,n):t.isAsyncPlaceholder=!0;else if(s(t.isStatic)&&s(e.isStatic)&&t.key===e.key&&(s(t.isCloned)||s(t.isOnce)))t.componentInstance=e.componentInstance;else{var h,f=t.data;a(f)&&a(h=f.hook)&&a(h=h.prepatch)&&h(e,t);var m=e.children,v=t.children;if(a(f)&&g(t)){for(h=0;h<o.update.length;++h)o.update[h](e,t);a(h=f.hook)&&a(h=h.update)&&h(e,t)}r(t.text)?a(m)&&a(v)?m!==v&&function(e,t,n,o,i){var s,l,c,u=0,h=0,f=t.length-1,m=t[0],g=t[f],v=n.length-1,y=n[0],b=n[v],x=!i;for(0;u<=f&&h<=v;)r(m)?m=t[++u]:r(g)?g=t[--f]:So(m,y)?(C(m,y,o,n,h),m=t[++u],y=n[++h]):So(g,b)?(C(g,b,o,n,v),g=t[--f],b=n[--v]):So(m,b)?(C(m,b,o,n,v),x&&d.insertBefore(e,m.elm,d.nextSibling(g.elm)),m=t[++u],b=n[--v]):So(g,y)?(C(g,y,o,n,h),x&&d.insertBefore(e,g.elm,m.elm),g=t[--f],y=n[++h]):(r(s)&&(s=Io(t,u,f)),r(l=a(y.key)?s[y.key]:T(y,t,u,f))?p(y,o,e,m.elm,!1,n,h):So(c=t[l],y)?(C(c,y,o,n,h),t[l]=void 0,x&&d.insertBefore(e,c.elm,m.elm)):p(y,o,e,m.elm,!1,n,h),y=n[++h]);u>f?w(e,r(n[v+1])?null:n[v+1].elm,n,h,v,o):h>v&&k(t,u,f)}(u,m,v,n,c):a(v)?(a(e.text)&&d.setTextContent(u,""),w(u,null,v,0,v.length-1,n)):a(m)?k(m,0,m.length-1):a(e.text)&&d.setTextContent(u,""):e.text!==t.text&&d.setTextContent(u,t.text),a(f)&&a(h=f.hook)&&a(h=h.postpatch)&&h(e,t)}}}function S(e,t,n){if(s(n)&&a(e.parent))e.parent.data.pendingInsert=t;else for(var o=0;o<t.length;++o)t[o].data.hook.insert(t[o])}var I=y("attrs,class,staticClass,staticStyle,key");function A(e,t,n,o){var i,r=t.tag,l=t.data,c=t.children;if(o=o||l&&l.pre,t.elm=e,s(t.isComment)&&a(t.asyncFactory))return t.isAsyncPlaceholder=!0,!0;if(a(l)&&(a(i=l.hook)&&a(i=i.init)&&i(t,!0),a(i=t.componentInstance)))return h(t,n),!0;if(a(r)){if(a(c))if(e.hasChildNodes())if(a(i=l)&&a(i=i.domProps)&&a(i=i.innerHTML)){if(i!==e.innerHTML)return!1}else{for(var d=!0,u=e.firstChild,p=0;p<c.length;p++){if(!u||!A(u,c[p],n,o)){d=!1;break}u=u.nextSibling}if(!d||u)return!1}else m(t,c,n);if(a(l)){var f=!1;for(var g in l)if(!I(g)){f=!0,v(t,n);break}!f&&l.class&&Ht(l.class)}}else e.data!==t.text&&(e.data=t.text);return!0}return function(e,t,n,i){if(!r(t)){var l,c=!1,u=[];if(r(e))c=!0,p(t,u);else{var h=a(e.nodeType);if(!h&&So(e,t))C(e,t,u,null,null,i);else{if(h){if(1===e.nodeType&&e.hasAttribute("data-server-rendered")&&(e.removeAttribute("data-server-rendered"),n=!0),s(n)&&A(e,t,u))return S(t,u,!0),e;l=e,e=new pe(d.tagName(l).toLowerCase(),{},[],void 0,l)}var f=e.elm,m=d.parentNode(f);if(p(t,u,f._leaveCb?null:m,d.nextSibling(f)),a(t.parent))for(var v=t.parent,y=g(t);v;){for(var b=0;b<o.destroy.length;++b)o.destroy[b](v);if(v.elm=t.elm,y){for(var w=0;w<o.create.length;++w)o.create[w](To,v);var _=v.data.hook.insert;if(_.merged)for(var T=1;T<_.fns.length;T++)_.fns[T]()}else ko(v);v=v.parent}a(m)?k([e],0,0):a(e.tag)&&x(e)}}return S(t,u,c),t.elm}a(e)&&x(e)}}({nodeOps:wo,modules:[Vo,Ho,Qo,Xo,li,Q?{create:zi,activate:zi,remove:function(e,t){!0!==e.data.show?Ei(e,t):t()}}:{}].concat(Po)});X&&document.addEventListener("selectionchange",(function(){var e=document.activeElement;e&&e.vmodel&&Hi(e,"input")}));var Pi={inserted:function(e,t,n,o){"select"===n.tag?(o.elm&&!o.elm._vOptions?Ge(n,"postpatch",(function(){Pi.componentUpdated(e,t,n)})):qi(e,t,n.context),e._vOptions=[].map.call(e.options,Vi)):("textarea"===n.tag||bo(e.type))&&(e._vModifiers=t.modifiers,t.modifiers.lazy||(e.addEventListener("compositionstart",Mi),e.addEventListener("compositionend",Li),e.addEventListener("change",Li),X&&(e.vmodel=!0)))},componentUpdated:function(e,t,n){if("select"===n.tag){qi(e,t,n.context);var o=e._vOptions,i=e._vOptions=[].map.call(e.options,Vi);if(i.some((function(e,t){return!j(e,o[t])})))(e.multiple?t.value.some((function(e){return Fi(e,i)})):t.value!==t.oldValue&&Fi(t.value,i))&&Hi(e,"change")}}};function qi(e,t,n){ji(e,t,n),(K||Z)&&setTimeout((function(){ji(e,t,n)}),0)}function ji(e,t,n){var o=t.value,i=e.multiple;if(!i||Array.isArray(o)){for(var r,a,s=0,l=e.options.length;s<l;s++)if(a=e.options[s],i)r=F(o,Vi(a))>-1,a.selected!==r&&(a.selected=r);else if(j(Vi(a),o))return void(e.selectedIndex!==s&&(e.selectedIndex=s));i||(e.selectedIndex=-1)}}function Fi(e,t){return t.every((function(t){return!j(t,e)}))}function Vi(e){return"_value"in e?e._value:e.value}function Mi(e){e.target.composing=!0}function Li(e){e.target.composing&&(e.target.composing=!1,Hi(e.target,"input"))}function Hi(e,t){var n=document.createEvent("HTMLEvents");n.initEvent(t,!0,!0),e.dispatchEvent(n)}function Ni(e){return!e.componentInstance||e.data&&e.data.transition?e:Ni(e.componentInstance._vnode)}var $i={model:Pi,show:{bind:function(e,t,n){var o=t.value,i=(n=Ni(n)).data&&n.data.transition,r=e.__vOriginalDisplay="none"===e.style.display?"":e.style.display;o&&i?(n.data.show=!0,Ai(n,(function(){e.style.display=r}))):e.style.display=o?r:"none"},update:function(e,t,n){var o=t.value;!o!=!t.oldValue&&((n=Ni(n)).data&&n.data.transition?(n.data.show=!0,o?Ai(n,(function(){e.style.display=e.__vOriginalDisplay})):Ei(n,(function(){e.style.display="none"}))):e.style.display=o?e.__vOriginalDisplay:"none")},unbind:function(e,t,n,o,i){i||(e.style.display=e.__vOriginalDisplay)}}},Ui={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function Bi(e){var t=e&&e.componentOptions;return t&&t.Ctor.options.abstract?Bi(_t(t.children)):e}function Gi(e){var t={},n=e.$options;for(var o in n.propsData)t[o]=e[o];var i=n._parentListeners;for(var o in i)t[C(o)]=i[o];return t}function Wi(e,t){if(/\d-keep-alive$/.test(t.tag))return e("keep-alive",{props:t.componentOptions.propsData})}var Qi=function(e){return e.tag||ht(e)},Yi=function(e){return"show"===e.name},Ki={name:"transition",props:Ui,abstract:!0,render:function(e){var t=this,n=this.$slots.default;if(n&&(n=n.filter(Qi)).length){0;var o=this.mode;0;var i=n[0];if(function(e){for(;e=e.parent;)if(e.data.transition)return!0}(this.$vnode))return i;var r=Bi(i);if(!r)return i;if(this._leaving)return Wi(e,i);var a="__transition-".concat(this._uid,"-");r.key=null==r.key?r.isComment?a+"comment":a+r.tag:l(r.key)?0===String(r.key).indexOf(a)?r.key:a+r.key:r.key;var s=(r.data||(r.data={})).transition=Gi(this),c=this._vnode,d=Bi(c);if(r.data.directives&&r.data.directives.some(Yi)&&(r.data.show=!0),d&&d.data&&!function(e,t){return t.key===e.key&&t.tag===e.tag}(r,d)&&!ht(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var u=d.data.transition=D({},s);if("out-in"===o)return this._leaving=!0,Ge(u,"afterLeave",(function(){t._leaving=!1,t.$forceUpdate()})),Wi(e,i);if("in-out"===o){if(ht(r))return c;var p,h=function(){p()};Ge(s,"afterEnter",h),Ge(s,"enterCancelled",h),Ge(u,"delayLeave",(function(e){p=e}))}}return i}}},Xi=D({tag:String,moveClass:String},Ui);function Zi(e){e.elm._moveCb&&e.elm._moveCb(),e.elm._enterCb&&e.elm._enterCb()}function Ji(e){e.data.newPos=e.elm.getBoundingClientRect()}function er(e){var t=e.data.pos,n=e.data.newPos,o=t.left-n.left,i=t.top-n.top;if(o||i){e.data.moved=!0;var r=e.elm.style;r.transform=r.WebkitTransform="translate(".concat(o,"px,").concat(i,"px)"),r.transitionDuration="0s"}}delete Xi.mode;var tr={Transition:Ki,TransitionGroup:{props:Xi,beforeMount:function(){var e=this,t=this._update;this._update=function(n,o){var i=Kt(e);e.__patch__(e._vnode,e.kept,!1,!0),e._vnode=e.kept,i(),t.call(e,n,o)}},render:function(e){for(var t=this.tag||this.$vnode.data.tag||"span",n=Object.create(null),o=this.prevChildren=this.children,i=this.$slots.default||[],r=this.children=[],a=Gi(this),s=0;s<i.length;s++){if((d=i[s]).tag)if(null!=d.key&&0!==String(d.key).indexOf("__vlist"))r.push(d),n[d.key]=d,(d.data||(d.data={})).transition=a;else;}if(o){var l=[],c=[];for(s=0;s<o.length;s++){var d;(d=o[s]).data.transition=a,d.data.pos=d.elm.getBoundingClientRect(),n[d.key]?l.push(d):c.push(d)}this.kept=e(t,null,l),this.removed=c}return e(t,null,r)},updated:function(){var e=this.prevChildren,t=this.moveClass||(this.name||"v")+"-move";e.length&&this.hasMove(e[0].elm,t)&&(e.forEach(Zi),e.forEach(Ji),e.forEach(er),this._reflow=document.body.offsetHeight,e.forEach((function(e){if(e.data.moved){var n=e.elm,o=n.style;xi(n,t),o.transform=o.WebkitTransform=o.transitionDuration="",n.addEventListener(gi,n._moveCb=function e(o){o&&o.target!==n||o&&!/transform$/.test(o.propertyName)||(n.removeEventListener(gi,e),n._moveCb=null,ki(n,t))})}})))},methods:{hasMove:function(e,t){if(!fi)return!1;if(this._hasMove)return this._hasMove;var n=e.cloneNode();e._transitionClasses&&e._transitionClasses.forEach((function(e){ui(n,e)})),di(n,t),n.style.display="none",this.$el.appendChild(n);var o=Ci(n);return this.$el.removeChild(n),this._hasMove=o.hasTransform}}}};function nr(e,t){for(var n in t)e[n]=t[n];return e}Gn.config.mustUseProp=function(e,t,n){return"value"===n&&to(e)&&"button"!==t||"selected"===n&&"option"===e||"checked"===n&&"input"===e||"muted"===n&&"video"===e},Gn.config.isReservedTag=vo,Gn.config.isReservedAttr=eo,Gn.config.getTagNamespace=function(e){return go(e)?"svg":"math"===e?"math":void 0},Gn.config.isUnknownElement=function(e){if(!Q)return!0;if(vo(e))return!1;if(e=e.toLowerCase(),null!=yo[e])return yo[e];var t=document.createElement(e);return e.indexOf("-")>-1?yo[e]=t.constructor===window.HTMLUnknownElement||t.constructor===window.HTMLElement:yo[e]=/HTMLUnknownElement/.test(t.toString())},D(Gn.options.directives,$i),D(Gn.options.components,tr),Gn.prototype.__patch__=Q?Oi:O,Gn.prototype.$mount=function(e,t){return function(e,t,n){var o;e.$el=t,e.$options.render||(e.$options.render=he),Jt(e,"beforeMount"),o=function(){e._update(e._render(),n)},new Ut(e,o,O,{before:function(){e._isMounted&&!e._isDestroyed&&Jt(e,"beforeUpdate")}},!0),n=!1;var i=e._preWatchers;if(i)for(var r=0;r<i.length;r++)i[r].run();return null==e.$vnode&&(e._isMounted=!0,Jt(e,"mounted")),e}(this,e=e&&Q?function(e){if("string"==typeof e){var t=document.querySelector(e);return t||document.createElement("div")}return e}(e):void 0,t)},Q&&setTimeout((function(){N.devtools&&ae&&ae.emit("init",Gn)}),0);var or=/[!'()*]/g,ir=function(e){return"%"+e.charCodeAt(0).toString(16)},rr=/%2C/g,ar=function(e){return encodeURIComponent(e).replace(or,ir).replace(rr,",")};function sr(e){try{return decodeURIComponent(e)}catch(e){0}return e}var lr=function(e){return null==e||"object"==typeof e?e:String(e)};function cr(e){var t={};return(e=e.trim().replace(/^(\?|#|&)/,""))?(e.split("&").forEach((function(e){var n=e.replace(/\+/g," ").split("="),o=sr(n.shift()),i=n.length>0?sr(n.join("=")):null;void 0===t[o]?t[o]=i:Array.isArray(t[o])?t[o].push(i):t[o]=[t[o],i]})),t):t}function dr(e){var t=e?Object.keys(e).map((function(t){var n=e[t];if(void 0===n)return"";if(null===n)return ar(t);if(Array.isArray(n)){var o=[];return n.forEach((function(e){void 0!==e&&(null===e?o.push(ar(t)):o.push(ar(t)+"="+ar(e)))})),o.join("&")}return ar(t)+"="+ar(n)})).filter((function(e){return e.length>0})).join("&"):null;return t?"?"+t:""}var ur=/\/?$/;function pr(e,t,n,o){var i=o&&o.options.stringifyQuery,r=t.query||{};try{r=hr(r)}catch(e){}var a={name:t.name||e&&e.name,meta:e&&e.meta||{},path:t.path||"/",hash:t.hash||"",query:r,params:t.params||{},fullPath:gr(t,i),matched:e?mr(e):[]};return n&&(a.redirectedFrom=gr(n,i)),Object.freeze(a)}function hr(e){if(Array.isArray(e))return e.map(hr);if(e&&"object"==typeof e){var t={};for(var n in e)t[n]=hr(e[n]);return t}return e}var fr=pr(null,{path:"/"});function mr(e){for(var t=[];e;)t.unshift(e),e=e.parent;return t}function gr(e,t){var n=e.path,o=e.query;void 0===o&&(o={});var i=e.hash;return void 0===i&&(i=""),(n||"/")+(t||dr)(o)+i}function vr(e,t,n){return t===fr?e===t:!!t&&(e.path&&t.path?e.path.replace(ur,"")===t.path.replace(ur,"")&&(n||e.hash===t.hash&&yr(e.query,t.query)):!(!e.name||!t.name)&&(e.name===t.name&&(n||e.hash===t.hash&&yr(e.query,t.query)&&yr(e.params,t.params))))}function yr(e,t){if(void 0===e&&(e={}),void 0===t&&(t={}),!e||!t)return e===t;var n=Object.keys(e).sort(),o=Object.keys(t).sort();return n.length===o.length&&n.every((function(n,i){var r=e[n];if(o[i]!==n)return!1;var a=t[n];return null==r||null==a?r===a:"object"==typeof r&&"object"==typeof a?yr(r,a):String(r)===String(a)}))}function br(e){for(var t=0;t<e.matched.length;t++){var n=e.matched[t];for(var o in n.instances){var i=n.instances[o],r=n.enteredCbs[o];if(i&&r){delete n.enteredCbs[o];for(var a=0;a<r.length;a++)i._isBeingDestroyed||r[a](i)}}}}var wr={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(e,t){var n=t.props,o=t.children,i=t.parent,r=t.data;r.routerView=!0;for(var a=i.$createElement,s=n.name,l=i.$route,c=i._routerViewCache||(i._routerViewCache={}),d=0,u=!1;i&&i._routerRoot!==i;){var p=i.$vnode?i.$vnode.data:{};p.routerView&&d++,p.keepAlive&&i._directInactive&&i._inactive&&(u=!0),i=i.$parent}if(r.routerViewDepth=d,u){var h=c[s],f=h&&h.component;return f?(h.configProps&&xr(f,r,h.route,h.configProps),a(f,r,o)):a()}var m=l.matched[d],g=m&&m.components[s];if(!m||!g)return c[s]=null,a();c[s]={component:g},r.registerRouteInstance=function(e,t){var n=m.instances[s];(t&&n!==e||!t&&n===e)&&(m.instances[s]=t)},(r.hook||(r.hook={})).prepatch=function(e,t){m.instances[s]=t.componentInstance},r.hook.init=function(e){e.data.keepAlive&&e.componentInstance&&e.componentInstance!==m.instances[s]&&(m.instances[s]=e.componentInstance),br(l)};var v=m.props&&m.props[s];return v&&(nr(c[s],{route:l,configProps:v}),xr(g,r,l,v)),a(g,r,o)}};function xr(e,t,n,o){var i=t.props=function(e,t){switch(typeof t){case"undefined":return;case"object":return t;case"function":return t(e);case"boolean":return t?e.params:void 0;default:0}}(n,o);if(i){i=t.props=nr({},i);var r=t.attrs=t.attrs||{};for(var a in i)e.props&&a in e.props||(r[a]=i[a],delete i[a])}}function kr(e,t,n){var o=e.charAt(0);if("/"===o)return e;if("?"===o||"#"===o)return t+e;var i=t.split("/");n&&i[i.length-1]||i.pop();for(var r=e.replace(/^\//,"").split("/"),a=0;a<r.length;a++){var s=r[a];".."===s?i.pop():"."!==s&&i.push(s)}return""!==i[0]&&i.unshift(""),i.join("/")}function _r(e){return e.replace(/\/(?:\s*\/)+/g,"/")}var Tr=Array.isArray||function(e){return"[object Array]"==Object.prototype.toString.call(e)},Cr=Mr,Sr=Dr,Ir=function(e,t){return Or(Dr(e,t),t)},Ar=Or,Er=Vr,Rr=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function Dr(e,t){for(var n,o=[],i=0,r=0,a="",s=t&&t.delimiter||"/";null!=(n=Rr.exec(e));){var l=n[0],c=n[1],d=n.index;if(a+=e.slice(r,d),r=d+l.length,c)a+=c[1];else{var u=e[r],p=n[2],h=n[3],f=n[4],m=n[5],g=n[6],v=n[7];a&&(o.push(a),a="");var y=null!=p&&null!=u&&u!==p,b="+"===g||"*"===g,w="?"===g||"*"===g,x=n[2]||s,k=f||m;o.push({name:h||i++,prefix:p||"",delimiter:x,optional:w,repeat:b,partial:y,asterisk:!!v,pattern:k?qr(k):v?".*":"[^"+Pr(x)+"]+?"})}}return r<e.length&&(a+=e.substr(r)),a&&o.push(a),o}function zr(e){return encodeURI(e).replace(/[\/?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()}))}function Or(e,t){for(var n=new Array(e.length),o=0;o<e.length;o++)"object"==typeof e[o]&&(n[o]=new RegExp("^(?:"+e[o].pattern+")$",Fr(t)));return function(t,o){for(var i="",r=t||{},a=(o||{}).pretty?zr:encodeURIComponent,s=0;s<e.length;s++){var l=e[s];if("string"!=typeof l){var c,d=r[l.name];if(null==d){if(l.optional){l.partial&&(i+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(Tr(d)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var u=0;u<d.length;u++){if(c=a(d[u]),!n[s].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");i+=(0===u?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(d).replace(/[?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()})):a(d),!n[s].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');i+=l.prefix+c}}else i+=l}return i}}function Pr(e){return e.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function qr(e){return e.replace(/([=!:$\/()])/g,"\\$1")}function jr(e,t){return e.keys=t,e}function Fr(e){return e&&e.sensitive?"":"i"}function Vr(e,t,n){Tr(t)||(n=t||n,t=[]);for(var o=(n=n||{}).strict,i=!1!==n.end,r="",a=0;a<e.length;a++){var s=e[a];if("string"==typeof s)r+=Pr(s);else{var l=Pr(s.prefix),c="(?:"+s.pattern+")";t.push(s),s.repeat&&(c+="(?:"+l+c+")*"),r+=c=s.optional?s.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var d=Pr(n.delimiter||"/"),u=r.slice(-d.length)===d;return o||(r=(u?r.slice(0,-d.length):r)+"(?:"+d+"(?=$))?"),r+=i?"$":o&&u?"":"(?="+d+"|$)",jr(new RegExp("^"+r,Fr(n)),t)}function Mr(e,t,n){return Tr(t)||(n=t||n,t=[]),n=n||{},e instanceof RegExp?function(e,t){var n=e.source.match(/\((?!\?)/g);if(n)for(var o=0;o<n.length;o++)t.push({name:o,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return jr(e,t)}(e,t):Tr(e)?function(e,t,n){for(var o=[],i=0;i<e.length;i++)o.push(Mr(e[i],t,n).source);return jr(new RegExp("(?:"+o.join("|")+")",Fr(n)),t)}(e,t,n):function(e,t,n){return Vr(Dr(e,n),t,n)}(e,t,n)}Cr.parse=Sr,Cr.compile=Ir,Cr.tokensToFunction=Ar,Cr.tokensToRegExp=Er;var Lr=Object.create(null);function Hr(e,t,n){t=t||{};try{var o=Lr[e]||(Lr[e]=Cr.compile(e));return"string"==typeof t.pathMatch&&(t[0]=t.pathMatch),o(t,{pretty:!0})}catch(e){return""}finally{delete t[0]}}function Nr(e,t,n,o){var i="string"==typeof e?{path:e}:e;if(i._normalized)return i;if(i.name){var r=(i=nr({},e)).params;return r&&"object"==typeof r&&(i.params=nr({},r)),i}if(!i.path&&i.params&&t){(i=nr({},i))._normalized=!0;var a=nr(nr({},t.params),i.params);if(t.name)i.name=t.name,i.params=a;else if(t.matched.length){var s=t.matched[t.matched.length-1].path;i.path=Hr(s,a,t.path)}else 0;return i}var l=function(e){var t="",n="",o=e.indexOf("#");o>=0&&(t=e.slice(o),e=e.slice(0,o));var i=e.indexOf("?");return i>=0&&(n=e.slice(i+1),e=e.slice(0,i)),{path:e,query:n,hash:t}}(i.path||""),c=t&&t.path||"/",d=l.path?kr(l.path,c,n||i.append):c,u=function(e,t,n){void 0===t&&(t={});var o,i=n||cr;try{o=i(e||"")}catch(e){o={}}for(var r in t){var a=t[r];o[r]=Array.isArray(a)?a.map(lr):lr(a)}return o}(l.query,i.query,o&&o.options.parseQuery),p=i.hash||l.hash;return p&&"#"!==p.charAt(0)&&(p="#"+p),{_normalized:!0,path:d,query:u,hash:p}}var $r,Ur=function(){},Br={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(e){var t=this,n=this.$router,o=this.$route,i=n.resolve(this.to,o,this.append),r=i.location,a=i.route,s=i.href,l={},c=n.options.linkActiveClass,d=n.options.linkExactActiveClass,u=null==c?"router-link-active":c,p=null==d?"router-link-exact-active":d,h=null==this.activeClass?u:this.activeClass,f=null==this.exactActiveClass?p:this.exactActiveClass,m=a.redirectedFrom?pr(null,Nr(a.redirectedFrom),null,n):a;l[f]=vr(o,m,this.exactPath),l[h]=this.exact||this.exactPath?l[f]:function(e,t){return 0===e.path.replace(ur,"/").indexOf(t.path.replace(ur,"/"))&&(!t.hash||e.hash===t.hash)&&function(e,t){for(var n in t)if(!(n in e))return!1;return!0}(e.query,t.query)}(o,m);var g=l[f]?this.ariaCurrentValue:null,v=function(e){Gr(e)&&(t.replace?n.replace(r,Ur):n.push(r,Ur))},y={click:Gr};Array.isArray(this.event)?this.event.forEach((function(e){y[e]=v})):y[this.event]=v;var b={class:l},w=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:a,navigate:v,isActive:l[h],isExactActive:l[f]});if(w){if(1===w.length)return w[0];if(w.length>1||!w.length)return 0===w.length?e():e("span",{},w)}if("a"===this.tag)b.on=y,b.attrs={href:s,"aria-current":g};else{var x=function e(t){var n;if(t)for(var o=0;o<t.length;o++){if("a"===(n=t[o]).tag)return n;if(n.children&&(n=e(n.children)))return n}}(this.$slots.default);if(x){x.isStatic=!1;var k=x.data=nr({},x.data);for(var _ in k.on=k.on||{},k.on){var T=k.on[_];_ in y&&(k.on[_]=Array.isArray(T)?T:[T])}for(var C in y)C in k.on?k.on[C].push(y[C]):k.on[C]=v;var S=x.data.attrs=nr({},x.data.attrs);S.href=s,S["aria-current"]=g}else b.on=y}return e(this.tag,b,this.$slots.default)}};function Gr(e){if(!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey||e.defaultPrevented||void 0!==e.button&&0!==e.button)){if(e.currentTarget&&e.currentTarget.getAttribute){var t=e.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(t))return}return e.preventDefault&&e.preventDefault(),!0}}var Wr="undefined"!=typeof window;function Qr(e,t,n,o,i){var r=t||[],a=n||Object.create(null),s=o||Object.create(null);e.forEach((function(e){!function e(t,n,o,i,r,a){var s=i.path,l=i.name;0;var c=i.pathToRegexpOptions||{},d=function(e,t,n){n||(e=e.replace(/\/$/,""));if("/"===e[0])return e;if(null==t)return e;return _r(t.path+"/"+e)}(s,r,c.strict);"boolean"==typeof i.caseSensitive&&(c.sensitive=i.caseSensitive);var u={path:d,regex:Yr(d,c),components:i.components||{default:i.component},alias:i.alias?"string"==typeof i.alias?[i.alias]:i.alias:[],instances:{},enteredCbs:{},name:l,parent:r,matchAs:a,redirect:i.redirect,beforeEnter:i.beforeEnter,meta:i.meta||{},props:null==i.props?{}:i.components?i.props:{default:i.props}};i.children&&i.children.forEach((function(i){var r=a?_r(a+"/"+i.path):void 0;e(t,n,o,i,u,r)}));n[u.path]||(t.push(u.path),n[u.path]=u);if(void 0!==i.alias)for(var p=Array.isArray(i.alias)?i.alias:[i.alias],h=0;h<p.length;++h){0;var f={path:p[h],children:i.children};e(t,n,o,f,r,u.path||"/")}l&&(o[l]||(o[l]=u))}(r,a,s,e,i)}));for(var l=0,c=r.length;l<c;l++)"*"===r[l]&&(r.push(r.splice(l,1)[0]),c--,l--);return{pathList:r,pathMap:a,nameMap:s}}function Yr(e,t){return Cr(e,[],t)}function Kr(e,t){var n=Qr(e),o=n.pathList,i=n.pathMap,r=n.nameMap;function a(e,n,a){var s=Nr(e,n,!1,t),c=s.name;if(c){var d=r[c];if(!d)return l(null,s);var u=d.regex.keys.filter((function(e){return!e.optional})).map((function(e){return e.name}));if("object"!=typeof s.params&&(s.params={}),n&&"object"==typeof n.params)for(var p in n.params)!(p in s.params)&&u.indexOf(p)>-1&&(s.params[p]=n.params[p]);return s.path=Hr(d.path,s.params),l(d,s,a)}if(s.path){s.params={};for(var h=0;h<o.length;h++){var f=o[h],m=i[f];if(Xr(m.regex,s.path,s.params))return l(m,s,a)}}return l(null,s)}function s(e,n){var o=e.redirect,i="function"==typeof o?o(pr(e,n,null,t)):o;if("string"==typeof i&&(i={path:i}),!i||"object"!=typeof i)return l(null,n);var s=i,c=s.name,d=s.path,u=n.query,p=n.hash,h=n.params;if(u=s.hasOwnProperty("query")?s.query:u,p=s.hasOwnProperty("hash")?s.hash:p,h=s.hasOwnProperty("params")?s.params:h,c){r[c];return a({_normalized:!0,name:c,query:u,hash:p,params:h},void 0,n)}if(d){var f=function(e,t){return kr(e,t.parent?t.parent.path:"/",!0)}(d,e);return a({_normalized:!0,path:Hr(f,h),query:u,hash:p},void 0,n)}return l(null,n)}function l(e,n,o){return e&&e.redirect?s(e,o||n):e&&e.matchAs?function(e,t,n){var o=a({_normalized:!0,path:Hr(n,t.params)});if(o){var i=o.matched,r=i[i.length-1];return t.params=o.params,l(r,t)}return l(null,t)}(0,n,e.matchAs):pr(e,n,o,t)}return{match:a,addRoute:function(e,t){var n="object"!=typeof e?r[e]:void 0;Qr([t||e],o,i,r,n),n&&n.alias.length&&Qr(n.alias.map((function(e){return{path:e,children:[t]}})),o,i,r,n)},getRoutes:function(){return o.map((function(e){return i[e]}))},addRoutes:function(e){Qr(e,o,i,r)}}}function Xr(e,t,n){var o=t.match(e);if(!o)return!1;if(!n)return!0;for(var i=1,r=o.length;i<r;++i){var a=e.keys[i-1];a&&(n[a.name||"pathMatch"]="string"==typeof o[i]?sr(o[i]):o[i])}return!0}var Zr=Wr&&window.performance&&window.performance.now?window.performance:Date;function Jr(){return Zr.now().toFixed(3)}var ea=Jr();function ta(){return ea}function na(e){return ea=e}var oa=Object.create(null);function ia(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var e=window.location.protocol+"//"+window.location.host,t=window.location.href.replace(e,""),n=nr({},window.history.state);return n.key=ta(),window.history.replaceState(n,"",t),window.addEventListener("popstate",sa),function(){window.removeEventListener("popstate",sa)}}function ra(e,t,n,o){if(e.app){var i=e.options.scrollBehavior;i&&e.app.$nextTick((function(){var r=function(){var e=ta();if(e)return oa[e]}(),a=i.call(e,t,n,o?r:null);a&&("function"==typeof a.then?a.then((function(e){pa(e,r)})).catch((function(e){0})):pa(a,r))}))}}function aa(){var e=ta();e&&(oa[e]={x:window.pageXOffset,y:window.pageYOffset})}function sa(e){aa(),e.state&&e.state.key&&na(e.state.key)}function la(e){return da(e.x)||da(e.y)}function ca(e){return{x:da(e.x)?e.x:window.pageXOffset,y:da(e.y)?e.y:window.pageYOffset}}function da(e){return"number"==typeof e}var ua=/^#\d/;function pa(e,t){var n,o="object"==typeof e;if(o&&"string"==typeof e.selector){var i=ua.test(e.selector)?document.getElementById(e.selector.slice(1)):document.querySelector(e.selector);if(i){var r=e.offset&&"object"==typeof e.offset?e.offset:{};t=function(e,t){var n=document.documentElement.getBoundingClientRect(),o=e.getBoundingClientRect();return{x:o.left-n.left-t.x,y:o.top-n.top-t.y}}(i,r={x:da((n=r).x)?n.x:0,y:da(n.y)?n.y:0})}else la(e)&&(t=ca(e))}else o&&la(e)&&(t=ca(e));t&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:t.x,top:t.y,behavior:e.behavior}):window.scrollTo(t.x,t.y))}var ha,fa=Wr&&((-1===(ha=window.navigator.userAgent).indexOf("Android 2.")&&-1===ha.indexOf("Android 4.0")||-1===ha.indexOf("Mobile Safari")||-1!==ha.indexOf("Chrome")||-1!==ha.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function ma(e,t){aa();var n=window.history;try{if(t){var o=nr({},n.state);o.key=ta(),n.replaceState(o,"",e)}else n.pushState({key:na(Jr())},"",e)}catch(n){window.location[t?"replace":"assign"](e)}}function ga(e){ma(e,!0)}var va={redirected:2,aborted:4,cancelled:8,duplicated:16};function ya(e,t){return wa(e,t,va.redirected,'Redirected when going from "'+e.fullPath+'" to "'+function(e){if("string"==typeof e)return e;if("path"in e)return e.path;var t={};return xa.forEach((function(n){n in e&&(t[n]=e[n])})),JSON.stringify(t,null,2)}(t)+'" via a navigation guard.')}function ba(e,t){return wa(e,t,va.cancelled,'Navigation cancelled from "'+e.fullPath+'" to "'+t.fullPath+'" with a new navigation.')}function wa(e,t,n,o){var i=new Error(o);return i._isRouter=!0,i.from=e,i.to=t,i.type=n,i}var xa=["params","query","hash"];function ka(e){return Object.prototype.toString.call(e).indexOf("Error")>-1}function _a(e,t){return ka(e)&&e._isRouter&&(null==t||e.type===t)}function Ta(e,t,n){var o=function(i){i>=e.length?n():e[i]?t(e[i],(function(){o(i+1)})):o(i+1)};o(0)}function Ca(e){return function(t,n,o){var i=!1,r=0,a=null;Sa(e,(function(e,t,n,s){if("function"==typeof e&&void 0===e.cid){i=!0,r++;var l,c=Ea((function(t){var i;((i=t).__esModule||Aa&&"Module"===i[Symbol.toStringTag])&&(t=t.default),e.resolved="function"==typeof t?t:$r.extend(t),n.components[s]=t,--r<=0&&o()})),d=Ea((function(e){var t="Failed to resolve async component "+s+": "+e;a||(a=ka(e)?e:new Error(t),o(a))}));try{l=e(c,d)}catch(e){d(e)}if(l)if("function"==typeof l.then)l.then(c,d);else{var u=l.component;u&&"function"==typeof u.then&&u.then(c,d)}}})),i||o()}}function Sa(e,t){return Ia(e.map((function(e){return Object.keys(e.components).map((function(n){return t(e.components[n],e.instances[n],e,n)}))})))}function Ia(e){return Array.prototype.concat.apply([],e)}var Aa="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function Ea(e){var t=!1;return function(){for(var n=[],o=arguments.length;o--;)n[o]=arguments[o];if(!t)return t=!0,e.apply(this,n)}}var Ra=function(e,t){this.router=e,this.base=function(e){if(!e)if(Wr){var t=document.querySelector("base");e=(e=t&&t.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else e="/";"/"!==e.charAt(0)&&(e="/"+e);return e.replace(/\/$/,"")}(t),this.current=fr,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function Da(e,t,n,o){var i=Sa(e,(function(e,o,i,r){var a=function(e,t){"function"!=typeof e&&(e=$r.extend(e));return e.options[t]}(e,t);if(a)return Array.isArray(a)?a.map((function(e){return n(e,o,i,r)})):n(a,o,i,r)}));return Ia(o?i.reverse():i)}function za(e,t){if(t)return function(){return e.apply(t,arguments)}}Ra.prototype.listen=function(e){this.cb=e},Ra.prototype.onReady=function(e,t){this.ready?e():(this.readyCbs.push(e),t&&this.readyErrorCbs.push(t))},Ra.prototype.onError=function(e){this.errorCbs.push(e)},Ra.prototype.transitionTo=function(e,t,n){var o,i=this;try{o=this.router.match(e,this.current)}catch(e){throw this.errorCbs.forEach((function(t){t(e)})),e}var r=this.current;this.confirmTransition(o,(function(){i.updateRoute(o),t&&t(o),i.ensureURL(),i.router.afterHooks.forEach((function(e){e&&e(o,r)})),i.ready||(i.ready=!0,i.readyCbs.forEach((function(e){e(o)})))}),(function(e){n&&n(e),e&&!i.ready&&(_a(e,va.redirected)&&r===fr||(i.ready=!0,i.readyErrorCbs.forEach((function(t){t(e)}))))}))},Ra.prototype.confirmTransition=function(e,t,n){var o=this,i=this.current;this.pending=e;var r,a,s=function(e){!_a(e)&&ka(e)&&(o.errorCbs.length?o.errorCbs.forEach((function(t){t(e)})):console.error(e)),n&&n(e)},l=e.matched.length-1,c=i.matched.length-1;if(vr(e,i)&&l===c&&e.matched[l]===i.matched[c])return this.ensureURL(),e.hash&&ra(this.router,i,e,!1),s(((a=wa(r=i,e,va.duplicated,'Avoided redundant navigation to current location: "'+r.fullPath+'".')).name="NavigationDuplicated",a));var d=function(e,t){var n,o=Math.max(e.length,t.length);for(n=0;n<o&&e[n]===t[n];n++);return{updated:t.slice(0,n),activated:t.slice(n),deactivated:e.slice(n)}}(this.current.matched,e.matched),u=d.updated,p=d.deactivated,h=d.activated,f=[].concat(function(e){return Da(e,"beforeRouteLeave",za,!0)}(p),this.router.beforeHooks,function(e){return Da(e,"beforeRouteUpdate",za)}(u),h.map((function(e){return e.beforeEnter})),Ca(h)),m=function(t,n){if(o.pending!==e)return s(ba(i,e));try{t(e,i,(function(t){!1===t?(o.ensureURL(!0),s(function(e,t){return wa(e,t,va.aborted,'Navigation aborted from "'+e.fullPath+'" to "'+t.fullPath+'" via a navigation guard.')}(i,e))):ka(t)?(o.ensureURL(!0),s(t)):"string"==typeof t||"object"==typeof t&&("string"==typeof t.path||"string"==typeof t.name)?(s(ya(i,e)),"object"==typeof t&&t.replace?o.replace(t):o.push(t)):n(t)}))}catch(e){s(e)}};Ta(f,m,(function(){Ta(function(e){return Da(e,"beforeRouteEnter",(function(e,t,n,o){return function(e,t,n){return function(o,i,r){return e(o,i,(function(e){"function"==typeof e&&(t.enteredCbs[n]||(t.enteredCbs[n]=[]),t.enteredCbs[n].push(e)),r(e)}))}}(e,n,o)}))}(h).concat(o.router.resolveHooks),m,(function(){if(o.pending!==e)return s(ba(i,e));o.pending=null,t(e),o.router.app&&o.router.app.$nextTick((function(){br(e)}))}))}))},Ra.prototype.updateRoute=function(e){this.current=e,this.cb&&this.cb(e)},Ra.prototype.setupListeners=function(){},Ra.prototype.teardown=function(){this.listeners.forEach((function(e){e()})),this.listeners=[],this.current=fr,this.pending=null};var Oa=function(e){function t(t,n){e.call(this,t,n),this._startLocation=Pa(this.base)}return e&&(t.__proto__=e),t.prototype=Object.create(e&&e.prototype),t.prototype.constructor=t,t.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var t=this.router,n=t.options.scrollBehavior,o=fa&&n;o&&this.listeners.push(ia());var i=function(){var n=e.current,i=Pa(e.base);e.current===fr&&i===e._startLocation||e.transitionTo(i,(function(e){o&&ra(t,e,n,!0)}))};window.addEventListener("popstate",i),this.listeners.push((function(){window.removeEventListener("popstate",i)}))}},t.prototype.go=function(e){window.history.go(e)},t.prototype.push=function(e,t,n){var o=this,i=this.current;this.transitionTo(e,(function(e){ma(_r(o.base+e.fullPath)),ra(o.router,e,i,!1),t&&t(e)}),n)},t.prototype.replace=function(e,t,n){var o=this,i=this.current;this.transitionTo(e,(function(e){ga(_r(o.base+e.fullPath)),ra(o.router,e,i,!1),t&&t(e)}),n)},t.prototype.ensureURL=function(e){if(Pa(this.base)!==this.current.fullPath){var t=_r(this.base+this.current.fullPath);e?ma(t):ga(t)}},t.prototype.getCurrentLocation=function(){return Pa(this.base)},t}(Ra);function Pa(e){var t=window.location.pathname,n=t.toLowerCase(),o=e.toLowerCase();return!e||n!==o&&0!==n.indexOf(_r(o+"/"))||(t=t.slice(e.length)),(t||"/")+window.location.search+window.location.hash}var qa=function(e){function t(t,n,o){e.call(this,t,n),o&&function(e){var t=Pa(e);if(!/^\/#/.test(t))return window.location.replace(_r(e+"/#"+t)),!0}(this.base)||ja()}return e&&(t.__proto__=e),t.prototype=Object.create(e&&e.prototype),t.prototype.constructor=t,t.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var t=this.router.options.scrollBehavior,n=fa&&t;n&&this.listeners.push(ia());var o=function(){var t=e.current;ja()&&e.transitionTo(Fa(),(function(o){n&&ra(e.router,o,t,!0),fa||La(o.fullPath)}))},i=fa?"popstate":"hashchange";window.addEventListener(i,o),this.listeners.push((function(){window.removeEventListener(i,o)}))}},t.prototype.push=function(e,t,n){var o=this,i=this.current;this.transitionTo(e,(function(e){Ma(e.fullPath),ra(o.router,e,i,!1),t&&t(e)}),n)},t.prototype.replace=function(e,t,n){var o=this,i=this.current;this.transitionTo(e,(function(e){La(e.fullPath),ra(o.router,e,i,!1),t&&t(e)}),n)},t.prototype.go=function(e){window.history.go(e)},t.prototype.ensureURL=function(e){var t=this.current.fullPath;Fa()!==t&&(e?Ma(t):La(t))},t.prototype.getCurrentLocation=function(){return Fa()},t}(Ra);function ja(){var e=Fa();return"/"===e.charAt(0)||(La("/"+e),!1)}function Fa(){var e=window.location.href,t=e.indexOf("#");return t<0?"":e=e.slice(t+1)}function Va(e){var t=window.location.href,n=t.indexOf("#");return(n>=0?t.slice(0,n):t)+"#"+e}function Ma(e){fa?ma(Va(e)):window.location.hash=e}function La(e){fa?ga(Va(e)):window.location.replace(Va(e))}var Ha=function(e){function t(t,n){e.call(this,t,n),this.stack=[],this.index=-1}return e&&(t.__proto__=e),t.prototype=Object.create(e&&e.prototype),t.prototype.constructor=t,t.prototype.push=function(e,t,n){var o=this;this.transitionTo(e,(function(e){o.stack=o.stack.slice(0,o.index+1).concat(e),o.index++,t&&t(e)}),n)},t.prototype.replace=function(e,t,n){var o=this;this.transitionTo(e,(function(e){o.stack=o.stack.slice(0,o.index).concat(e),t&&t(e)}),n)},t.prototype.go=function(e){var t=this,n=this.index+e;if(!(n<0||n>=this.stack.length)){var o=this.stack[n];this.confirmTransition(o,(function(){var e=t.current;t.index=n,t.updateRoute(o),t.router.afterHooks.forEach((function(t){t&&t(o,e)}))}),(function(e){_a(e,va.duplicated)&&(t.index=n)}))}},t.prototype.getCurrentLocation=function(){var e=this.stack[this.stack.length-1];return e?e.fullPath:"/"},t.prototype.ensureURL=function(){},t}(Ra),Na=function(e){void 0===e&&(e={}),this.app=null,this.apps=[],this.options=e,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Kr(e.routes||[],this);var t=e.mode||"hash";switch(this.fallback="history"===t&&!fa&&!1!==e.fallback,this.fallback&&(t="hash"),Wr||(t="abstract"),this.mode=t,t){case"history":this.history=new Oa(this,e.base);break;case"hash":this.history=new qa(this,e.base,this.fallback);break;case"abstract":this.history=new Ha(this,e.base);break;default:0}},$a={currentRoute:{configurable:!0}};Na.prototype.match=function(e,t,n){return this.matcher.match(e,t,n)},$a.currentRoute.get=function(){return this.history&&this.history.current},Na.prototype.init=function(e){var t=this;if(this.apps.push(e),e.$once("hook:destroyed",(function(){var n=t.apps.indexOf(e);n>-1&&t.apps.splice(n,1),t.app===e&&(t.app=t.apps[0]||null),t.app||t.history.teardown()})),!this.app){this.app=e;var n=this.history;if(n instanceof Oa||n instanceof qa){var o=function(e){n.setupListeners(),function(e){var o=n.current,i=t.options.scrollBehavior;fa&&i&&"fullPath"in e&&ra(t,e,o,!1)}(e)};n.transitionTo(n.getCurrentLocation(),o,o)}n.listen((function(e){t.apps.forEach((function(t){t._route=e}))}))}},Na.prototype.beforeEach=function(e){return Ba(this.beforeHooks,e)},Na.prototype.beforeResolve=function(e){return Ba(this.resolveHooks,e)},Na.prototype.afterEach=function(e){return Ba(this.afterHooks,e)},Na.prototype.onReady=function(e,t){this.history.onReady(e,t)},Na.prototype.onError=function(e){this.history.onError(e)},Na.prototype.push=function(e,t,n){var o=this;if(!t&&!n&&"undefined"!=typeof Promise)return new Promise((function(t,n){o.history.push(e,t,n)}));this.history.push(e,t,n)},Na.prototype.replace=function(e,t,n){var o=this;if(!t&&!n&&"undefined"!=typeof Promise)return new Promise((function(t,n){o.history.replace(e,t,n)}));this.history.replace(e,t,n)},Na.prototype.go=function(e){this.history.go(e)},Na.prototype.back=function(){this.go(-1)},Na.prototype.forward=function(){this.go(1)},Na.prototype.getMatchedComponents=function(e){var t=e?e.matched?e:this.resolve(e).route:this.currentRoute;return t?[].concat.apply([],t.matched.map((function(e){return Object.keys(e.components).map((function(t){return e.components[t]}))}))):[]},Na.prototype.resolve=function(e,t,n){var o=Nr(e,t=t||this.history.current,n,this),i=this.match(o,t),r=i.redirectedFrom||i.fullPath;return{location:o,route:i,href:function(e,t,n){var o="hash"===n?"#"+t:t;return e?_r(e+"/"+o):o}(this.history.base,r,this.mode),normalizedTo:o,resolved:i}},Na.prototype.getRoutes=function(){return this.matcher.getRoutes()},Na.prototype.addRoute=function(e,t){this.matcher.addRoute(e,t),this.history.current!==fr&&this.history.transitionTo(this.history.getCurrentLocation())},Na.prototype.addRoutes=function(e){this.matcher.addRoutes(e),this.history.current!==fr&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(Na.prototype,$a);var Ua=Na;function Ba(e,t){return e.push(t),function(){var n=e.indexOf(t);n>-1&&e.splice(n,1)}}Na.install=function e(t){if(!e.installed||$r!==t){e.installed=!0,$r=t;var n=function(e){return void 0!==e},o=function(e,t){var o=e.$options._parentVnode;n(o)&&n(o=o.data)&&n(o=o.registerRouteInstance)&&o(e,t)};t.mixin({beforeCreate:function(){n(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),t.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,o(this,this)},destroyed:function(){o(this)}}),Object.defineProperty(t.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(t.prototype,"$route",{get:function(){return this._routerRoot._route}}),t.component("RouterView",wr),t.component("RouterLink",Br);var i=t.config.optionMergeStrategies;i.beforeRouteEnter=i.beforeRouteLeave=i.beforeRouteUpdate=i.created}},Na.version="3.6.5",Na.isNavigationFailure=_a,Na.NavigationFailureType=va,Na.START_LOCATION=fr,Wr&&window.Vue&&window.Vue.use(Na);n(105);n(13),n(131);var Ga={NotFound:()=>Promise.all([n.e(0),n.e(4)]).then(n.bind(null,335)),Layout:()=>Promise.all([n.e(0),n.e(2)]).then(n.bind(null,334))},Wa={"v-3300e2e2":()=>n.e(6).then(n.bind(null,336)),"v-11cfbe2c":()=>n.e(7).then(n.bind(null,337)),"v-ebc6ecc8":()=>n.e(8).then(n.bind(null,338)),"v-4d77aef9":()=>n.e(9).then(n.bind(null,339)),"v-0c78072d":()=>n.e(10).then(n.bind(null,340)),"v-51ae0d5a":()=>n.e(11).then(n.bind(null,341)),"v-c711cda8":()=>n.e(12).then(n.bind(null,342)),"v-0d1770c8":()=>n.e(13).then(n.bind(null,343)),"v-d9328e42":()=>n.e(14).then(n.bind(null,344)),"v-b84e0cea":()=>n.e(15).then(n.bind(null,345)),"v-70336c66":()=>n.e(16).then(n.bind(null,346)),"v-54036cfc":()=>n.e(17).then(n.bind(null,347)),"v-cc7d6564":()=>n.e(18).then(n.bind(null,348)),"v-5c59659c":()=>n.e(19).then(n.bind(null,349)),"v-70767908":()=>n.e(20).then(n.bind(null,350)),"v-0ef24a56":()=>Promise.all([n.e(0),n.e(5)]).then(n.bind(null,351))};function Qa(e){const t=Object.create(null);return function(n){return t[n]||(t[n]=e(n))}}const Ya=/-(\w)/g,Ka=Qa(e=>e.replace(Ya,(e,t)=>t?t.toUpperCase():"")),Xa=/\B([A-Z])/g,Za=Qa(e=>e.replace(Xa,"-$1").toLowerCase()),Ja=Qa(e=>e.charAt(0).toUpperCase()+e.slice(1));function es(e,t){if(!t)return;if(e(t))return e(t);return t.includes("-")?e(Ja(Ka(t))):e(Ja(t))||e(Za(t))}const ts=Object.assign({},Ga,Wa),ns=e=>ts[e],os=e=>Wa[e],is=e=>Ga[e],rs=e=>Gn.component(e);function as(e){return es(os,e)}function ss(e){return es(is,e)}function ls(e){return es(ns,e)}function cs(e){return es(rs,e)}function ds(...e){return Promise.all(e.filter(e=>e).map(async e=>{if(!cs(e)&&ls(e)){const t=await ls(e)();Gn.component(e,t.default)}}))}function us(e,t){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[e]=t)}var ps=n(93),hs=n.n(ps),fs=n(94),ms=n.n(fs),gs={created(){if(this.siteMeta=this.$site.headTags.filter(([e])=>"meta"===e).map(([e,t])=>t),this.$ssrContext){const t=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=t)?e.map(e=>{let t="<meta";return Object.keys(e).forEach(n=>{t+=` ${n}="${ms()(e[n])}"`}),t+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=ys(this.$canonicalUrl)}var e},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const e=this.getMergedMetaTags();this.currentMetaTags=bs(e,this.currentMetaTags)},getMergedMetaTags(){const e=this.$page.frontmatter.meta||[];return hs()([{name:"description",content:this.$description}],e,this.siteMeta,ws)},updateCanonicalLink(){vs(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",ys(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){bs(null,this.currentMetaTags),vs()}};function vs(){const e=document.querySelector("link[rel='canonical']");e&&e.remove()}function ys(e=""){return e?`<link href="${e}" rel="canonical" />`:""}function bs(e,t){if(t&&[...t].filter(e=>e.parentNode===document.head).forEach(e=>document.head.removeChild(e)),e)return e.map(e=>{const t=document.createElement("meta");return Object.keys(e).forEach(n=>{t.setAttribute(n,e[n])}),document.head.appendChild(t),t})}function ws(e){for(const t of["name","property","itemprop"])if(e.hasOwnProperty(t))return e[t]+t;return JSON.stringify(e)}var xs=n(50),ks={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:n.n(xs)()((function(){this.setActiveHash()}),300),setActiveHash(){const e=[].slice.call(document.querySelectorAll(".sidebar-link")),t=[].slice.call(document.querySelectorAll(".header-anchor")).filter(t=>e.some(e=>e.hash===t.hash)),n=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),o=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),i=window.innerHeight+n;for(let e=0;e<t.length;e++){const r=t[e],a=t[e+1],s=0===e&&0===n||n>=r.parentElement.offsetTop+10&&(!a||n<a.parentElement.offsetTop-10),l=decodeURIComponent(this.$route.hash);if(s&&l!==decodeURIComponent(r.hash)){const n=r;if(i===o)for(let n=e+1;n<t.length;n++)if(l===decodeURIComponent(t[n].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(n.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},_s=n(24),Ts=n.n(_s),Cs={mounted(){Ts.a.configure({showSpinner:!1}),this.$router.beforeEach((e,t,n)=>{e.path===t.path||Gn.component(e.name)||Ts.a.start(),n()}),this.$router.afterEach(()=>{Ts.a.done(),this.isSidebarOpen=!1})}};n(240),n(241);class Ss{constructor(){this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}show({text:e="",duration:t=3e3}){let n=document.createElement("div");n.className="message move-in",n.innerHTML=`\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">${e}</div>\n    `,this.containerEl.appendChild(n),t>0&&setTimeout(()=>{this.close(n)},t)}close(e){e.className=e.className.replace("move-in",""),e.className+="move-out",e.addEventListener("animationend",()=>{e.remove()})}}var Is={mounted(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy(){setTimeout(()=>{(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach(e=>{document.querySelectorAll(e).forEach(this.generateCopyButton)})},1e3)},generateCopyButton(e){if(e.classList.contains("codecopy-enabled"))return;const t=document.createElement("i");t.className="code-copy",t.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',t.title="Copy to clipboard",t.addEventListener("click",()=>{this.copyToClipboard(e.innerText)}),e.appendChild(t),e.classList.add("codecopy-enabled")},copyToClipboard(e){const t=document.createElement("textarea");t.value=e,t.setAttribute("readonly",""),t.style.position="absolute",t.style.left="-9999px",document.body.appendChild(t);const n=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);t.select(),document.execCommand("copy");(new Ss).show({text:"Copy successfully",duration:1e3}),document.body.removeChild(t),n&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(n))}}};!function(e,t){void 0===t&&(t={});var n=t.insertAt;if(e&&"undefined"!=typeof document){var o=document.head||document.getElementsByTagName("head")[0],i=document.createElement("style");i.type="text/css","top"===n&&o.firstChild?o.insertBefore(i,o.firstChild):o.appendChild(i),i.styleSheet?i.styleSheet.cssText=e:i.appendChild(document.createTextNode(e))}}("@media (max-width: 1000px) {\n  .vuepress-plugin-demo-block__h_code {\n    display: none;\n  }\n  .vuepress-plugin-demo-block__app {\n    margin-left: auto !important;\n    margin-right: auto !important;\n  }\n}\n.vuepress-plugin-demo-block__wrapper {\n  margin-top: 10px;\n  border: 1px solid #ebebeb;\n  border-radius: 4px;\n  transition: all 0.2s;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display {\n  height: 400px;\n  display: flex;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__app {\n  width: 300px;\n  border: 1px solid #ebebeb;\n  box-shadow: 1px 1px 3px #ebebeb;\n  margin-right: 5px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code {\n  flex: 1;\n  overflow: auto;\n  height: 100%;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code > pre {\n  overflow: visible;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  max-height: 400px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper div {\n  box-sizing: border-box;\n}\n.vuepress-plugin-demo-block__wrapper:hover {\n  box-shadow: 0 0 11px rgba(33, 33, 33, 0.2);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code {\n  overflow: hidden;\n  height: 0;\n  padding: 0 !important;\n  background-color: #282c34;\n  border-radius: 0 !important;\n  transition: height 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code pre {\n  margin: 0 !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  padding: 20px;\n  border-bottom: 1px solid #ebebeb;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer {\n  position: relative;\n  text-align: center;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__codepen {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__expand::before {\n  border-top: none;\n  border-right: 6px solid transparent;\n  border-bottom: 6px solid #ccc;\n  border-left: 6px solid transparent;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__codepen,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand span,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand::before {\n  border-top-color: #3eaf7c !important;\n  border-bottom-color: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover svg {\n  fill: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand-text {\n  transition: all 0.5s;\n  opacity: 0;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:nth-last-child(2) {\n  right: 50px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:last-child {\n  right: 10px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button {\n  border-color: transparent;\n  background-color: transparent;\n  font-size: 14px;\n  color: #3eaf7c;\n  cursor: pointer;\n  outline: none;\n  margin: 0;\n  width: 46px;\n  position: relative;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::before {\n  content: attr(data-tip);\n  white-space: nowrap;\n  position: absolute;\n  top: -30px;\n  left: 50%;\n  color: #eee;\n  line-height: 1;\n  z-index: 1000;\n  border-radius: 4px;\n  padding: 6px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  background-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::after {\n  content: '' !important;\n  display: block;\n  position: absolute;\n  left: 50%;\n  top: -5px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  border: 5px solid transparent;\n  border-top-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button svg {\n  width: 34px;\n  height: 20px;\n  fill: #ccc;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__codepen {\n  position: absolute;\n  top: 10px;\n  transition: all 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand {\n  position: relative;\n  width: 100px;\n  height: 40px;\n  margin: 0;\n  color: #3eaf7c;\n  font-size: 14px;\n  background-color: transparent;\n  border-color: transparent;\n  outline: none;\n  transition: all 0.5s;\n  cursor: pointer;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand::before {\n  content: \"\";\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  width: 0;\n  height: 0;\n  border-top: 6px solid #ccc;\n  border-right: 6px solid transparent;\n  border-left: 6px solid transparent;\n  -webkit-transform: translate(-50%, -50%);\n          transform: translate(-50%, -50%);\n}\n");var As={jsLib:[],cssLib:[],jsfiddle:!0,codepen:!0,codepenLayout:"left",codepenJsProcessor:"babel",codepenEditors:"101",horizontal:!1,vue:"https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js",react:"https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js",reactDOM:"https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"},Es={},Rs=function(e){return'<div id="app">\n'.concat(e,"\n</div>")},Ds=function(e){return window.$VUEPRESS_DEMO_BLOCK&&void 0!==window.$VUEPRESS_DEMO_BLOCK[e]?window.$VUEPRESS_DEMO_BLOCK[e]:As[e]},zs=function e(t,n,o){var i=document.createElement(t);return n&&Object.keys(n).forEach((function(e){if(e.indexOf("data"))i[e]=n[e];else{var t=e.replace("data","");i.dataset[t]=n[e]}})),o&&o.forEach((function(t){var n=t.tag,o=t.attrs,r=t.children;i.appendChild(e(n,o,r))})),i},Os=function(e,t,n){var o,i=(o=e.querySelectorAll(".".concat(t)),Array.prototype.slice.call(o));return 1!==i.length||n?i:i[0]},Ps=function(e,t){var n,o,i=e.match(/<style>([\s\S]+)<\/style>/),r=e.match(/<template>([\s\S]+)<\/template>/),a=e.match(/<script>([\s\S]+)<\/script>/),s={css:i&&i[1].replace(/^\n|\n$/g,""),html:r&&r[1].replace(/^\n|\n$/g,""),js:a&&a[1].replace(/^\n|\n$/g,""),jsLib:t.jsLib||[],cssLib:t.cssLib||[]};s.htmlTpl=Rs(s.html),s.jsTpl=(n=s.js,o=n.replace(/export\s+default\s*?\{\n*/,"").replace(/\n*\}\s*$/,"").trim(),"new Vue({\n  el: '#app',\n  ".concat(o,"\n})")),s.script=function(e,t){var n=e.split(/export\s+default/),o="(function() {".concat(n[0]," ; return ").concat(n[1],"})()"),i=window.Babel?window.Babel.transform(o,{presets:["es2015"]}).code:o,r=[eval][0](i);return r.template=t,r}(s.js,s.html);var l=Ds("vue");return s.jsLib.unshift(l),s},qs=function(e,t){var n,o=e.match(/<style>([\s\S]+)<\/style>/),i=e.match(/<html>([\s\S]+)<\/html>/),r=e.match(/<script>([\s\S]+)<\/script>/),a={css:o&&o[1].replace(/^\n|\n$/g,""),html:i&&i[1].replace(/^\n|\n$/g,""),js:r&&r[1].replace(/^\n|\n$/g,""),jsLib:t.jsLib||[],cssLib:t.cssLib||[]};return a.htmlTpl=a.html,a.jsTpl=a.js,a.script=(n=a.js,window.Babel?window.Babel.transform(n,{presets:["es2015"]}).code:n),a},js=function(e){return e=e.replace("export default ","").replace(/App\.__style__(\s*)=(\s*)`([\s\S]*)?`/,""),e+='ReactDOM.render(React.createElement(App), document.getElementById("app"))'};function Fs(){var e=Os(document,"vuepress-plugin-demo-block__wrapper",!0);e.length?e.forEach((function(e){if("true"!==e.dataset.created){e.style.display="block";var t=Os(e,"vuepress-plugin-demo-block__code"),n=Os(e,"vuepress-plugin-demo-block__display"),o=Os(e,"vuepress-plugin-demo-block__footer"),i=Os(n,"vuepress-plugin-demo-block__app"),r=decodeURIComponent(e.dataset.code),a=decodeURIComponent(e.dataset.config),s=decodeURIComponent(e.dataset.type);a=a?JSON.parse(a):{};var l=t.querySelector("div").clientHeight,c="react"===s?function(e,t){var n=(0,window.Babel.transform)(e,{presets:["es2015","react"]}).code,o="(function(exports){var module={};module.exports=exports;".concat(n,";return module.exports.__esModule?module.exports.default:module.exports;})({})"),i=new Function("return ".concat(o))(),r={js:i,css:i.__style__||"",jsLib:t.jsLib||[],cssLib:t.cssLib||[],jsTpl:js(e),htmlTpl:Rs("")},a=Ds("react"),s=Ds("reactDOM");return r.jsLib.unshift(a,s),r}(r,a):"vanilla"===s?qs(r,a):Ps(r,a),d=zs("button",{className:"".concat("vuepress-plugin-demo-block__expand")});if(o.appendChild(d),d.addEventListener("click",Vs.bind(null,d,l,t,o)),Ds("jsfiddle")&&o.appendChild(function(e){var t=e.css,n=e.htmlTpl,o=e.jsTpl,i=e.jsLib,r=e.cssLib,a=i.concat(r).concat(Ds("cssLib")).concat(Ds("jsLib")).join(",");return zs("form",{className:"vuepress-plugin-demo-block__jsfiddle",target:"_blank",action:"https://jsfiddle.net/api/post/library/pure/",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"css",value:t}},{tag:"input",attrs:{type:"hidden",name:"html",value:n}},{tag:"input",attrs:{type:"hidden",name:"js",value:o}},{tag:"input",attrs:{type:"hidden",name:"panel_js",value:3}},{tag:"input",attrs:{type:"hidden",name:"wrap",value:1}},{tag:"input",attrs:{type:"hidden",name:"resources",value:a}},{tag:"button",attrs:{type:"submit",className:"vuepress-plugin-demo-block__button",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088289967" class="icon" style="" viewBox="0 0 1170 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1952" xmlns:xlink="http://www.w3.org/1999/xlink" width="228.515625" height="200"><defs><style type="text/css"></style></defs><path d="M1028.571429 441.142857q63.428571 26.285714 102.571428 83.142857T1170.285714 650.857143q0 93.714286-67.428571 160.285714T940 877.714286q-2.285714 0-6.571429-0.285715t-6-0.285714H232q-97.142857-5.714286-164.571429-71.714286T0 645.142857q0-62.857143 31.428571-116t84-84q-6.857143-22.285714-6.857142-46.857143 0-65.714286 46.857142-112t113.714286-46.285714q54.285714 0 98.285714 33.142857 42.857143-88 127.142858-141.714286t186.571428-53.714285q94.857143 0 174.857143 46T982.571429 248.571429t46.571428 172q0 3.428571-0.285714 10.285714t-0.285714 10.285714zM267.428571 593.142857q0 69.714286 48 110.285714t118.857143 40.571429q78.285714 0 137.142857-56.571429-9.142857-11.428571-27.142857-32.285714T519.428571 626.285714q-38.285714 37.142857-82.285714 37.142857-31.428571 0-53.428571-19.142857T361.714286 594.285714q0-30.285714 22-49.714285t52.285714-19.428572q25.142857 0 48.285714 12t41.714286 31.428572 37.142857 42.857142 39.428572 46.857143 44 42.857143 55.428571 31.428572 69.428571 12q69.142857 0 116.857143-40.857143T936 594.857143q0-69.142857-48-109.714286t-118.285714-40.571428q-81.714286 0-137.714286 55.428571l53.142857 61.714286q37.714286-36.571429 81.142857-36.571429 29.714286 0 52.571429 18.857143t22.857143 48q0 32.571429-21.142857 52.285714t-53.714286 19.714286q-24.571429 0-47.142857-12t-41.142857-31.428571-37.428572-42.857143-39.714286-46.857143-44.285714-42.857143-55.142857-31.428571T434.285714 444.571429q-69.714286 0-118.285714 40.285714T267.428571 593.142857z" p-id="1953"></path></svg>',datatip:"JSFiddle"}}])}(c)),Ds("codepen")&&o.appendChild(function(e){var t=e.css,n=e.htmlTpl,o=e.jsTpl,i=e.jsLib,r=e.cssLib,a=JSON.stringify({css:t,html:n,js:o,js_external:i.concat(Ds("jsLib")).join(";"),css_external:r.concat(Ds("cssLib")).join(";"),layout:Ds("codepenLayout"),js_pre_processor:Ds("codepenJsProcessor"),editors:Ds("codepenEditors")});return zs("form",{className:"vuepress-plugin-demo-block__codepen",target:"_blank",action:"https://codepen.io/pen/define",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"data",value:a}},{tag:"button",attrs:{type:"submit",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088271207" class="icon" style="" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1737" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><defs><style type="text/css"></style></defs><path d="M123.428571 668l344.571429 229.714286v-205.142857L277.142857 565.142857z m-35.428571-82.285714l110.285714-73.714286-110.285714-73.714286v147.428572z m468 312l344.571429-229.714286-153.714286-102.857143-190.857143 127.428572v205.142857z m-44-281.714286l155.428571-104-155.428571-104-155.428571 104zM277.142857 458.857143l190.857143-127.428572V126.285714L123.428571 356z m548.571429 53.142857l110.285714 73.714286V438.285714z m-78.857143-53.142857l153.714286-102.857143-344.571429-229.714286v205.142857z m277.142857-102.857143v312q0 23.428571-19.428571 36.571429l-468 312q-12 7.428571-24.571429 7.428571t-24.571429-7.428571L19.428571 704.571429q-19.428571-13.142857-19.428571-36.571429V356q0-23.428571 19.428571-36.571429L487.428571 7.428571q12-7.428571 24.571429-7.428571t24.571429 7.428571l468 312q19.428571 13.142857 19.428571 36.571429z" p-id="1738"></path></svg>',className:"vuepress-plugin-demo-block__button",datatip:"Codepen"}}])}(c)),void 0!==a.horizontal?a.horizontal:Ds("horizontal")){e.classList.add("vuepress-plugin-demo-block__horizontal");var u=t.firstChild.cloneNode(!0);u.classList.add("vuepress-plugin-demo-block__h_code"),n.appendChild(u)}if(c.css&&function(e){if(!Es[e]){var t=zs("style",{innerHTML:e});document.body.appendChild(t),Es[e]=!0}}(c.css),"react"===s)ReactDOM.render(React.createElement(c.js),i);else if("vue"===s){var p=(new(Vue.extend(c.script))).$mount();i.appendChild(p.$el)}else"vanilla"===s&&(i.innerHTML=c.html,new Function("return (function(){".concat(c.script,"})()"))());e.dataset.created="true"}})):setTimeout((function(e){Fs()}),300)}function Vs(e,t,n,o){var i="1"!==e.dataset.isExpand;n.style.height=i?"".concat(t,"px"):0,i?o.classList.add("vuepress-plugin-demo-block__show-link"):o.classList.remove("vuepress-plugin-demo-block__show-link"),e.dataset.isExpand=i?"1":"0"}var Ms={mounted:function(){window.$VUEPRESS_DEMO_BLOCK={jsfiddle:!1,codepen:!0,horizontal:!1},Fs()},updated:function(){Fs()}},Ls="auto",Hs="zoom-in",Ns="zoom-out",$s="grab",Us="move";function Bs(e,t,n){var o=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],i={passive:!1};o?e.addEventListener(t,n,i):e.removeEventListener(t,n,i)}function Gs(e,t){if(e){var n=new Image;n.onload=function(){t&&t(n)},n.src=e}}function Ws(e){return e.dataset.original?e.dataset.original:"A"===e.parentNode.tagName?e.parentNode.getAttribute("href"):null}function Qs(e,t,n){!function(e){var t=Ys,n=Ks;if(e.transition){var o=e.transition;delete e.transition,e[t]=o}if(e.transform){var i=e.transform;delete e.transform,e[n]=i}}(t);var o=e.style,i={};for(var r in t)n&&(i[r]=o[r]||""),o[r]=t[r];return i}var Ys="transition",Ks="transform",Xs="transform",Zs="transitionend";var Js=function(){},el={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Js,onClose:Js,onGrab:Js,onMove:Js,onRelease:Js,onBeforeOpen:Js,onBeforeClose:Js,onBeforeGrab:Js,onBeforeRelease:Js,onImageLoading:Js,onImageLoaded:Js},tl={init:function(e){var t,n;t=this,n=e,Object.getOwnPropertyNames(Object.getPrototypeOf(t)).forEach((function(e){t[e]=t[e].bind(n)}))},click:function(e){if(e.preventDefault(),ol(e))return window.open(this.target.srcOriginal||e.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(e.currentTarget)},scroll:function(){var e=document.documentElement||document.body.parentNode||document.body,t=window.pageXOffset||e.scrollLeft,n=window.pageYOffset||e.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:t,y:n});var o=this.lastScrollPosition.x-t,i=this.lastScrollPosition.y-n,r=this.options.scrollThreshold;(Math.abs(i)>=r||Math.abs(o)>=r)&&(this.lastScrollPosition=null,this.close())},keydown:function(e){(function(e){return"Escape"===(e.key||e.code)||27===e.keyCode})(e)&&(this.released?this.close():this.release(this.close))},mousedown:function(e){if(nl(e)&&!ol(e)){e.preventDefault();var t=e.clientX,n=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,n)}.bind(this),200)}},mousemove:function(e){this.released||this.move(e.clientX,e.clientY)},mouseup:function(e){nl(e)&&!ol(e)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(e){e.preventDefault();var t=e.touches[0],n=t.clientX,o=t.clientY;this.pressTimer=setTimeout(function(){this.grab(n,o)}.bind(this),200)},touchmove:function(e){if(!this.released){var t=e.touches[0],n=t.clientX,o=t.clientY;this.move(n,o)}},touchend:function(e){(function(e){e.targetTouches.length})(e)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function nl(e){return 0===e.button}function ol(e){return e.metaKey||e.ctrlKey}var il={init:function(e){this.el=document.createElement("div"),this.instance=e,this.parent=document.body,Qs(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(e.options),Bs(this.el,"click",e.handler.clickOverlay.bind(e))},updateStyle:function(e){Qs(this.el,{zIndex:e.zIndex,backgroundColor:e.bgColor,transition:"opacity\n        "+e.transitionDuration+"s\n        "+e.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},rl="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},al=function(){function e(e,t){for(var n=0;n<t.length;n++){var o=t[n];o.enumerable=o.enumerable||!1,o.configurable=!0,"value"in o&&(o.writable=!0),Object.defineProperty(e,o.key,o)}}return function(t,n,o){return n&&e(t.prototype,n),o&&e(t,o),t}}(),sl=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var o in n)Object.prototype.hasOwnProperty.call(n,o)&&(e[o]=n[o])}return e},ll={init:function(e,t){this.el=e,this.instance=t,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=Ws(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var e=this.instance.options,t=e.zIndex,n=e.enableGrab,o=e.transitionDuration,i=e.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:t+1,cursor:n?$s:Ns,transition:Xs+"\n        "+o+"s\n        "+i,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=Qs(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,Qs(this.el,{transform:"none"})},grab:function(e,t,n){var o=cl(),i=o.x-e,r=o.y-t;Qs(this.el,{cursor:Us,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+r)+"px, 0px)\n        scale("+(this.scale.x+n)+","+(this.scale.y+n)+")"})},move:function(e,t,n){var o=cl(),i=o.x-e,r=o.y-t;Qs(this.el,{transition:Xs,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+r)+"px, 0px)\n        scale("+(this.scale.x+n)+","+(this.scale.y+n)+")"})},restoreCloseStyle:function(){Qs(this.el,this.styleClose)},restoreOpenStyle:function(){Qs(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var e=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var t=this.el.cloneNode(!1);t.setAttribute("src",this.srcOriginal),t.style.position="fixed",t.style.visibility="hidden",e.appendChild(t),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),e.removeChild(t)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var e=cl(),t=this.rect.left+this.rect.width/2,n=this.rect.top+this.rect.height/2;return{x:e.x-t,y:e.y-n}},calculateScale:function(){var e=this.el.dataset,t=e.zoomingHeight,n=e.zoomingWidth,o=this.instance.options,i=o.customSize,r=o.scaleBase;if(!i&&t&&n)return{x:n/this.rect.width,y:t/this.rect.height};if(i&&"object"===(void 0===i?"undefined":rl(i)))return{x:i.width/this.rect.width,y:i.height/this.rect.height};var a=this.rect.width/2,s=this.rect.height/2,l=cl(),c={x:l.x-a,y:l.y-s},d=c.x/a,u=c.y/s,p=r+Math.min(d,u);if(i&&"string"==typeof i){var h=n||this.el.naturalWidth,f=t||this.el.naturalHeight,m=parseFloat(i)*h/(100*this.rect.width),g=parseFloat(i)*f/(100*this.rect.height);if(p>m||p>g)return{x:m,y:g}}return{x:p,y:p}}};function cl(){var e=document.documentElement;return{x:Math.min(e.clientWidth,window.innerWidth)/2,y:Math.min(e.clientHeight,window.innerHeight)/2}}function dl(e,t,n){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(o){Bs(e,o,t[o],n)}))}var ul=function(){function e(t){!function(e,t){if(!(e instanceof t))throw new TypeError("Cannot call a class as a function")}(this,e),this.target=Object.create(ll),this.overlay=Object.create(il),this.handler=Object.create(tl),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=sl({},el,t),this.overlay.init(this),this.handler.init(this)}return al(e,[{key:"listen",value:function(e){if("string"==typeof e)for(var t=document.querySelectorAll(e),n=t.length;n--;)this.listen(t[n]);else"IMG"===e.tagName&&(e.style.cursor=Hs,Bs(e,"click",this.handler.click),this.options.preloadImage&&Gs(Ws(e)));return this}},{key:"config",value:function(e){return e?(sl(this.options,e),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(e){var t=this,n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var o="string"==typeof e?document.querySelector(e):e;if("IMG"===o.tagName){if(this.options.onBeforeOpen(o),this.target.init(o,this),!this.options.preloadImage){var i=this.target.srcOriginal;null!=i&&(this.options.onImageLoading(o),Gs(i,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),Bs(document,"scroll",this.handler.scroll),Bs(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&Bs(window,"resize",this.handler.resizeWindow);var r=function e(){Bs(o,Zs,e,!1),t.lock=!1,t.target.upgradeSource(),t.options.enableGrab&&dl(document,t.handler,!0),n(o)};return Bs(o,Zs,r),this}}}},{key:"close",value:function(){var e=this,t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var n=this.target.el;this.options.onBeforeClose(n),this.lock=!0,this.body.style.cursor=Ls,this.overlay.fadeOut(),this.target.zoomOut(),Bs(document,"scroll",this.handler.scroll,!1),Bs(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&Bs(window,"resize",this.handler.resizeWindow,!1);var o=function o(){Bs(n,Zs,o,!1),e.shown=!1,e.lock=!1,e.target.downgradeSource(),e.options.enableGrab&&dl(document,e.handler,!1),e.target.restoreCloseStyle(),e.overlay.remove(),t(n)};return Bs(n,Zs,o),this}}},{key:"grab",value:function(e,t){var n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,o=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var i=this.target.el;this.options.onBeforeGrab(i),this.released=!1,this.target.grab(e,t,n);var r=function e(){Bs(i,Zs,e,!1),o(i)};return Bs(i,Zs,r),this}}},{key:"move",value:function(e,t){var n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,o=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Us,this.target.move(e,t,n);var i=this.target.el,r=function e(){Bs(i,Zs,e,!1),o(i)};return Bs(i,Zs,r),this}}},{key:"release",value:function(){var e=this,t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var n=this.target.el;this.options.onBeforeRelease(n),this.lock=!0,this.body.style.cursor=Ls,this.target.restoreOpenStyle();var o=function o(){Bs(n,Zs,o,!1),e.lock=!1,e.released=!0,t(n)};return Bs(n,Zs,o),this}}}]),e}();const pl=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),hl=Number("500");class fl{constructor(){this.instance=new ul(pl)}update(e=".theme-vdoing-content img:not(.no-zoom)"){"undefined"!=typeof window&&this.instance.listen(e)}updateDelay(e=".theme-vdoing-content img:not(.no-zoom)",t=hl){setTimeout(()=>this.update(e),t)}}var ml=[gs,ks,Cs,Is,Ms,{watch:{"$page.path"(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted(){this.$vuepress.zooming=new fl,this.$vuepress.zooming.updateDelay()}}],gl={name:"GlobalLayout",computed:{layout(){const e=this.getLayout();return us("layout",e),Gn.component(e)}},methods:{getLayout(){if(this.$page.path){const e=this.$page.frontmatter.layout;return e&&(this.$vuepress.getLayoutAsyncComponent(e)||this.$vuepress.getVueComponent(e))?e:"Layout"}return"NotFound"}}},vl=n(7),yl=Object(vl.a)(gl,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(e,t,n){switch(t){case"components":e[t]||(e[t]={}),Object.assign(e[t],n);break;case"mixins":e[t]||(e[t]=[]),e[t].push(...n);break;default:throw new Error("Unknown option name.")}}(yl,"mixins",ml);const bl=[{name:"v-3300e2e2",path:"/pages/dc46b8/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-3300e2e2").then(n)}},{path:"/pages/dc46b8/index.html",redirect:"/pages/dc46b8/"},{path:"/10.Getting Started/10.Getting Started/10.Framerate, resolution, bitrate.html",redirect:"/pages/dc46b8/"},{name:"v-11cfbe2c",path:"/pages/88ed7f/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-11cfbe2c").then(n)}},{path:"/pages/88ed7f/index.html",redirect:"/pages/88ed7f/"},{path:"/10.Getting Started/10.Getting Started/20.What is VFI.html",redirect:"/pages/88ed7f/"},{name:"v-ebc6ecc8",path:"/pages/681961/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-ebc6ecc8").then(n)}},{path:"/pages/681961/index.html",redirect:"/pages/681961/"},{path:"/10.Getting Started/10.Getting Started/30.What is SR.html",redirect:"/pages/681961/"},{name:"v-4d77aef9",path:"/pages/0e988c/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-4d77aef9").then(n)}},{path:"/pages/0e988c/index.html",redirect:"/pages/0e988c/"},{path:"/10.Getting Started/10.Getting Started/40.Quick tour of SVFI.html",redirect:"/pages/0e988c/"},{name:"v-0c78072d",path:"/pages/8cc1b5/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-0c78072d").then(n)}},{path:"/pages/8cc1b5/index.html",redirect:"/pages/8cc1b5/"},{path:"/10.Getting Started/20.Related Articles/01.Image representation and quality.html",redirect:"/pages/8cc1b5/"},{name:"v-51ae0d5a",path:"/pages/76d9d4/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-51ae0d5a").then(n)}},{path:"/pages/76d9d4/index.html",redirect:"/pages/76d9d4/"},{path:"/10.Getting Started/20.Related Articles/03.Encodings, Encoders, and Encapsulation Formats.html",redirect:"/pages/76d9d4/"},{name:"v-c711cda8",path:"/pages/7b7d11/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-c711cda8").then(n)}},{path:"/pages/7b7d11/index.html",redirect:"/pages/7b7d11/"},{path:"/10.Getting Started/20.Related Articles/07.what is encoding.html",redirect:"/pages/7b7d11/"},{name:"v-0d1770c8",path:"/pages/89244b/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-0d1770c8").then(n)}},{path:"/pages/89244b/index.html",redirect:"/pages/89244b/"},{path:"/10.Getting Started/20.Related Articles/08.What is HDR.html",redirect:"/pages/89244b/"},{name:"v-d9328e42",path:"/pages/f8b952/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-d9328e42").then(n)}},{path:"/pages/f8b952/index.html",redirect:"/pages/f8b952/"},{path:"/20.Manuals/10.Quick Start.html",redirect:"/pages/f8b952/"},{name:"v-b84e0cea",path:"/pages/052617/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-b84e0cea").then(n)}},{path:"/pages/052617/index.html",redirect:"/pages/052617/"},{path:"/20.Manuals/20.Option Manuals.html",redirect:"/pages/052617/"},{name:"v-70336c66",path:"/pages/ceb849/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-70336c66").then(n)}},{path:"/pages/ceb849/index.html",redirect:"/pages/ceb849/"},{path:"/20.Manuals/21.Advanced CLI.html",redirect:"/pages/ceb849/"},{name:"v-54036cfc",path:"/pages/9cc27d/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-54036cfc").then(n)}},{path:"/pages/9cc27d/index.html",redirect:"/pages/9cc27d/"},{path:"/30.FAQ/01.Q&A.html",redirect:"/pages/9cc27d/"},{name:"v-cc7d6564",path:"/pages/18309a/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-cc7d6564").then(n)}},{path:"/pages/18309a/index.html",redirect:"/pages/18309a/"},{path:"/30.FAQ/10.Tips.html",redirect:"/pages/18309a/"},{name:"v-5c59659c",path:"/pages/1b12ed/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-5c59659c").then(n)}},{path:"/pages/1b12ed/index.html",redirect:"/pages/1b12ed/"},{path:"/40.Support/01.Support.html",redirect:"/pages/1b12ed/"},{name:"v-70767908",path:"/blog/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-70767908").then(n)}},{path:"/blog/index.html",redirect:"/blog/"},{path:"/@pages/archivesPage.html",redirect:"/blog/"},{name:"v-0ef24a56",path:"/",component:yl,beforeEnter:(e,t,n)=>{ds("Layout","v-0ef24a56").then(n)}},{path:"/index.html",redirect:"/"},{path:"*",component:yl}],wl={title:"SVFI Doc",description:"SVFI User Documentation",base:"/",headTags:[["link",{rel:"icon",href:"/img/svfi.ico"}],["meta",{name:"keywords",content:"vuepress,theme,blog,vdoing"}],["meta",{name:"theme-color",content:"#11a8cd"}],["meta",{name:"wwads-cn-verify",content:"6c4b761a28b734fe93831e3fb400ce87"}],["script",{src:"https://cdn.wwads.cn/js/makemoney.js",type:"text/javascript"}],["meta",{name:"referrer",content:"no-referrer"}]],pages:[{title:"What is frame rate, resolution and bitrate",frontmatter:{title:"What is frame rate, resolution and bitrate",date:"2022-08-19T12:49:11.000Z",permalink:"/pages/dc46b8/"},regularPath:"/10.Getting%20Started/10.Getting%20Started/10.Framerate,%20resolution,%20bitrate.html",relativePath:"10.Getting Started/10.Getting Started/10.Framerate, resolution, bitrate.md",key:"v-3300e2e2",path:"/pages/dc46b8/",headers:[{level:2,title:"What is frame rate and what is resolution",slug:"what-is-frame-rate-and-what-is-resolution",normalizedTitle:"what is frame rate and what is resolution",charIndex:89},{level:2,title:"Bitrate",slug:"bitrate",normalizedTitle:"bitrate",charIndex:1432},{level:3,title:"Rate Control Method",slug:"rate-control-method",normalizedTitle:"rate control method",charIndex:2429},{level:4,title:"CBR(Constant Bitrate)",slug:"cbr-constant-bitrate",normalizedTitle:"cbr(constant bitrate)",charIndex:2590},{level:4,title:"VBR(Variable Bitrate)",slug:"vbr-variable-bitrate",normalizedTitle:"vbr(variable bitrate)",charIndex:3266},{level:4,title:"CQP (Constant QP)",slug:"cqp-constant-qp",normalizedTitle:"cqp (constant qp)",charIndex:4414},{level:4,title:"CRF(Constant Rate Factor)",slug:"crf-constant-rate-factor",normalizedTitle:"crf(constant rate factor)",charIndex:5451}],headersStr:"What is frame rate and what is resolution Bitrate Rate Control Method CBR(Constant Bitrate) VBR(Variable Bitrate) CQP (Constant QP) CRF(Constant Rate Factor)",content:'Tip\n\nThe following content is taken from FXXS-Encode-Guide, thanks for their efforts\n\n\n# What is frame rate and what is resolution\n\nVideo is made up of continuous images. Each image, we call a frame. Images are made up of pixels. The number of pixels in an image is called the resolution of the image. For example, an image of 1920x1080 means that it is composed of 1920x1080 pixels horizontally and vertically. The resolution of the video is the resolution of each frame of the image.\n\nA video, how many images are composed of each second, is called the frame-rate of this video. Common frame rates are 24000/1001=23.976, 30000/1001=29.970, 60000/1001=59.940, 25.000, 50.000 and so on. This number is the number of images flashed in one second. For example, 23.976 means that there are 24000 images in 1001 seconds. The frame rate of the video can be constant (cfr, Const Frame-Rate) or variable (vfr, Variable Frame-Rate)\n\nWarning\n\nPlease note that SVFI does not support variable frame rate (vfr) processing, and feeding such video to SVFI will cause the audio and picture to be out of sync.\n\nSVFI only supports frame rates of integers (such as 25.00, 24.00) and NTSC standard frame rate(frame rates with a denominator of 1001, such as 23.976, 29.97, 59.94) as input.\n\nFeeding other types of frame rates (such as 24.5) will be regarded as NTSC standard frame rates and will cause the output audio and video to be out of sync.\n\n\n# Bitrate\n\nBitrate is defined as video file volume divided by time. The unit is generally Kbps (Kbit/s) or Mbps (Mbit/s). Note that 1B (Byte) = 8b (bit). So a 24 minute, 900MB video:\n\n * Size: 900MB = 900MByte = 7200Mbit\n\n * Time: 24min = 1440s\n\n * Bit rate: 7200/1440 = 5000 Kbps = 5Mbps\n\nWhen the time of video files is basically the same (for example, an episode of anime lasts for around 24 minutes), the bit rate and volume are basically equivalent, and they are parameters used to describe the size of the video. Files with the same length and resolution have different sizes, but the code rate is actually different.\n\nThe bit rate can also be interpreted as the total amount of data used to record video per unit time. A video with a higher bit rate means more data used to record the video, and the potential interpretation is that the video can have better quality. (Note that it is only potential, we will analyze why high bit rate does not necessarily equal high image quality later)\n\n\n# Rate Control Method\n\nBit rate control refers to the process of determining the output bit rate in video encoding. Commonly used bit rate control methods are:\n\n# CBR(Constant Bitrate)\n\nCBR means that the file is at a constant bitrate from beginning to end. Compared with VBR and ABR, the compressed file size is very large, and the video quality will not be significantly improved compared with VBR and ABR.\n\nFeatures:\n\n * The bit rate is stable, but the quality is unstable, and the effective bandwidth utilization rate is not high, especially when the value is set unreasonably. In complex motion scenes, if the set bit rate is not enough, the picture will be very blurred, which greatly affects the viewing experience;\n * However, the output video bit rate is basically stable, which is convenient for calculating the video volume;\n\n# VBR(Variable Bitrate)\n\nDynamic bit rate, that is, there is no fixed bit rate. Audio and video encoding software instantly determines which bit rate to use according to the complexity of the audio and video data during the encode process. This is a way to take quality into account while taking file size into consideration.\n\nCompared to CBR Applicable scenarios: VBR is suitable for scenarios where the bandwidth and encoding speed are not limited, but the quality is high. Especially in complex sports scenes with large motions, it can maintain a relatively high definition and output quality is relatively stable, suitable for on-demand, recording or storage systems that are not sensitive to delay.\n\nFeatures:\n\n * The code rate is unstable, and the quality is basically stable and very high;\n * The encoding speed is generally slow, and the on-demand, download and storage systems can be used first, which is not suitable for low-latency and live broadcast systems;\n * This model does not consider the output video bandwidth at all. For the sake of quality, it takes up as much bit rate as it needs, and does not consider the encoding speed;\n\n# CQP (Constant QP)\n\n> It should be counted as the way the encoder implements VBR\n\nFixed QP, the simplest code rate control method, each frame of image is encoded according to a specific QP, the amount of data encoded in each frame is unknown, neither the code rate priority model nor the quality priority model, but it is Implement the simplest model;\n\nApplicable scenarios: This method is generally not recommended, because this method does not consider the complexity of the encoded content, and processes each frame with the same compression ratio. The output video quality and bit rate are not fixed. Personally, I think only very simple scenes, such as static scenes with little movement, can be used. When encountering complex scenes, the bit rate fluctuations are very large. Or it can be used in algorithm research or verification.\n\nFeatures:\n\n * The instantaneous bit rate will fluctuate with the complexity of the scene;\n * The encoding speed is fast, the control is the simplest, and the QP value of each frame is the same;\n\n# CRF(Constant Rate Factor)\n\nIt should be counted as the way the encoder implements VBR Constant bit rate factor. Target a certain "visual quality" as an output. It does this by reducing the bitrate of frames that are expensive but hard to see with the naked eye (high-speed motion or rich textures) and boosting the bitrate of static frames.\n\nFeatures:\n\nQP changes between frames, QP changes of intra-frame macroblocks, the output bit rate is unknown, and the visual quality of each frame output is basically constant. This method is equivalent to the method of fixed quality mode + limiting bit rate peak value.\n\n * Applicable scenarios: When there are certain requirements for video quality, the CRF value can be simply understood as a fixed output value for video quality expectations. It is hoped that there will be a stable value no matter in complex moving scenes or in simple static situations. Subjective video quality can choose this mode, which is a video quality priority model. Video quality can be simply understood as the clarity of the video, the fineness of the pixels and the fluency of the video.\n * Similar to constant QP, but the pursuit of constant subjectively perceived quality, the instantaneous bit rate will also fluctuate with the complexity of the scene, and the QP values are different between video frames or between internal macroblocks;\n * For scenes with fast motion or rich details, the quantization distortion will be appropriately increased (because the human eye is not sensitive), and vice versa for static or flat areas, the quantization distortion will be reduced;\n\nTip\n\nSVFI has built-in rendering quality CRF (that is, CRF or CQP), target bit rate (CBR) two rate control schemes.\n\nWhen using rendering quality CRF to control bit rate, SVFI uses CRF control for encoders other than NVENC; uses CQP bit rate control for NVENC encoder.\n\nWhen using target bit rate to control the bit rate, uniformly use constant bit rate (CBR) control.',normalizedContent:'tip\n\nthe following content is taken from fxxs-encode-guide, thanks for their efforts\n\n\n# what is frame rate and what is resolution\n\nvideo is made up of continuous images. each image, we call a frame. images are made up of pixels. the number of pixels in an image is called the resolution of the image. for example, an image of 1920x1080 means that it is composed of 1920x1080 pixels horizontally and vertically. the resolution of the video is the resolution of each frame of the image.\n\na video, how many images are composed of each second, is called the frame-rate of this video. common frame rates are 24000/1001=23.976, 30000/1001=29.970, 60000/1001=59.940, 25.000, 50.000 and so on. this number is the number of images flashed in one second. for example, 23.976 means that there are 24000 images in 1001 seconds. the frame rate of the video can be constant (cfr, const frame-rate) or variable (vfr, variable frame-rate)\n\nwarning\n\nplease note that svfi does not support variable frame rate (vfr) processing, and feeding such video to svfi will cause the audio and picture to be out of sync.\n\nsvfi only supports frame rates of integers (such as 25.00, 24.00) and ntsc standard frame rate(frame rates with a denominator of 1001, such as 23.976, 29.97, 59.94) as input.\n\nfeeding other types of frame rates (such as 24.5) will be regarded as ntsc standard frame rates and will cause the output audio and video to be out of sync.\n\n\n# bitrate\n\nbitrate is defined as video file volume divided by time. the unit is generally kbps (kbit/s) or mbps (mbit/s). note that 1b (byte) = 8b (bit). so a 24 minute, 900mb video:\n\n * size: 900mb = 900mbyte = 7200mbit\n\n * time: 24min = 1440s\n\n * bit rate: 7200/1440 = 5000 kbps = 5mbps\n\nwhen the time of video files is basically the same (for example, an episode of anime lasts for around 24 minutes), the bit rate and volume are basically equivalent, and they are parameters used to describe the size of the video. files with the same length and resolution have different sizes, but the code rate is actually different.\n\nthe bit rate can also be interpreted as the total amount of data used to record video per unit time. a video with a higher bit rate means more data used to record the video, and the potential interpretation is that the video can have better quality. (note that it is only potential, we will analyze why high bit rate does not necessarily equal high image quality later)\n\n\n# rate control method\n\nbit rate control refers to the process of determining the output bit rate in video encoding. commonly used bit rate control methods are:\n\n# cbr(constant bitrate)\n\ncbr means that the file is at a constant bitrate from beginning to end. compared with vbr and abr, the compressed file size is very large, and the video quality will not be significantly improved compared with vbr and abr.\n\nfeatures:\n\n * the bit rate is stable, but the quality is unstable, and the effective bandwidth utilization rate is not high, especially when the value is set unreasonably. in complex motion scenes, if the set bit rate is not enough, the picture will be very blurred, which greatly affects the viewing experience;\n * however, the output video bit rate is basically stable, which is convenient for calculating the video volume;\n\n# vbr(variable bitrate)\n\ndynamic bit rate, that is, there is no fixed bit rate. audio and video encoding software instantly determines which bit rate to use according to the complexity of the audio and video data during the encode process. this is a way to take quality into account while taking file size into consideration.\n\ncompared to cbr applicable scenarios: vbr is suitable for scenarios where the bandwidth and encoding speed are not limited, but the quality is high. especially in complex sports scenes with large motions, it can maintain a relatively high definition and output quality is relatively stable, suitable for on-demand, recording or storage systems that are not sensitive to delay.\n\nfeatures:\n\n * the code rate is unstable, and the quality is basically stable and very high;\n * the encoding speed is generally slow, and the on-demand, download and storage systems can be used first, which is not suitable for low-latency and live broadcast systems;\n * this model does not consider the output video bandwidth at all. for the sake of quality, it takes up as much bit rate as it needs, and does not consider the encoding speed;\n\n# cqp (constant qp)\n\n> it should be counted as the way the encoder implements vbr\n\nfixed qp, the simplest code rate control method, each frame of image is encoded according to a specific qp, the amount of data encoded in each frame is unknown, neither the code rate priority model nor the quality priority model, but it is implement the simplest model;\n\napplicable scenarios: this method is generally not recommended, because this method does not consider the complexity of the encoded content, and processes each frame with the same compression ratio. the output video quality and bit rate are not fixed. personally, i think only very simple scenes, such as static scenes with little movement, can be used. when encountering complex scenes, the bit rate fluctuations are very large. or it can be used in algorithm research or verification.\n\nfeatures:\n\n * the instantaneous bit rate will fluctuate with the complexity of the scene;\n * the encoding speed is fast, the control is the simplest, and the qp value of each frame is the same;\n\n# crf(constant rate factor)\n\nit should be counted as the way the encoder implements vbr constant bit rate factor. target a certain "visual quality" as an output. it does this by reducing the bitrate of frames that are expensive but hard to see with the naked eye (high-speed motion or rich textures) and boosting the bitrate of static frames.\n\nfeatures:\n\nqp changes between frames, qp changes of intra-frame macroblocks, the output bit rate is unknown, and the visual quality of each frame output is basically constant. this method is equivalent to the method of fixed quality mode + limiting bit rate peak value.\n\n * applicable scenarios: when there are certain requirements for video quality, the crf value can be simply understood as a fixed output value for video quality expectations. it is hoped that there will be a stable value no matter in complex moving scenes or in simple static situations. subjective video quality can choose this mode, which is a video quality priority model. video quality can be simply understood as the clarity of the video, the fineness of the pixels and the fluency of the video.\n * similar to constant qp, but the pursuit of constant subjectively perceived quality, the instantaneous bit rate will also fluctuate with the complexity of the scene, and the qp values are different between video frames or between internal macroblocks;\n * for scenes with fast motion or rich details, the quantization distortion will be appropriately increased (because the human eye is not sensitive), and vice versa for static or flat areas, the quantization distortion will be reduced;\n\ntip\n\nsvfi has built-in rendering quality crf (that is, crf or cqp), target bit rate (cbr) two rate control schemes.\n\nwhen using rendering quality crf to control bit rate, svfi uses crf control for encoders other than nvenc; uses cqp bit rate control for nvenc encoder.\n\nwhen using target bit rate to control the bit rate, uniformly use constant bit rate (cbr) control.',charsets:{}},{title:"What is VFI",frontmatter:{title:"What is VFI",date:"2022-08-18T13:49:13.000Z",permalink:"/pages/88ed7f/"},regularPath:"/10.Getting%20Started/10.Getting%20Started/20.What%20is%20VFI.html",relativePath:"10.Getting Started/10.Getting Started/20.What is VFI.md",key:"v-11cfbe2c",path:"/pages/88ed7f/",headers:[{level:2,title:"Video Frame Interpolation = VFI",slug:"video-frame-interpolation-vfi",normalizedTitle:"video frame interpolation = vfi",charIndex:2}],headersStr:"Video Frame Interpolation = VFI",content:'# Video Frame Interpolation = VFI\n\nFrame interpolation refers to improving the video frame rate through certain specific algorithms, that is, inserting new frames that are not included in the original video between frames to make the look and feel smoother.\n\nCommon algorithms include "MEMC" based on hardware, the software-based optical flow method.\n\nSVFI uses the deep learning-based frame interpolation algorithm RIFE and its derivatives to complement the frame, and its effect is better than other algorithms.\n\nDescription of SVFI built-in VFI\n\nEffect reference:\n\nGenshin Impact\n\nDrama CM short film, 8K 60fps\n\nSVFI Vision\n\nUmaron\n\nSeason 2 NCOP 8K 60fps\n\nSVFI Vision\n\nRe Zero-Starting Life in Another World\n\nSeason 2 NCED Believe in you\n\nSVFI Vision\n\nconfig:\n    target: _blank\n    imgHeight: auto\n    objectFit: contain\n    lineClamp: 1\n\ndata:\n- img: /img/bilibili/yuan.jpg\n  name: Genshin Impact\n  desc: Drama CM short film, 8K 60fps\n  link: https://www.bilibili.com/video/BV1FS4y1C7RD\n  author: SVFI Vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/umaron.jpg\n  name: Umaron\n  desc: Season 2 NCOP 8K 60fps\n  link: https://www.bilibili.com/video/BV1QY411b7e4\n  author: SVFI Vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/emilia.jpg\n  name: Re Zero-Starting Life in Another World\n  desc: Season 2 NCED Believe in you\n  link: https://www.bilibili.com/video/BV1kF411p7FB\n  author: SVFI Vision\n  avatar: /img/svfi.ico\n',normalizedContent:'# video frame interpolation = vfi\n\nframe interpolation refers to improving the video frame rate through certain specific algorithms, that is, inserting new frames that are not included in the original video between frames to make the look and feel smoother.\n\ncommon algorithms include "memc" based on hardware, the software-based optical flow method.\n\nsvfi uses the deep learning-based frame interpolation algorithm rife and its derivatives to complement the frame, and its effect is better than other algorithms.\n\ndescription of svfi built-in vfi\n\neffect reference:\n\ngenshin impact\n\ndrama cm short film, 8k 60fps\n\nsvfi vision\n\numaron\n\nseason 2 ncop 8k 60fps\n\nsvfi vision\n\nre zero-starting life in another world\n\nseason 2 nced believe in you\n\nsvfi vision\n\nconfig:\n    target: _blank\n    imgheight: auto\n    objectfit: contain\n    lineclamp: 1\n\ndata:\n- img: /img/bilibili/yuan.jpg\n  name: genshin impact\n  desc: drama cm short film, 8k 60fps\n  link: https://www.bilibili.com/video/bv1fs4y1c7rd\n  author: svfi vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/umaron.jpg\n  name: umaron\n  desc: season 2 ncop 8k 60fps\n  link: https://www.bilibili.com/video/bv1qy411b7e4\n  author: svfi vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/emilia.jpg\n  name: re zero-starting life in another world\n  desc: season 2 nced believe in you\n  link: https://www.bilibili.com/video/bv1kf411p7fb\n  author: svfi vision\n  avatar: /img/svfi.ico\n',charsets:{}},{title:"What is SR",frontmatter:{title:"What is SR",date:"2022-08-18T13:49:13.000Z",permalink:"/pages/681961/"},regularPath:"/10.Getting%20Started/10.Getting%20Started/30.What%20is%20SR.html",relativePath:"10.Getting Started/10.Getting Started/30.What is SR.md",key:"v-ebc6ecc8",path:"/pages/681961/",headers:[{level:2,title:"SR = Super Resolution = Upscale",slug:"sr-super-resolution-upscale",normalizedTitle:"sr = super resolution = upscale",charIndex:2}],headersStr:"SR = Super Resolution = Upscale",content:'# SR = Super Resolution = Upscale\n\nImprove the video resolution through some specific algorithms, that is, increase the resolution of each frame of the video to make the look and feel clearer.\n\nCommon algorithms include BICUBIC, LANCZOS, etc., which are not implemented by AI, commonly known as "Resize".\n\nSVFI uses deep network-based artificial intelligence algorithm RealESR, RealCUGAN for super-resolution, and its super-resolution effect on animation is better than other algorithms.\n\nTip\n\nFor the processing task of the same video, SVFI will perform super resolution first, and then perform frame supplementation\n\nDescription of SVFI built-in super resolution algorithm\n\nEffect reference:\n\nGenshin Impact\n\nDrama CM short film, 8K 60fps\n\nSVFI Vision\n\nUmaron\n\nSeason 2 NCOP 8K 60fps\n\nSVFI Vision\n\nRe Zero-Starting Life in Another World\n\nSeason 2 NCED Believe in you\n\nSVFI Vision\n\nconfig:\n    target: _blank\n    imgHeight: auto\n    objectFit: contain\n    lineClamp: 1\n\ndata:\n- img: /img/bilibili/yuan.jpg\n  name: Genshin Impact\n  desc: Drama CM short film, 8K 60fps\n  link: https://www.bilibili.com/video/BV1FS4y1C7RD\n  author: SVFI Vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/umaron.jpg\n  name: Umaron\n  desc: Season 2 NCOP 8K 60fps\n  link: https://www.bilibili.com/video/BV1QY411b7e4\n  author: SVFI Vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/emilia.jpg\n  name: Re Zero-Starting Life in Another World\n  desc: Season 2 NCED Believe in you\n  link: https://www.bilibili.com/video/BV1kF411p7FB\n  author: SVFI Vision\n  avatar: /img/svfi.ico\n',normalizedContent:'# sr = super resolution = upscale\n\nimprove the video resolution through some specific algorithms, that is, increase the resolution of each frame of the video to make the look and feel clearer.\n\ncommon algorithms include bicubic, lanczos, etc., which are not implemented by ai, commonly known as "resize".\n\nsvfi uses deep network-based artificial intelligence algorithm realesr, realcugan for super-resolution, and its super-resolution effect on animation is better than other algorithms.\n\ntip\n\nfor the processing task of the same video, svfi will perform super resolution first, and then perform frame supplementation\n\ndescription of svfi built-in super resolution algorithm\n\neffect reference:\n\ngenshin impact\n\ndrama cm short film, 8k 60fps\n\nsvfi vision\n\numaron\n\nseason 2 ncop 8k 60fps\n\nsvfi vision\n\nre zero-starting life in another world\n\nseason 2 nced believe in you\n\nsvfi vision\n\nconfig:\n    target: _blank\n    imgheight: auto\n    objectfit: contain\n    lineclamp: 1\n\ndata:\n- img: /img/bilibili/yuan.jpg\n  name: genshin impact\n  desc: drama cm short film, 8k 60fps\n  link: https://www.bilibili.com/video/bv1fs4y1c7rd\n  author: svfi vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/umaron.jpg\n  name: umaron\n  desc: season 2 ncop 8k 60fps\n  link: https://www.bilibili.com/video/bv1qy411b7e4\n  author: svfi vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/emilia.jpg\n  name: re zero-starting life in another world\n  desc: season 2 nced believe in you\n  link: https://www.bilibili.com/video/bv1kf411p7fb\n  author: svfi vision\n  avatar: /img/svfi.ico\n',charsets:{}},{title:"Get Started with SVFI",frontmatter:{title:"Get Started with SVFI",date:"2023-05-25T21:49:13.000Z",permalink:"/pages/0e988c/"},regularPath:"/10.Getting%20Started/10.Getting%20Started/40.Quick%20tour%20of%20SVFI.html",relativePath:"10.Getting Started/10.Getting Started/40.Quick tour of SVFI.md",key:"v-4d77aef9",path:"/pages/0e988c/",headers:[{level:2,title:"SVFI Getting Started Guide V2.0",slug:"svfi-getting-started-guide-v2-0",normalizedTitle:"svfi getting started guide v2.0",charIndex:2}],headersStr:"SVFI Getting Started Guide V2.0",content:'# SVFI Getting Started Guide V2.0\n\nTip\n\nThe following is a basic tutorial for interpolating a video using SVFI.\n\nWelcome to Squirrel Video Frame Interpolation software. This software is dedicated to upgrading the stuttering low frame rate video to smooth high frame rate video.\n\nRegardless of whether you have had any similar video-frame-interpolation experience before, please believe that if you follow the process below, you will have a pleasant experience.\n\n 1. Make sure your graphics card meets the minimum requirements of this software: GTX 750Ti and above, graphics card above Maxwell architecture. If you do not meet this requirement, please refund and stop the loss in time, and use other frame-filling software.\n 2. Find SVFI on the content library page of Steam\n\n\n\n 3. Start SVFI\n\n\n\n 4. Wait for the software to start, you will see the following blank operation page. This is the main interface of SVFI. Please note that this tutorial is for the full series of SVFI 3.x versions, and the software version number you see may be different from the screenshot.\n    * To add frames to a video, you first need to select the option ❶ "Input Video File" below, and select the video to be added in the pop-up file selection window.\n    * You can also "drag" the video directly into window ❷.\n\n\n\n 5. After importing a video, we need to make some basic settings.\n    \n    * First thing: From option ❶ "Set Output Folder", select the location of the completed frame video, which is Output Folder. If not specified, the output video with completed frames will be in the folder where the input video is located.\n    \n    * Set the option ❷ "Output file format" to determine the video format of the completed frame as .mp4, .mkv or .mov. The mp4 file is more versatile, mkv supports more types of audio, and mov uses fewer scenes. For Apple Eco clips. Please try to be consistent with the input video, if the format of the input video is not among the above three, use mkv.\n    \n    * You will find that the frame rate of the input video has been displayed on the left side of option ❸. You can directly select the frame rate multiplier of the supplementary frame in option ❸ "frame interpolation rate", or manually fill in the frame rate in ❹ "Output frame rate". You can fill in any number, 60 or 120, SVFI will handle everything for you.\n\n\n\n 6. Congratulations, you have completed all the necessary settings. Now click ❺Interpolate to make a cup of coffee and wait for the output result in the output folder. -Before clicking ❺Interpolate, in order to prevent errors, please close any other software that may occupy video memory.\n    * If the software still encounters an error, or you do not see the output video you are thinking about in the output folder, please move to FAQ for possible solutions;\n    * If the solution to the corresponding problem is not listed on this page, please post a message in the Steam Discussion Forum to contact the developer for help, and we will reply within 48h~96h.',normalizedContent:'# svfi getting started guide v2.0\n\ntip\n\nthe following is a basic tutorial for interpolating a video using svfi.\n\nwelcome to squirrel video frame interpolation software. this software is dedicated to upgrading the stuttering low frame rate video to smooth high frame rate video.\n\nregardless of whether you have had any similar video-frame-interpolation experience before, please believe that if you follow the process below, you will have a pleasant experience.\n\n 1. make sure your graphics card meets the minimum requirements of this software: gtx 750ti and above, graphics card above maxwell architecture. if you do not meet this requirement, please refund and stop the loss in time, and use other frame-filling software.\n 2. find svfi on the content library page of steam\n\n\n\n 3. start svfi\n\n\n\n 4. wait for the software to start, you will see the following blank operation page. this is the main interface of svfi. please note that this tutorial is for the full series of svfi 3.x versions, and the software version number you see may be different from the screenshot.\n    * to add frames to a video, you first need to select the option ❶ "input video file" below, and select the video to be added in the pop-up file selection window.\n    * you can also "drag" the video directly into window ❷.\n\n\n\n 5. after importing a video, we need to make some basic settings.\n    \n    * first thing: from option ❶ "set output folder", select the location of the completed frame video, which is output folder. if not specified, the output video with completed frames will be in the folder where the input video is located.\n    \n    * set the option ❷ "output file format" to determine the video format of the completed frame as .mp4, .mkv or .mov. the mp4 file is more versatile, mkv supports more types of audio, and mov uses fewer scenes. for apple eco clips. please try to be consistent with the input video, if the format of the input video is not among the above three, use mkv.\n    \n    * you will find that the frame rate of the input video has been displayed on the left side of option ❸. you can directly select the frame rate multiplier of the supplementary frame in option ❸ "frame interpolation rate", or manually fill in the frame rate in ❹ "output frame rate". you can fill in any number, 60 or 120, svfi will handle everything for you.\n\n\n\n 6. congratulations, you have completed all the necessary settings. now click ❺interpolate to make a cup of coffee and wait for the output result in the output folder. -before clicking ❺interpolate, in order to prevent errors, please close any other software that may occupy video memory.\n    * if the software still encounters an error, or you do not see the output video you are thinking about in the output folder, please move to faq for possible solutions;\n    * if the solution to the corresponding problem is not listed on this page, please post a message in the steam discussion forum to contact the developer for help, and we will reply within 48h~96h.',charsets:{}},{title:"Image representation and quality",frontmatter:{title:"Image representation and quality",date:"2022-08-19T14:25:46.000Z",permalink:"/pages/8cc1b5/"},regularPath:"/10.Getting%20Started/20.Related%20Articles/01.Image%20representation%20and%20quality.html",relativePath:"10.Getting Started/20.Related Articles/01.Image representation and quality.md",key:"v-0c78072d",path:"/pages/8cc1b5/",headers:[{level:2,title:"Image representation method",slug:"image-representation-method",normalizedTitle:"image representation method",charIndex:89},{level:3,title:"RGB model",slug:"rgb-model",normalizedTitle:"rgb model",charIndex:121},{level:3,title:"YUV model",slug:"yuv-model",normalizedTitle:"yuv model",charIndex:1260},{level:3,title:"color depth",slug:"color-depth",normalizedTitle:"color depth",charIndex:3406},{level:2,title:"Brief description of clarity and quality",slug:"brief-description-of-clarity-and-quality",normalizedTitle:"brief description of clarity and quality",charIndex:5884}],headersStr:"Image representation method RGB model YUV model color depth Brief description of clarity and quality",content:'Tip\n\nThe following content is taken from FXXS-Encode-Guide, thanks for their efforts\n\n\n# Image representation method\n\n\n# RGB model\n\nThe three primary colors of light are red (Red), green (Green), blue (Blue). Modern display technology achieves any color of visible light by combining the three primary colors of different intensities. In image storage, the method of recording images by recording the red, green and blue intensity of each pixel is called the RGB model (RGB Model)\n\nAmong the common image formats, PNG and BMP are based on the RGB model.\n\nFor example, the original image:\n\n\n\nOnly display the intensity of the R G B channel respectively, the effect is as follows:\n\n\n\n\n\n\n\nUnder the three channels, the amount of information and the degree of detail are not necessarily evenly distributed. For example, you can pay attention to the blush on Nan Xiaoniao\'s face, the degree of distinction is different on the three planes-the red plane is almost impossible to distinguish, and the difference is mainly caused by the green and blue planes. On the peripheral white cheeks, the three colors are almost saturated; but in the flush part, only the red is saturated, and the green and blue are not saturated. This is what causes the red to stand out.\n\n\n# YUV model\n\nIn addition to the RGB model, there is also a widely used model called the YUV model, also known as the Luma-Chroma model. It converts the three channels of RGB into a channel representing brightness (Y, also known as Luma) and two channels representing chroma (UV, and becomes Chroma) through mathematical conversion.\n\nThe YUV model does something similar. Another representation is obtained through a reasonable conversion of the RGB data. Under the YUV model, there are different implementations. Take a more commonly used YCbCr model: it converts RGB into a brightness (Y), and blue chroma (Cb) and red chroma (Cr). You don\'t need to understand the complex formula behind the conversion, just look at the effect:\n\nOnly the luma channel:\n\n\n\nOnly shades of blue:\n\n\n\nOnly shades of red:\n\n\n\nIn image and video processing and storage, YUV format is generally more popular for the following reasons:\n\n 1. The sensitivity of the human eye to brightness is much higher than that of chroma, so the effective information seen by the human eye mainly comes from brightness. The YUV model can assign most of the effective information to the Y channel. The UV channel has relatively less information recorded. Compared with the more even distribution of the RGB model, the YUV model concentrates most of the effective information in the Y channel, which not only reduces the amount of redundant information, but also facilitates compression\n\n 2. Maintain backward compatibility with black and white display devices\n\n 3. In image editing, it is more convenient to adjust the brightness and color saturation under the YUV model.\n\nAlmost all video formats, as well as the widely used JPEG image format, are based on the YCbCr model. When playing, the player needs to convert the YCbCr information into RGB through calculation. This step is called Rendering\n\nRecords for each channel, usually represented by integers. For example, RGB24 has 8 bits for each RGB, and uses 0~255 (8bit binary number range) to represent the strength of a certain color. The YUV model is no exception. It also uses integers to represent the height of each channel.\n\n\n# color depth\n\nColor depth (bit-depth), which is what we usually call 8bit and 10bit, refers to the precision of each channel. 8bit means that each channel is represented by an 8bit integer (0~255), and 10bit is displayed by a 10bit integer (0~1023). 16bit is 0~65535\n\n(Note that the above statement is imprecise. When video is encoded, not all ranges of 0~255 can be used, but may be reserved, and only a part of it is used, such as 16~235 .This we will not expand in detail)\n\nYour display is 8bit, which means it can display all the intensities of each RGB channel from 0 to 255. However, the color depth of the video is the color depth of YUV. When playing, YUV needs to be converted to RGB through calculation. Therefore, the high precision of 10bit is indirect, which increases the precision in the calculation process to make the final color more delicate.\n\nHow to understand 8bit display, it is necessary to play 10bit:\n\nThe radius of a circle is 12.33m, find its area, keep two decimal places.\n\nThe accuracy of the radius is given two decimal places, and the result also requires two decimal places, so how high the accuracy of pi needs to be? Only two decimal places?\n\nTaking pi=3.14, the calculated area is 477.37 square meters\n\nTaking pi=3.1416, the calculated area is 477.61 square meters\n\nThe accuracy of taking pi is high enough, and the calculated area is 477.61 square meters. So taking pi=3.1416 is enough, but 3.14 is not enough.\n\nIn other words, even if the precision requirement of the final output is low, it does not mean that the numbers involved in the calculation and the calculation process can maintain low precision. Under the premise that the final output is 8bit RGB, the reason why 10bit YUV still has an accuracy advantage over 8bit YUV is here. In fact, after 8bit YUV conversion, the coverage accuracy is about 26% of 8bit RGB, and after 10bit conversion, the accuracy can cover about 97% - do you want your 8bit display to play 97% of the fineness? Look at 10bit.\n\nThe 8bit precision is insufficient, mainly manifested in areas with low brightness, which is easy to form color bands.\n\n\n\nNote that the circle on the right side of the picture has the same effect as the wave. This is the performance of insufficient color accuracy.\n\nThe advantage of 10bit is not only the improvement of display accuracy, but also has advantages over 8bit in terms of improving video compression rate and reducing distortion. This aspect will not be expanded.\n\n\n# Brief description of clarity and quality\n\n> A frequently seen saying: "The resolution of this video is 1080p". In fact, after reading the above, you should know that 1080p is only the resolution of the video, and it cannot directly represent the definition-for example, I can upscale a 480p dvd video to 1080p, so what? Has its clarity improved?\n\nThe quality of the video is determined by the following points\n\n 1. The quality of the source.\n\nAs the saying goes, the upper beam is not straight and the lower beam is crooked. If the quality of the source itself is poor, don\'t expect to get any better than that. Therefore, suppressors tend to choose a better source for suppression—for example, BDRip is generally better than TVRip, even if it is 720p. Blu-ray is also divided into sales regions. For example, for a Japanese cartoon, the Japanese version sold in Japan generally has better picture quality than the US version, Taiwan version, and Hong Kong version. By choosing a better source, you can give priority to the picture quality.\n\n 2. Playback conditions.\n\nWhether the audience has used enough hardware and software to support high-quality playback. That\'s why we popularized good players while releasing Rip; sometimes a good player is better than how much effort was put into production.\n\n 3. Bit rate input VS encoding complexity.\n\nThe time and space complexity of the video, and called the encoding complexity. Videos with high encoding complexity often have many details and high dynamics (such as "Magical Girl Madoka Theatrical Version: Rebellious Story"). Such videos naturally require a higher bit rate to maintain an excellent viewing effect.\n\nOn the contrary, some videos have low coding complexity (for example, "Would you like some rabbits today", with less dynamics and soft line details), this kind of video saves bit rate.\n\n 4. Efficiency and rationality of code rate allocation.\n\nHow effective the same bit rate can be is called efficiency. For example, H264 is more efficient than the previous RealVideo; 10bit is more efficient than 8bit; the encoder is advanced, the parameters are set reasonably, and all kinds of high-end parameters of the encoder are fully enabled (usually at the cost of encoding time), and the bit rate efficiency is high.\n\nThe rationality is whether the code rate is reasonable or not in terms of space-time distribution. If the distribution is reasonable, the viewing effect for the audience will be more unified and coordinated. The efficiency and rationality of the bit rate distribution is a requirement for producers, who are required to have a good understanding of film source analysis and parameter settings.\n\nIf the bit rate allocation and rationality are done well, it is often possible to produce conscientious works with low bit rate and high image quality.\n\n 5. Preprocessing before encoding. There are three types of preprocessing:\n\n * Objective fixes. Emphasis on repairing the inherent defects of the film source, such as jaggies, color bands, halos, etc.\n\n * Subjective adjustment, emphasizing the adjustment of the film source to be more suitable for human eyes, such as moderate sharpening and color adjustment (sometimes you can use scientific methods to determine that there is a problem with the color of the film source, and then fix it accordingly).\n\n * Remove invalid high-frequency information, such as noise reduction, to avoid wasting bit rate on invalid noise points\n   \n   If the preprocessing is done well, it can often surpass the source in terms of image quality, or save bit rate overhead without sacrificing clarity.\n   \n   But preprocessing is a double-edged sword. While optimizing, side effects may be introduced. Operations such as noise reduction, anti-aliasing, and halo removal will inevitably lose some effective details (more or less, depending on the level of the producer); subjective adjustments are likely to introduce side effects (such as excessive sharpening will cause aliasing and halo Wheel), or become the author\'s self-satisfaction, forming a deception to the audience.\n\nTo sum up, an excellent picture quality is jointly determined by the film source, the producer, and the viewer; the bit rate is only a part of the factor, not the decisive effect.',normalizedContent:'tip\n\nthe following content is taken from fxxs-encode-guide, thanks for their efforts\n\n\n# image representation method\n\n\n# rgb model\n\nthe three primary colors of light are red (red), green (green), blue (blue). modern display technology achieves any color of visible light by combining the three primary colors of different intensities. in image storage, the method of recording images by recording the red, green and blue intensity of each pixel is called the rgb model (rgb model)\n\namong the common image formats, png and bmp are based on the rgb model.\n\nfor example, the original image:\n\n\n\nonly display the intensity of the r g b channel respectively, the effect is as follows:\n\n\n\n\n\n\n\nunder the three channels, the amount of information and the degree of detail are not necessarily evenly distributed. for example, you can pay attention to the blush on nan xiaoniao\'s face, the degree of distinction is different on the three planes-the red plane is almost impossible to distinguish, and the difference is mainly caused by the green and blue planes. on the peripheral white cheeks, the three colors are almost saturated; but in the flush part, only the red is saturated, and the green and blue are not saturated. this is what causes the red to stand out.\n\n\n# yuv model\n\nin addition to the rgb model, there is also a widely used model called the yuv model, also known as the luma-chroma model. it converts the three channels of rgb into a channel representing brightness (y, also known as luma) and two channels representing chroma (uv, and becomes chroma) through mathematical conversion.\n\nthe yuv model does something similar. another representation is obtained through a reasonable conversion of the rgb data. under the yuv model, there are different implementations. take a more commonly used ycbcr model: it converts rgb into a brightness (y), and blue chroma (cb) and red chroma (cr). you don\'t need to understand the complex formula behind the conversion, just look at the effect:\n\nonly the luma channel:\n\n\n\nonly shades of blue:\n\n\n\nonly shades of red:\n\n\n\nin image and video processing and storage, yuv format is generally more popular for the following reasons:\n\n 1. the sensitivity of the human eye to brightness is much higher than that of chroma, so the effective information seen by the human eye mainly comes from brightness. the yuv model can assign most of the effective information to the y channel. the uv channel has relatively less information recorded. compared with the more even distribution of the rgb model, the yuv model concentrates most of the effective information in the y channel, which not only reduces the amount of redundant information, but also facilitates compression\n\n 2. maintain backward compatibility with black and white display devices\n\n 3. in image editing, it is more convenient to adjust the brightness and color saturation under the yuv model.\n\nalmost all video formats, as well as the widely used jpeg image format, are based on the ycbcr model. when playing, the player needs to convert the ycbcr information into rgb through calculation. this step is called rendering\n\nrecords for each channel, usually represented by integers. for example, rgb24 has 8 bits for each rgb, and uses 0~255 (8bit binary number range) to represent the strength of a certain color. the yuv model is no exception. it also uses integers to represent the height of each channel.\n\n\n# color depth\n\ncolor depth (bit-depth), which is what we usually call 8bit and 10bit, refers to the precision of each channel. 8bit means that each channel is represented by an 8bit integer (0~255), and 10bit is displayed by a 10bit integer (0~1023). 16bit is 0~65535\n\n(note that the above statement is imprecise. when video is encoded, not all ranges of 0~255 can be used, but may be reserved, and only a part of it is used, such as 16~235 .this we will not expand in detail)\n\nyour display is 8bit, which means it can display all the intensities of each rgb channel from 0 to 255. however, the color depth of the video is the color depth of yuv. when playing, yuv needs to be converted to rgb through calculation. therefore, the high precision of 10bit is indirect, which increases the precision in the calculation process to make the final color more delicate.\n\nhow to understand 8bit display, it is necessary to play 10bit:\n\nthe radius of a circle is 12.33m, find its area, keep two decimal places.\n\nthe accuracy of the radius is given two decimal places, and the result also requires two decimal places, so how high the accuracy of pi needs to be? only two decimal places?\n\ntaking pi=3.14, the calculated area is 477.37 square meters\n\ntaking pi=3.1416, the calculated area is 477.61 square meters\n\nthe accuracy of taking pi is high enough, and the calculated area is 477.61 square meters. so taking pi=3.1416 is enough, but 3.14 is not enough.\n\nin other words, even if the precision requirement of the final output is low, it does not mean that the numbers involved in the calculation and the calculation process can maintain low precision. under the premise that the final output is 8bit rgb, the reason why 10bit yuv still has an accuracy advantage over 8bit yuv is here. in fact, after 8bit yuv conversion, the coverage accuracy is about 26% of 8bit rgb, and after 10bit conversion, the accuracy can cover about 97% - do you want your 8bit display to play 97% of the fineness? look at 10bit.\n\nthe 8bit precision is insufficient, mainly manifested in areas with low brightness, which is easy to form color bands.\n\n\n\nnote that the circle on the right side of the picture has the same effect as the wave. this is the performance of insufficient color accuracy.\n\nthe advantage of 10bit is not only the improvement of display accuracy, but also has advantages over 8bit in terms of improving video compression rate and reducing distortion. this aspect will not be expanded.\n\n\n# brief description of clarity and quality\n\n> a frequently seen saying: "the resolution of this video is 1080p". in fact, after reading the above, you should know that 1080p is only the resolution of the video, and it cannot directly represent the definition-for example, i can upscale a 480p dvd video to 1080p, so what? has its clarity improved?\n\nthe quality of the video is determined by the following points\n\n 1. the quality of the source.\n\nas the saying goes, the upper beam is not straight and the lower beam is crooked. if the quality of the source itself is poor, don\'t expect to get any better than that. therefore, suppressors tend to choose a better source for suppression—for example, bdrip is generally better than tvrip, even if it is 720p. blu-ray is also divided into sales regions. for example, for a japanese cartoon, the japanese version sold in japan generally has better picture quality than the us version, taiwan version, and hong kong version. by choosing a better source, you can give priority to the picture quality.\n\n 2. playback conditions.\n\nwhether the audience has used enough hardware and software to support high-quality playback. that\'s why we popularized good players while releasing rip; sometimes a good player is better than how much effort was put into production.\n\n 3. bit rate input vs encoding complexity.\n\nthe time and space complexity of the video, and called the encoding complexity. videos with high encoding complexity often have many details and high dynamics (such as "magical girl madoka theatrical version: rebellious story"). such videos naturally require a higher bit rate to maintain an excellent viewing effect.\n\non the contrary, some videos have low coding complexity (for example, "would you like some rabbits today", with less dynamics and soft line details), this kind of video saves bit rate.\n\n 4. efficiency and rationality of code rate allocation.\n\nhow effective the same bit rate can be is called efficiency. for example, h264 is more efficient than the previous realvideo; 10bit is more efficient than 8bit; the encoder is advanced, the parameters are set reasonably, and all kinds of high-end parameters of the encoder are fully enabled (usually at the cost of encoding time), and the bit rate efficiency is high.\n\nthe rationality is whether the code rate is reasonable or not in terms of space-time distribution. if the distribution is reasonable, the viewing effect for the audience will be more unified and coordinated. the efficiency and rationality of the bit rate distribution is a requirement for producers, who are required to have a good understanding of film source analysis and parameter settings.\n\nif the bit rate allocation and rationality are done well, it is often possible to produce conscientious works with low bit rate and high image quality.\n\n 5. preprocessing before encoding. there are three types of preprocessing:\n\n * objective fixes. emphasis on repairing the inherent defects of the film source, such as jaggies, color bands, halos, etc.\n\n * subjective adjustment, emphasizing the adjustment of the film source to be more suitable for human eyes, such as moderate sharpening and color adjustment (sometimes you can use scientific methods to determine that there is a problem with the color of the film source, and then fix it accordingly).\n\n * remove invalid high-frequency information, such as noise reduction, to avoid wasting bit rate on invalid noise points\n   \n   if the preprocessing is done well, it can often surpass the source in terms of image quality, or save bit rate overhead without sacrificing clarity.\n   \n   but preprocessing is a double-edged sword. while optimizing, side effects may be introduced. operations such as noise reduction, anti-aliasing, and halo removal will inevitably lose some effective details (more or less, depending on the level of the producer); subjective adjustments are likely to introduce side effects (such as excessive sharpening will cause aliasing and halo wheel), or become the author\'s self-satisfaction, forming a deception to the audience.\n\nto sum up, an excellent picture quality is jointly determined by the film source, the producer, and the viewer; the bit rate is only a part of the factor, not the decisive effect.',charsets:{}},{title:"Encoding, Encoder and Encapsulation Format",frontmatter:{title:"Encoding, Encoder and Encapsulation Format",date:"2022-08-18T13:49:13.000Z",permalink:"/pages/76d9d4/"},regularPath:"/10.Getting%20Started/20.Related%20Articles/03.Encodings,%20Encoders,%20and%20Encapsulation%20Formats.html",relativePath:"10.Getting Started/20.Related Articles/03.Encodings, Encoders, and Encapsulation Formats.md",key:"v-51ae0d5a",path:"/pages/76d9d4/",headers:[{level:2,title:"container",slug:"container",normalizedTitle:"container",charIndex:89},{level:2,title:"Some caveats",slug:"some-caveats",normalizedTitle:"some caveats",charIndex:472},{level:3,title:"flac - Free Lossless Audio Codec - Free lossless audio compression coding",slug:"flac-free-lossless-audio-codec-free-lossless-audio-compression-coding",normalizedTitle:"flac - free lossless audio codec - free lossless audio compression coding",charIndex:489},{level:3,title:"m4a",slug:"m4a",normalizedTitle:"m4a",charIndex:809},{level:3,title:"mp3",slug:"mp3",normalizedTitle:"mp3",charIndex:989},{level:3,title:"wav related",slug:"wav-related",normalizedTitle:"wav related",charIndex:1175},{level:3,title:"H.265 related",slug:"h-265-related",normalizedTitle:"h.265 related",charIndex:2179},{level:3,title:"qaac",slug:"qaac",normalizedTitle:"qaac",charIndex:2947},{level:2,title:"Package format",slug:"package-format",normalizedTitle:"package format",charIndex:3334},{level:2,title:"Encoding format",slug:"encoding-format",normalizedTitle:"encoding format",charIndex:4382},{level:2,title:"Transcoding",slug:"transcoding",normalizedTitle:"transcoding",charIndex:7827},{level:2,title:"Repackage (Remux)",slug:"repackage-remux",normalizedTitle:"repackage (remux)",charIndex:8254},{level:2,title:"Detach (Demux)",slug:"detach-demux",normalizedTitle:"detach (demux)",charIndex:9415},{level:2,title:"Lossless compression and lossy compression",slug:"lossless-compression-and-lossy-compression",normalizedTitle:"lossless compression and lossy compression",charIndex:9503},{level:2,title:"audio",slug:"audio",normalizedTitle:"audio",charIndex:371},{level:3,title:"Lossy Audio",slug:"lossy-audio",normalizedTitle:"lossy audio",charIndex:11145},{level:4,title:"DTS",slug:"dts",normalizedTitle:"dts",charIndex:10439},{level:4,title:"AC3",slug:"ac3",normalizedTitle:"ac3",charIndex:284},{level:4,title:"Dolby Digital Plus/DDP/EAC3",slug:"dolby-digital-plus-ddp-eac3",normalizedTitle:"dolby digital plus/ddp/eac3",charIndex:13064},{level:4,title:"AAC",slug:"aac",normalizedTitle:"aac",charIndex:288},{level:4,title:"Some lossy audio tests",slug:"some-lossy-audio-tests",normalizedTitle:"some lossy audio tests",charIndex:16296}],headersStr:"container Some caveats flac - Free Lossless Audio Codec - Free lossless audio compression coding m4a mp3 wav related H.265 related qaac Package format Encoding format Transcoding Repackage (Remux) Detach (Demux) Lossless compression and lossy compression audio Lossy Audio DTS AC3 Dolby Digital Plus/DDP/EAC3 AAC Some lossy audio tests",content:'Tip\n\nThe following content is taken from FXXS-Encode-Guide, thanks for their efforts\n\n\n# container\n\nContainer (Container) is also known as encapsulation format, or format (Format), multimedia container (Multimedia Container), common encapsulation format (MP4/MKV...), and H.264/H.265/AC3/AAC belongs to the encoding method \\ encoding Format (Codec), the video stream and audio stream encapsulated in MP4 and MKV (encapsulation format) can use the same encoding format\n\n\n# Some caveats\n\n\n# flac - Free Lossless Audio Codec - Free lossless audio compression coding\n\nflac is both an encoding format and an encapsulation format (flac uses the same name as the container name (that is, it is also the name of the encapsulation format), <https://developer.mozilla.org/zh-TW/docs/Web/Media/Formats/ Containers>)\n\n\n# m4a\n\nm4a (encapsulation format, audio-only MPEG-4/mp4 files will use the .m4a extension) can be encoded using ALAC (Apple Lossless Audio Codec) or AAC (Advanced Audio Coding).\n\n\n# mp3\n\nMP3 (formally MPEG-1 Audio Layer III or MPEG-2 Audio Layer III) is composed of MPEG-1 (encapsulation container) and an audio track encoded with MPEG-1 Audio Layer III encoding.\n\n\n# wav related\n\nWAV belongs to the package format, and the packaged PCM belongs to the original audio data, which is completely uncompressed encoding method\n\nPCM - Pulse Code Modulation (Pulse Code Modulation) is a method of digitizing analog signals. It is the most commonly used and simplest waveform encoding method.\n\nLPCM is a specific type of PCM, and although ‎‎PCM‎‎ is a more general term, it is often used to describe data encoded as LPCM. ‎\n\nBWF (Broadcast Wave Format)\n\nThe suffix is also wav, as the successor of WAV, which is a standard audio format created by the European Broadcasting Union (European Broadcasting Union). BWF has more data regarding the file. But there is no difference in track quality\n\nRF64 (the suffix is usually written w64 to distinguish) (multichannel audio file)\n\nAn extension of the WAV file format, which can be larger than 4GB in file size. It has been specified by the European Broadcasting Union. It has been accepted as the ITU recommendation ITU-R BS.2088.\n\n\n# H.265 related\n\nH.265/HEVC is the encoding\n\nH.265 is the standard, HEVC is the winner, also called MPEG-H Part 2\n\nSome people think that H.265 is used to refer to non-x265 HEVC commercial encoders in communication (personally thinks it is non-standard and has no definition, it is a colloquial content, for reference only)\n\nAnd some sites require that the Web-dl title must be written in H.265, and HEVC cannot be used, while the original disk/Remux is only allowed to write HEVC. (I personally think it belongs to regional rules, and it is not recommended to be understood as a standard)\n\nx265 is an open source and free encoder, and there are also some commercial encoders, such as Strene,[NVENC](https://www.nvidia.com/en- us/geforce/guides/broadcasting-guide/)\n\n\n# qaac\n\nqaac is a command-line AAC/ALAC encoder frontend based on the Apple encoder. Since 1.00, qaac uses CoreAudioToolbox.dll directly. Therefore, it is no longer necessary to install QuickTime. However, Apple application support is required. Support AAC-LC, AAC-HE, ALAC encoding. Generally, what we make is LC\n\n(qaac is open source, seems to be copylefted and has no restrictions)\n\n\n# Package format\n\nThere are many common MP4, MKV, RMVB, TS, FLV, AVI, etc., and the encodings supported by various packaging formats are different.\n\nMKV vs MP4, the main differences are:\n\n 1. MKV supports encapsulating FLAC as audio, but MP4 does not. But MP4 can also encapsulate lossless audio tracks (such as ALAC, although it is generally believed that ALAC is not as efficient as FLAC)\n 2. MKV supports encapsulating subtitles in ASS/SSA format, but MP4 does not. Generally, the subtitles produced by the subtitle group are in ASS format, so inner subtitles are more common in MKV format\n 3. As an industry standard, MP4 is generally more compatible with video editing software and playback devices than MKV (of course, Premiere can install plug-ins to support mkv, and iOS and Android can also be supported by third-party players), which is also the general compression group for mobile Device-optimized video is basically the reason for choosing the MP4 package.\n\nThere are also some obsolete packaging formats, such as RM, AVI and so on.\n\n\n# Encoding format\n\nUsually mediainfo needs to be packaged in a format to display relatively complete information, such as .pcm .aac .hevc cannot display information such as duration. .hevc cannot use the progress bar in the player.\n\nLPCM (Linear pulse-code modulation) is generally also called PCM, but a specific type of PCM is different from PCM (Pulse Code Modulation), and its quantization level is linear and uniform.\n\nH.26X series: led by ITU (International Telecommunication Union), including H.261, H.262, H.263, H.264, H.265.\n\n * H.264: The tenth part of H.264/MPEG-4, or AVC (Advanced Video Coding, Advanced Video Coding), is a video compression standard, a widely used high-precision video recording, compression and Publish format.\n * H.265: High Efficiency Video Coding (HEVC for short) is a video compression standard, the successor of H.264/MPEG-4 AVC. HEVC not only improves the image quality, but also achieves twice the compression rate of H.264/MPEG-4 AVC (equivalent to a 50% reduction in the bit rate under the same picture quality), and can support 4K resolution or even ultra-high-quality TV, the highest The resolution can reach 8192×4320 (8K resolution), which is the current development trend.\n\nMPEG series: dominated by MPEG (Moving Picture Experts Group) under ISO (International Standards Organization), video coding mainly includes:\n\n * MPEG-1 Part II: Mainly used on VCD, some online videos also use this format, the quality of this codec is roughly equivalent to the original VHS video tape.\n * MPEG-2 Part II: Equivalent to H.262, used in DVD, SVCD and most digital video broadcasting systems and cable distribution systems (Cable Distribution Systems).\n * The second part of MPEG-4: It can be used in network transmission, broadcasting and media storage. Compared with MPEG-2 and the first version of H.263, its compression performance has been improved.\n * MPEG-4 Part 10: It is technically the same standard as ITU-TH.264. The cooperation between the two gave birth to the H.264/AVC standard. ITU-T named it H.264, while ISO/IEC called it It is MPEG-4 Advanced Video Coding (Advanced Video Coding, AVC).\n\nAVS (Audio Video coding Standard): my country\'s independent intellectual property source coding standard, which is the abbreviation of the "Advanced Audio and Video Coding for Information Technology" series of standards. At present, two generations of AVS standards have been formulated.\n\n * The first generation of AVS standards includes the national standard "Advanced Audio and Video Coding for Information Technology Part 2: Video" (AVS1 for short) and "Advanced Audio and Video Coding for Information Technology Part 16: Video for Broadcasting and Television" (AVS+ for short). The compression efficiency of AVS+ is equivalent to the highest level (High Profile) of the international similar standard H.264/AVC.\n * The second-generation AVS standard, referred to as AVS2, is the primary application target for ultra-high-definition video, and supports efficient compression of ultra-high-resolution (4K and above) and high-dynamic-range video. The compression efficiency of AVS2 has doubled that of the previous generation standard AVS+ and H.264/AVC, and surpassed the same type of international standard HEVC/H.265.\n\nOther series, such as VP8, VP9 (led by Google), RealVideo (produced by RealNetworks) and other encoding methods, are less used in Internet video and will not be introduced here.\n\n\n# Transcoding\n\nVideo transcoding (Video Transcoding) refers to converting a compressed and encoded video stream into another video stream to adapt to different network bandwidths, different terminal processing capabilities, and different user needs. Transcoding is essentially a process of first decoding and then encoding, so the code stream before and after conversion may or may not follow the same video coding standard.\n\n\n# Repackage (Remux)\n\nTranscapsulation refers to the conversion of the video or audio package format, such as converting AVI video to MP4, during which no audio and video encoding and decoding work is performed, but the video and audio compression stream is directly converted from one It is obtained from a package format file and then packaged into another package format file. Simply put, copy-paste. Compared with transcoding, transcapsulation has two characteristics: The processing speed is extremely fast. The audio and video encoding and decoding process is very complicated, occupying most of the transcoding time. Transcapsulation does not require encoding and decoding, saving a lot of processing time. The audio and video quality is lossless. There is no decoding (decompression) and encoding (compression) process, so there will be no audio and video compression damage. The repackaged file has almost the same resolution and bit rate as the original file, so it is also called "original picture" when playing. (Some packaging formats will have lossless compression algorithms, like While muxing sup into MKV there can be "zlib" compression used)\n\n\n# Detach (Demux)\n\nExtract\n\nSeparate the encoding format from the encapsulation format\n\n\n# Lossless compression and lossy compression\n\nFirst of all, PCM and LPCM are uncompressed formats, not called lossless compression. (But there should be nothing wrong with calling it lossless)\n\nLossless in daily life is more colloquial and needs to be understood. Generally speaking, the lossless format refers to the lossless algorithm, which does not mean that the generated files must be lossless. Some audio tracks will lose bit depth or frequency. There will also be some unconventional operations from lossy to lossless.\n\nIt is usually said that the original disc and Remux are lossless. The original disc is generally the highest quality source that can be found, but its encoding format is lossy, and it is released after lossy compression from the master tape (colloquial). Remux is mainly for lossless extraction or conversion of video tracks and main audio tracks. (But there will be Hybrid)\n\n\n# audio\n\nThe compression rate of DTS (referring to the company) audio tracks is very poor, and they use this as a promotion, thinking that it will improve decoding efficiency and sound quality will be better.\n\nDTS-HD MA is lossless\n\nDTS-HD HRA is lossy\n\nBoth also have a core (DTS core, usually DTS refers to DTS core, but I don\'t know what it refers to).\n\nIts encoder also contains the coding core of DTS Digital (DTS lossy audio format) so that when the player is not compatible with DTS-HD MA or DTS-HD HRA (DTS-HD High Resolution Audio), it can switch to DTS Digital (DTS Digital Generally, it will be encoded at a constant bit rate of 255Kbps, placed in the same stream as DTS-HD MA/HRA audio, and the format name is DTS-HD Core)\n\n\n# Lossy Audio\n\nAC3 is an industry standard for the film and television industry. But its release time is very early. (slightly earlier than MP3)\n\n# DTS\n\nDTS core 768 is considered inferior to AC3 640.\n\n# AC3\n\nDolby Laboratories used the MDCT algorithm along with perceptual coding principles to develop the AC-3 audio format for theater needs. The AC-3 format was released in 1991 as a Dolby Digital standard.\n\n * AC-3 (Audio Codec 3, Advanced Codec 3, Acoustic Coder 3. [Different from Adaptive Transform Acoustic Coding 3/ATRAC3, another format developed by Sony])\n\n * Dolby Digital Surround EX\n\nDolby Digital Surround EX (Dolby Digital Surround EX), is a collaboration product between Dolby Laboratories and Lucasfilm THX in the first Star Wars: Menace Lurking movie released in May 1999. In consideration of economic benefits and backward compatibility, a rear surround channel is inserted between the left surround and right surround to form a 6.1 output. It uses matrix encoding, which is a separate channel, the situation is like the relationship between the front left and right channels and the center channel. So it can output 5.1 on standard 5.1 equipment, and can output 6.1 on equipment that supports Dolby Digital Surround EX at the same time. This technique is used in the Star Wars series. Many DVDs support the output of Dolby Digital Surround EX.\n\n * Dolby Digital EX\n\nDolby Digital EX (Dolby Digital EX) is the civilian version of Dolby Digital Surround EX. Dolby Digital EX is similar to Dolby\'s early Pro-Logic technology, integrating matrix technology to add a center and a rear channel to the stereo sound track. Dolby Digital EX adds a rear channel on the basis of 5.1 Dolby Digital to create a 6.1 or 7.1 channel output. However, this technology is not considered a true 6.1 or 7.1 encoding, and unlike its rival DTS-ES format, it does not provide a complete and discrete 6 or 7 audio tracks.\n\n# Dolby Digital Plus/DDP/EAC3\n\nCurrently the preferred format of this group (except 1.0/2.0, it is generally recommended to use EX when making)\n\nDolby Digital Plus (Dolby Digital Plus), also known as E-AC-3, is an enhanced coding system based on AC3. It increases the maximum bit rate to 6 Mbps, supports 14 channels (13.1), and enhanced encoding technology can reduce compression artifacts. Not compatible with Dolby Digital equipment, but Dolby Digital Plus decoder can transcode Dolby Digital Plus into Dolby Digital through optical fiber/coaxial output. Dolby Digital Plus is the mandatory audio format for HD DVD and Blu-ray Disc. In Blu-ray players, Dolby Digital Plus is an optional format for the primary audio track (Primary Audio), and a mandatory format for the secondary audio track (Secondary Audio).\n\n# AAC\n\nAAC has more advantages at low codes. (but Opus does it better)\n\nBecause AAC is a huge family, they are divided into 9 specifications to meet the needs of different occasions. It is also because of the variety of AAC profiles that ordinary computer users feel very troubled:\n\n * MPEG-2 AAC LC low complexity specification (Low Complexity)\n\n * MPEG-2 AAC Main main specification\n\n * MPEG-2 AAC SSR variable sampling rate specification (Scaleable Sample Rate)\n\n * MPEG-4 AAC LC Low Complexity specification (Low Complexity), the audio part of the MP4 file that is more common in mobile phones now includes the audio file of this specification\n\n * MPEG-4 AAC Main Specification\n\n * MPEG-4 AAC SSR variable sampling rate specification (Scaleable Sample Rate)\n\n * MPEG-4 AAC LTP long term prediction specification (Long Term Prediction)\n\n * MPEG-4 AAC LD low delay specification (Low Delay)\n\n * MPEG-4 AAC HE high efficiency specification (High Efficiency)\n\nAmong the above-mentioned specifications, the main specification (Main) includes all functions except gain control, and its sound quality is the best, while the low-complexity specification (LC) is relatively simple, without gain control, but improves the coding efficiency. The specifications of "SSR" and "LC" are basically the same, but there is more gain control function. In addition, MPEG-4 AAC/LTP/LD/HE are used for encoding at low bit rates, especially "HE" is It is supported by Nero AAC encoder, which is a commonly used encoder recently, but generally speaking, the sound quality of the Main specification and the LC specification is not much different, so most of the AAC specifications currently used are "LC" specifications, because mobile phones must be considered Current memory capacity is not at a reasonable level.\n\nMPEG-4 AAC LC (Low Complexity) is the most commonly used specification. We call it "Low Complexity Specification", and we call it "LC-AAC" for short. This specification can find a balance between medium bit rate coding efficiency and sound quality. point. The so-called medium bit rate refers to the bit rate between 96kbps-192kbps. Therefore, if you want to use the LC-AAC specification, please try to control the code rate within the range mentioned above.\n\nffmpeg\n\nBased on quality produced from high to low:\n\nlibopus > libvorbis >= libfdk_aac > aac > libmp3lame >= eac3/ac3 > libtwolame > vorbis > mp2 > wmav2/wmav1\n\n# Some lossy audio tests\n\ndouble blind:\n\nhttps://web.archive.org/web/20060831191536/http://www.rjamorim.com/test/\n\nhttps://web.archive.org/web/20110522045514/http://cec.concordia.ca/econtact/9_4/tsabary.html\n\nhttps://tech.ebu.ch/docs/tech/tech3324.pdf',normalizedContent:'tip\n\nthe following content is taken from fxxs-encode-guide, thanks for their efforts\n\n\n# container\n\ncontainer (container) is also known as encapsulation format, or format (format), multimedia container (multimedia container), common encapsulation format (mp4/mkv...), and h.264/h.265/ac3/aac belongs to the encoding method \\ encoding format (codec), the video stream and audio stream encapsulated in mp4 and mkv (encapsulation format) can use the same encoding format\n\n\n# some caveats\n\n\n# flac - free lossless audio codec - free lossless audio compression coding\n\nflac is both an encoding format and an encapsulation format (flac uses the same name as the container name (that is, it is also the name of the encapsulation format), <https://developer.mozilla.org/zh-tw/docs/web/media/formats/ containers>)\n\n\n# m4a\n\nm4a (encapsulation format, audio-only mpeg-4/mp4 files will use the .m4a extension) can be encoded using alac (apple lossless audio codec) or aac (advanced audio coding).\n\n\n# mp3\n\nmp3 (formally mpeg-1 audio layer iii or mpeg-2 audio layer iii) is composed of mpeg-1 (encapsulation container) and an audio track encoded with mpeg-1 audio layer iii encoding.\n\n\n# wav related\n\nwav belongs to the package format, and the packaged pcm belongs to the original audio data, which is completely uncompressed encoding method\n\npcm - pulse code modulation (pulse code modulation) is a method of digitizing analog signals. it is the most commonly used and simplest waveform encoding method.\n\nlpcm is a specific type of pcm, and although ‎‎pcm‎‎ is a more general term, it is often used to describe data encoded as lpcm. ‎\n\nbwf (broadcast wave format)\n\nthe suffix is also wav, as the successor of wav, which is a standard audio format created by the european broadcasting union (european broadcasting union). bwf has more data regarding the file. but there is no difference in track quality\n\nrf64 (the suffix is usually written w64 to distinguish) (multichannel audio file)\n\nan extension of the wav file format, which can be larger than 4gb in file size. it has been specified by the european broadcasting union. it has been accepted as the itu recommendation itu-r bs.2088.\n\n\n# h.265 related\n\nh.265/hevc is the encoding\n\nh.265 is the standard, hevc is the winner, also called mpeg-h part 2\n\nsome people think that h.265 is used to refer to non-x265 hevc commercial encoders in communication (personally thinks it is non-standard and has no definition, it is a colloquial content, for reference only)\n\nand some sites require that the web-dl title must be written in h.265, and hevc cannot be used, while the original disk/remux is only allowed to write hevc. (i personally think it belongs to regional rules, and it is not recommended to be understood as a standard)\n\nx265 is an open source and free encoder, and there are also some commercial encoders, such as strene,[nvenc](https://www.nvidia.com/en- us/geforce/guides/broadcasting-guide/)\n\n\n# qaac\n\nqaac is a command-line aac/alac encoder frontend based on the apple encoder. since 1.00, qaac uses coreaudiotoolbox.dll directly. therefore, it is no longer necessary to install quicktime. however, apple application support is required. support aac-lc, aac-he, alac encoding. generally, what we make is lc\n\n(qaac is open source, seems to be copylefted and has no restrictions)\n\n\n# package format\n\nthere are many common mp4, mkv, rmvb, ts, flv, avi, etc., and the encodings supported by various packaging formats are different.\n\nmkv vs mp4, the main differences are:\n\n 1. mkv supports encapsulating flac as audio, but mp4 does not. but mp4 can also encapsulate lossless audio tracks (such as alac, although it is generally believed that alac is not as efficient as flac)\n 2. mkv supports encapsulating subtitles in ass/ssa format, but mp4 does not. generally, the subtitles produced by the subtitle group are in ass format, so inner subtitles are more common in mkv format\n 3. as an industry standard, mp4 is generally more compatible with video editing software and playback devices than mkv (of course, premiere can install plug-ins to support mkv, and ios and android can also be supported by third-party players), which is also the general compression group for mobile device-optimized video is basically the reason for choosing the mp4 package.\n\nthere are also some obsolete packaging formats, such as rm, avi and so on.\n\n\n# encoding format\n\nusually mediainfo needs to be packaged in a format to display relatively complete information, such as .pcm .aac .hevc cannot display information such as duration. .hevc cannot use the progress bar in the player.\n\nlpcm (linear pulse-code modulation) is generally also called pcm, but a specific type of pcm is different from pcm (pulse code modulation), and its quantization level is linear and uniform.\n\nh.26x series: led by itu (international telecommunication union), including h.261, h.262, h.263, h.264, h.265.\n\n * h.264: the tenth part of h.264/mpeg-4, or avc (advanced video coding, advanced video coding), is a video compression standard, a widely used high-precision video recording, compression and publish format.\n * h.265: high efficiency video coding (hevc for short) is a video compression standard, the successor of h.264/mpeg-4 avc. hevc not only improves the image quality, but also achieves twice the compression rate of h.264/mpeg-4 avc (equivalent to a 50% reduction in the bit rate under the same picture quality), and can support 4k resolution or even ultra-high-quality tv, the highest the resolution can reach 8192×4320 (8k resolution), which is the current development trend.\n\nmpeg series: dominated by mpeg (moving picture experts group) under iso (international standards organization), video coding mainly includes:\n\n * mpeg-1 part ii: mainly used on vcd, some online videos also use this format, the quality of this codec is roughly equivalent to the original vhs video tape.\n * mpeg-2 part ii: equivalent to h.262, used in dvd, svcd and most digital video broadcasting systems and cable distribution systems (cable distribution systems).\n * the second part of mpeg-4: it can be used in network transmission, broadcasting and media storage. compared with mpeg-2 and the first version of h.263, its compression performance has been improved.\n * mpeg-4 part 10: it is technically the same standard as itu-th.264. the cooperation between the two gave birth to the h.264/avc standard. itu-t named it h.264, while iso/iec called it it is mpeg-4 advanced video coding (advanced video coding, avc).\n\navs (audio video coding standard): my country\'s independent intellectual property source coding standard, which is the abbreviation of the "advanced audio and video coding for information technology" series of standards. at present, two generations of avs standards have been formulated.\n\n * the first generation of avs standards includes the national standard "advanced audio and video coding for information technology part 2: video" (avs1 for short) and "advanced audio and video coding for information technology part 16: video for broadcasting and television" (avs+ for short). the compression efficiency of avs+ is equivalent to the highest level (high profile) of the international similar standard h.264/avc.\n * the second-generation avs standard, referred to as avs2, is the primary application target for ultra-high-definition video, and supports efficient compression of ultra-high-resolution (4k and above) and high-dynamic-range video. the compression efficiency of avs2 has doubled that of the previous generation standard avs+ and h.264/avc, and surpassed the same type of international standard hevc/h.265.\n\nother series, such as vp8, vp9 (led by google), realvideo (produced by realnetworks) and other encoding methods, are less used in internet video and will not be introduced here.\n\n\n# transcoding\n\nvideo transcoding (video transcoding) refers to converting a compressed and encoded video stream into another video stream to adapt to different network bandwidths, different terminal processing capabilities, and different user needs. transcoding is essentially a process of first decoding and then encoding, so the code stream before and after conversion may or may not follow the same video coding standard.\n\n\n# repackage (remux)\n\ntranscapsulation refers to the conversion of the video or audio package format, such as converting avi video to mp4, during which no audio and video encoding and decoding work is performed, but the video and audio compression stream is directly converted from one it is obtained from a package format file and then packaged into another package format file. simply put, copy-paste. compared with transcoding, transcapsulation has two characteristics: the processing speed is extremely fast. the audio and video encoding and decoding process is very complicated, occupying most of the transcoding time. transcapsulation does not require encoding and decoding, saving a lot of processing time. the audio and video quality is lossless. there is no decoding (decompression) and encoding (compression) process, so there will be no audio and video compression damage. the repackaged file has almost the same resolution and bit rate as the original file, so it is also called "original picture" when playing. (some packaging formats will have lossless compression algorithms, like while muxing sup into mkv there can be "zlib" compression used)\n\n\n# detach (demux)\n\nextract\n\nseparate the encoding format from the encapsulation format\n\n\n# lossless compression and lossy compression\n\nfirst of all, pcm and lpcm are uncompressed formats, not called lossless compression. (but there should be nothing wrong with calling it lossless)\n\nlossless in daily life is more colloquial and needs to be understood. generally speaking, the lossless format refers to the lossless algorithm, which does not mean that the generated files must be lossless. some audio tracks will lose bit depth or frequency. there will also be some unconventional operations from lossy to lossless.\n\nit is usually said that the original disc and remux are lossless. the original disc is generally the highest quality source that can be found, but its encoding format is lossy, and it is released after lossy compression from the master tape (colloquial). remux is mainly for lossless extraction or conversion of video tracks and main audio tracks. (but there will be hybrid)\n\n\n# audio\n\nthe compression rate of dts (referring to the company) audio tracks is very poor, and they use this as a promotion, thinking that it will improve decoding efficiency and sound quality will be better.\n\ndts-hd ma is lossless\n\ndts-hd hra is lossy\n\nboth also have a core (dts core, usually dts refers to dts core, but i don\'t know what it refers to).\n\nits encoder also contains the coding core of dts digital (dts lossy audio format) so that when the player is not compatible with dts-hd ma or dts-hd hra (dts-hd high resolution audio), it can switch to dts digital (dts digital generally, it will be encoded at a constant bit rate of 255kbps, placed in the same stream as dts-hd ma/hra audio, and the format name is dts-hd core)\n\n\n# lossy audio\n\nac3 is an industry standard for the film and television industry. but its release time is very early. (slightly earlier than mp3)\n\n# dts\n\ndts core 768 is considered inferior to ac3 640.\n\n# ac3\n\ndolby laboratories used the mdct algorithm along with perceptual coding principles to develop the ac-3 audio format for theater needs. the ac-3 format was released in 1991 as a dolby digital standard.\n\n * ac-3 (audio codec 3, advanced codec 3, acoustic coder 3. [different from adaptive transform acoustic coding 3/atrac3, another format developed by sony])\n\n * dolby digital surround ex\n\ndolby digital surround ex (dolby digital surround ex), is a collaboration product between dolby laboratories and lucasfilm thx in the first star wars: menace lurking movie released in may 1999. in consideration of economic benefits and backward compatibility, a rear surround channel is inserted between the left surround and right surround to form a 6.1 output. it uses matrix encoding, which is a separate channel, the situation is like the relationship between the front left and right channels and the center channel. so it can output 5.1 on standard 5.1 equipment, and can output 6.1 on equipment that supports dolby digital surround ex at the same time. this technique is used in the star wars series. many dvds support the output of dolby digital surround ex.\n\n * dolby digital ex\n\ndolby digital ex (dolby digital ex) is the civilian version of dolby digital surround ex. dolby digital ex is similar to dolby\'s early pro-logic technology, integrating matrix technology to add a center and a rear channel to the stereo sound track. dolby digital ex adds a rear channel on the basis of 5.1 dolby digital to create a 6.1 or 7.1 channel output. however, this technology is not considered a true 6.1 or 7.1 encoding, and unlike its rival dts-es format, it does not provide a complete and discrete 6 or 7 audio tracks.\n\n# dolby digital plus/ddp/eac3\n\ncurrently the preferred format of this group (except 1.0/2.0, it is generally recommended to use ex when making)\n\ndolby digital plus (dolby digital plus), also known as e-ac-3, is an enhanced coding system based on ac3. it increases the maximum bit rate to 6 mbps, supports 14 channels (13.1), and enhanced encoding technology can reduce compression artifacts. not compatible with dolby digital equipment, but dolby digital plus decoder can transcode dolby digital plus into dolby digital through optical fiber/coaxial output. dolby digital plus is the mandatory audio format for hd dvd and blu-ray disc. in blu-ray players, dolby digital plus is an optional format for the primary audio track (primary audio), and a mandatory format for the secondary audio track (secondary audio).\n\n# aac\n\naac has more advantages at low codes. (but opus does it better)\n\nbecause aac is a huge family, they are divided into 9 specifications to meet the needs of different occasions. it is also because of the variety of aac profiles that ordinary computer users feel very troubled:\n\n * mpeg-2 aac lc low complexity specification (low complexity)\n\n * mpeg-2 aac main main specification\n\n * mpeg-2 aac ssr variable sampling rate specification (scaleable sample rate)\n\n * mpeg-4 aac lc low complexity specification (low complexity), the audio part of the mp4 file that is more common in mobile phones now includes the audio file of this specification\n\n * mpeg-4 aac main specification\n\n * mpeg-4 aac ssr variable sampling rate specification (scaleable sample rate)\n\n * mpeg-4 aac ltp long term prediction specification (long term prediction)\n\n * mpeg-4 aac ld low delay specification (low delay)\n\n * mpeg-4 aac he high efficiency specification (high efficiency)\n\namong the above-mentioned specifications, the main specification (main) includes all functions except gain control, and its sound quality is the best, while the low-complexity specification (lc) is relatively simple, without gain control, but improves the coding efficiency. the specifications of "ssr" and "lc" are basically the same, but there is more gain control function. in addition, mpeg-4 aac/ltp/ld/he are used for encoding at low bit rates, especially "he" is it is supported by nero aac encoder, which is a commonly used encoder recently, but generally speaking, the sound quality of the main specification and the lc specification is not much different, so most of the aac specifications currently used are "lc" specifications, because mobile phones must be considered current memory capacity is not at a reasonable level.\n\nmpeg-4 aac lc (low complexity) is the most commonly used specification. we call it "low complexity specification", and we call it "lc-aac" for short. this specification can find a balance between medium bit rate coding efficiency and sound quality. point. the so-called medium bit rate refers to the bit rate between 96kbps-192kbps. therefore, if you want to use the lc-aac specification, please try to control the code rate within the range mentioned above.\n\nffmpeg\n\nbased on quality produced from high to low:\n\nlibopus > libvorbis >= libfdk_aac > aac > libmp3lame >= eac3/ac3 > libtwolame > vorbis > mp2 > wmav2/wmav1\n\n# some lossy audio tests\n\ndouble blind:\n\nhttps://web.archive.org/web/20060831191536/http://www.rjamorim.com/test/\n\nhttps://web.archive.org/web/20110522045514/http://cec.concordia.ca/econtact/9_4/tsabary.html\n\nhttps://tech.ebu.ch/docs/tech/tech3324.pdf',charsets:{}},{title:"What is encoding",frontmatter:{title:"What is encoding",date:"2022-08-18T13:49:13.000Z",permalink:"/pages/7b7d11/"},regularPath:"/10.Getting%20Started/20.Related%20Articles/07.what%20is%20encoding.html",relativePath:"10.Getting Started/20.Related Articles/07.what is encoding.md",key:"v-c711cda8",path:"/pages/7b7d11/",headers:[{level:2,title:"Source, encoding, transparency comparison",slug:"source-encoding-transparency-comparison",normalizedTitle:"source, encoding, transparency comparison",charIndex:89}],headersStr:"Source, encoding, transparency comparison",content:"Tip\n\nThe following content is taken from FXXS-Encode-Guide, thanks for their efforts\n\n\n# Source, encoding, transparency comparison\n\nSource: The source refers to high-quality video that can be used for compression encoding, generally including Blu-ray discs, high-quality WEB (generally 4K SDR), etc. Tablets should be compressed from the best quality source to improve compression quality.\n\nWith the launch of BD of many old films, many blue light sources are pulled from low resolution to high resolution, which is called upscale. When suppressing, you should pay attention to restoring the resolution to the original version, and do not suppress too high resolution. rate (when not super resolution).\n\nThe WEB source should be used with caution, especially the 1080p web-dl is generally of low quality and cannot be re-compressed. Please do not suppress it if it is not necessary. 4k SDR WEB sources (generally from overseas streaming media such as Netflix and Prime Video) are currently rated well, and may be better than 1080p BD sources, and can be used for comparison with compressed 1080p videos.\n\nFor animation, the BD original disk is preferred, followed by high-quality WEB sources such as Crunchyroll\n\nWarning\n\nIt is not recommended to re-encode the video that has already been encoded. Generally speaking, re-encoding violates the principle of selecting the best quality source for compression.\n\nSVFI recommends to use the original disk source to compress\n\nEncoding: Encoding refers to compressing and encoding high-quality sources to further save volume. The encoding process mainly deals with source processing, and fixes obvious common problems in the source, usually including dirty edges, color bands, fragments, color deviation, gamma correction, etc.; removes black edges, and removes black borders in the 16:9 ratio of the source ;Suppress space, mainly re-encoding the source with a higher compression method, usually a lossy method.\n\nTransparency Comparison: Transparency is an index that evaluates the degree of similarity between codes. Good transparency means that even experienced coders cannot clearly distinguish the gap between the suppressed work and the source.\n\nAccording to the functions of I, P, and B frames, during the encoding process, the encoder generally respects the content of the I frame, even if it is compressed with inferior quality, it will not be significantly converted; for the p frame, the encoder can be converted into The B frame is encoded to save space, and the B frame is re-encoded. Therefore, the transparency should be judged from the comparison of the conversion quality of P->B and B->B in the source and suppression.\n\nBloating: It is called bloated, which means that the code rate is higher under the same transparency. According to the more widely accepted rules of suppression, when the code rate of the compressed work is too close to the source, it is also considered Bloating, and the values are as follows: (x265 recommends a smaller ratio)\n\n                           1080P   720P\nOriginal disk rate ratio   75%     50%\n\nTip\n\nSVFI uses prefabricated compression parameters to suppress the video stream after super resolution or frame complementation. When it is set to AUTO, it will automatically adjust the compression parameters according to the original video parameters",normalizedContent:"tip\n\nthe following content is taken from fxxs-encode-guide, thanks for their efforts\n\n\n# source, encoding, transparency comparison\n\nsource: the source refers to high-quality video that can be used for compression encoding, generally including blu-ray discs, high-quality web (generally 4k sdr), etc. tablets should be compressed from the best quality source to improve compression quality.\n\nwith the launch of bd of many old films, many blue light sources are pulled from low resolution to high resolution, which is called upscale. when suppressing, you should pay attention to restoring the resolution to the original version, and do not suppress too high resolution. rate (when not super resolution).\n\nthe web source should be used with caution, especially the 1080p web-dl is generally of low quality and cannot be re-compressed. please do not suppress it if it is not necessary. 4k sdr web sources (generally from overseas streaming media such as netflix and prime video) are currently rated well, and may be better than 1080p bd sources, and can be used for comparison with compressed 1080p videos.\n\nfor animation, the bd original disk is preferred, followed by high-quality web sources such as crunchyroll\n\nwarning\n\nit is not recommended to re-encode the video that has already been encoded. generally speaking, re-encoding violates the principle of selecting the best quality source for compression.\n\nsvfi recommends to use the original disk source to compress\n\nencoding: encoding refers to compressing and encoding high-quality sources to further save volume. the encoding process mainly deals with source processing, and fixes obvious common problems in the source, usually including dirty edges, color bands, fragments, color deviation, gamma correction, etc.; removes black edges, and removes black borders in the 16:9 ratio of the source ;suppress space, mainly re-encoding the source with a higher compression method, usually a lossy method.\n\ntransparency comparison: transparency is an index that evaluates the degree of similarity between codes. good transparency means that even experienced coders cannot clearly distinguish the gap between the suppressed work and the source.\n\naccording to the functions of i, p, and b frames, during the encoding process, the encoder generally respects the content of the i frame, even if it is compressed with inferior quality, it will not be significantly converted; for the p frame, the encoder can be converted into the b frame is encoded to save space, and the b frame is re-encoded. therefore, the transparency should be judged from the comparison of the conversion quality of p->b and b->b in the source and suppression.\n\nbloating: it is called bloated, which means that the code rate is higher under the same transparency. according to the more widely accepted rules of suppression, when the code rate of the compressed work is too close to the source, it is also considered bloating, and the values are as follows: (x265 recommends a smaller ratio)\n\n                           1080p   720p\noriginal disk rate ratio   75%     50%\n\ntip\n\nsvfi uses prefabricated compression parameters to suppress the video stream after super resolution or frame complementation. when it is set to auto, it will automatically adjust the compression parameters according to the original video parameters",charsets:{}},{title:"What is HDR",frontmatter:{title:"What is HDR",date:"2022-08-18T13:49:13.000Z",permalink:"/pages/89244b/"},regularPath:"/10.Getting%20Started/20.Related%20Articles/08.What%20is%20HDR.html",relativePath:"10.Getting Started/20.Related Articles/08.What is HDR.md",key:"v-0d1770c8",path:"/pages/89244b/",headers:[{level:2,title:"Introduction to HDR",slug:"introduction-to-hdr",normalizedTitle:"introduction to hdr",charIndex:365},{level:2,title:"Photometric concepts and units",slug:"photometric-concepts-and-units",normalizedTitle:"photometric concepts and units",charIndex:2222},{level:2,title:"photography HDR",slug:"photography-hdr",normalizedTitle:"photography hdr",charIndex:4881},{level:3,title:"dodging and burning and tone mapping",slug:"dodging-and-burning-and-tone-mapping",normalizedTitle:"dodging and burning and tone mapping",charIndex:6073},{level:2,title:"video HDR",slug:"video-hdr",normalizedTitle:"video hdr",charIndex:1917},{level:4,title:"PQ & HLG",slug:"pq-hlg",normalizedTitle:"pq &amp; hlg",charIndex:null},{level:2,title:"HDR Standard",slug:"hdr-standard",normalizedTitle:"hdr standard",charIndex:10487},{level:4,title:"HDR10",slug:"hdr10",normalizedTitle:"hdr10",charIndex:7752},{level:4,title:"DOLBY VISION",slug:"dolby-vision",normalizedTitle:"dolby vision",charIndex:10512},{level:4,title:"HDR10+",slug:"hdr10-2",normalizedTitle:"hdr10+",charIndex:10528},{level:4,title:"UHD Premium",slug:"uhd-premium",normalizedTitle:"uhd premium",charIndex:10538},{level:4,title:"DisPlayHDR",slug:"displayhdr",normalizedTitle:"displayhdr",charIndex:10553},{level:4,title:"Mobile HDR",slug:"mobile-hdr",normalizedTitle:"mobile hdr",charIndex:10567},{level:4,title:"CUVA HDR",slug:"cuva-hdr",normalizedTitle:"cuva hdr",charIndex:10581},{level:4,title:"VESA HDR",slug:"vesa-hdr",normalizedTitle:"vesa hdr",charIndex:10593},{level:2,title:"Mobile phone support for HDR",slug:"mobile-phone-support-for-hdr",normalizedTitle:"mobile phone support for hdr",charIndex:10606},{level:4,title:"iPhone",slug:"iphone",normalizedTitle:"iphone",charIndex:4788},{level:2,title:"FFmpeg analyzes HDR content",slug:"ffmpeg-analyzes-hdr-content",normalizedTitle:"ffmpeg analyzes hdr content",charIndex:11327},{level:4,title:"ffprobe analyzes the metadata of HDR content",slug:"ffprobe-analyzes-the-metadata-of-hdr-content",normalizedTitle:"ffprobe analyzes the metadata of hdr content",charIndex:11358},{level:4,title:"FFmpeg transcoding HDR content",slug:"ffmpeg-transcoding-hdr-content",normalizedTitle:"ffmpeg transcoding hdr content",charIndex:14072},{level:2,title:"Related terms",slug:"related-terms",normalizedTitle:"related terms",charIndex:14995},{level:4,title:"Color Calibration of Screens (screen color correction)",slug:"color-calibration-of-screens-screen-color-correction",normalizedTitle:"color calibration of screens (screen color correction)",charIndex:15012},{level:4,title:"Color Spaces",slug:"color-spaces",normalizedTitle:"color spaces",charIndex:15375},{level:4,title:"Contrast Ratio",slug:"contrast-ratio",normalizedTitle:"contrast ratio",charIndex:16119},{level:4,title:"CRI Color Remapping Information (color remapping information)",slug:"cri-color-remapping-information-color-remapping-information",normalizedTitle:"cri color remapping information (color remapping information)",charIndex:16298},{level:4,title:"DCI-P3, D65-P3, ST 428-1",slug:"dci-p3-d65-p3-st-428-1",normalizedTitle:"dci-p3, d65-p3, st 428-1",charIndex:16980},{level:4,title:"EDID Extended Display Identification Data",slug:"edid-extended-display-identification-data",normalizedTitle:"edid extended display identification data",charIndex:17767},{level:4,title:"EOTF Electro-optic transfer function",slug:"eotf-electro-optic-transfer-function",normalizedTitle:"eotf electro-optic transfer function",charIndex:18517},{level:4,title:"OETF Photoelectric Transfer Function",slug:"oetf-photoelectric-transfer-function",normalizedTitle:"oetf photoelectric transfer function",charIndex:18811},{level:4,title:"OOTF Optical-to-optical transfer function",slug:"ootf-optical-to-optical-transfer-function",normalizedTitle:"ootf optical-to-optical transfer function",charIndex:19356},{level:4,title:"Flicker Flicker",slug:"flicker-flicker",normalizedTitle:"flicker flicker",charIndex:19588},{level:4,title:"f-stop of Dynamic Range The number of f-stops of the dynamic range",slug:"f-stop-of-dynamic-range-the-number-of-f-stops-of-the-dynamic-range",normalizedTitle:"f-stop of dynamic range the number of f-stops of the dynamic range",charIndex:19956},{level:4,title:"Gamut or Color Gamut",slug:"gamut-or-color-gamut",normalizedTitle:"gamut or color gamut",charIndex:20576},{level:4,title:"Gamut Mapping Gamut Mapping",slug:"gamut-mapping-gamut-mapping",normalizedTitle:"gamut mapping gamut mapping",charIndex:21127},{level:4,title:"IMF Interactive Master File",slug:"imf-interactive-master-file",normalizedTitle:"imf interactive master file",charIndex:21980},{level:4,title:"Inverse Tone Mapping \\(ITM\\) Inverse tone mapping / reverse tone mapping",slug:"inverse-tone-mapping-itm-inverse-tone-mapping-reverse-tone-mapping",normalizedTitle:"inverse tone mapping (itm) inverse tone mapping / reverse tone mapping",charIndex:22637},{level:4,title:"LUT lookup table",slug:"lut-lookup-table",normalizedTitle:"lut lookup table",charIndex:22920},{level:4,title:"MaxCLL Metadata MaxCLL Metadata",slug:"maxcll-metadata-maxcll-metadata",normalizedTitle:"maxcll metadata maxcll metadata",charIndex:23876},{level:4,title:"MaxFALL Metadata MaxFALL Metadata",slug:"maxfall-metadata-maxfall-metadata",normalizedTitle:"maxfall metadata maxfall metadata",charIndex:24405},{level:4,title:"Peak Code Value Peak code value",slug:"peak-code-value-peak-code-value",normalizedTitle:"peak code value peak code value",charIndex:24733},{level:4,title:"Peak Display Luminance peak display brightness",slug:"peak-display-luminance-peak-display-brightness",normalizedTitle:"peak display luminance peak display brightness",charIndex:24892},{level:4,title:"Perceptual Quantizer sensory quantization curve",slug:"perceptual-quantizer-sensory-quantization-curve",normalizedTitle:"perceptual quantizer sensory quantization curve",charIndex:24989},{level:4,title:"Quantum Dot (QD) Displays",slug:"quantum-dot-qd-displays",normalizedTitle:"quantum dot (qd) displays",charIndex:25804},{level:4,title:"ST2084",slug:"st2084",normalizedTitle:"st2084",charIndex:18849},{level:4,title:"ST2086",slug:"st2086",normalizedTitle:"st2086",charIndex:26554},{level:4,title:"Tone Mapping/Tone Mapping Operator (TMO) tone mapping/tone mapping operator",slug:"tone-mapping-tone-mapping-operator-tmo-tone-mapping-tone-mapping-operator",normalizedTitle:"tone mapping/tone mapping operator (tmo) tone mapping/tone mapping operator",charIndex:26740},{level:4,title:"Wide Color Gamut (WCG) wide color gamut",slug:"wide-color-gamut-wcg-wide-color-gamut",normalizedTitle:"wide color gamut (wcg) wide color gamut",charIndex:27687},{level:4,title:"White Point White Point",slug:"white-point-white-point",normalizedTitle:"white point white point",charIndex:27964}],headersStr:"Introduction to HDR Photometric concepts and units photography HDR dodging and burning and tone mapping video HDR PQ & HLG HDR Standard HDR10 DOLBY VISION HDR10+ UHD Premium DisPlayHDR Mobile HDR CUVA HDR VESA HDR Mobile phone support for HDR iPhone FFmpeg analyzes HDR content ffprobe analyzes the metadata of HDR content FFmpeg transcoding HDR content Related terms Color Calibration of Screens (screen color correction) Color Spaces Contrast Ratio CRI Color Remapping Information (color remapping information) DCI-P3, D65-P3, ST 428-1 EDID Extended Display Identification Data EOTF Electro-optic transfer function OETF Photoelectric Transfer Function OOTF Optical-to-optical transfer function Flicker Flicker f-stop of Dynamic Range The number of f-stops of the dynamic range Gamut or Color Gamut Gamut Mapping Gamut Mapping IMF Interactive Master File Inverse Tone Mapping \\(ITM\\) Inverse tone mapping / reverse tone mapping LUT lookup table MaxCLL Metadata MaxCLL Metadata MaxFALL Metadata MaxFALL Metadata Peak Code Value Peak code value Peak Display Luminance peak display brightness Perceptual Quantizer sensory quantization curve Quantum Dot (QD) Displays ST2084 ST2086 Tone Mapping/Tone Mapping Operator (TMO) tone mapping/tone mapping operator Wide Color Gamut (WCG) wide color gamut White Point White Point",content:'Warning\n\nThis entry needs to be supplemented\n\nPlease refer to https://wangwei1237.github.io/2021/01/26/HDR-introduction/\n\nTip\n\nSVFI is able to automatically recognize if the input video has HDR and handle it properly. To specify the HDR format, please change Source HDR to a type other than AUTO, for example, to specify metadata by yourself, select CUSTOM HDR\n\n\n# Introduction to HDR Technology\n\nThe brightness range in the real world is very wide, and the brightness range that the human eye can perceive is about 100,000 nits. For example, using a spectrocolorimeter to measure a flower blooming towards the sun, the brightness of the yellow area can reach up to 14700 nits, the red edge is 2300 nits, and the central stamen and green leaves are only below 200 nits. However, under an SDR display with a narrow color gamut, brightness generally not exceeding 100 nits, and a contrast ratio of only 1000:1, the color of this photo will be much dimmer. But with the development of technology, HDR technology can achieve wide color gamut, 1000 nit brightness and tens of thousands of contrast. Although there is still a big difference from the actual standard, compared with the SDR of 30 years ago, HDR is still a big step forward.\n\n\n\n\n# Introduction to HDR\n\nHDR is an abbreviation for High Dynamic Range. HDR is a digital image technology that can be used to enhance the color and contrast range of digital images.\n\nCompared with SDR (Standard Dynamic Range: standard dynamic range), HDR has the following characteristics:\n\n * Wider color range\n * Brighter brightness cap\n * Darker brightness lower limit\n * The overall image quality has been improved in terms of contrast, grayscale resolution and other dimensions\n\nThus, HDR can bring users a more immersive experience.\n\nHDR technology can be applied to photography (photography) and video (video), but it should be noted that although the goals of photo HDR and video HDR are to make the digital content we see more similar to the real experience of human eyes, but these two are indeed two completely different concepts. Essentially, photo HDR is a process in which a camera captures photos (capture), while video HDR is a process in which display is displayed.\n\n\n# Photometric concepts and units\n\nBefore further introducing HDR, we need to understand some related concepts of Photometry so that we can have a better understanding of HDR understanding and awareness.\n\n * Radiation flux: In optical radiometry, the basic physical quantity used is radiation flux or radiation power, the symbol is expressed as: Φe\\Phi_{e}Φe , The unit is Watt (WWW). Radiant flux is emitted by a radiation source, transmitted through some propagation medium and received on some surface.\n\n * Luminous flux: Luminous flux refers to the derived amount of radiant flux evaluated according to the international standard human visual characteristics, that is, the photometric measure of radiant flux, and its symbol is: Φv\\Phi _{v}Φv . The luminous flux can be derived from the radiant flux.\n\n * Light Amount: Light Amount is also called Light Energy, which refers to the time integral of Luminous Flux within a given time interval (Δt\\Delta tΔt), and the symbol is expressed as: QvQ_ {v}Qv , and Qv=∫ΔtΦv⋅dtQ_{v}=\\int_{\\Delta t}{\\Phi_{v} \\ \\cdot dt}Qv =∫Δt Φv ⋅dt.\n\n * Luminous intensity: Luminous intensity (IvI_{v}Iv ) is the luminous flux dΦvd \\Phi_ emitted by the light source and propagated within the solid angle element dΩd \\OmegadΩ in the specified direction The quotient of {v}dΦv and the solid angle element dΩd \\OmegadΩ, namely: Iv=dΦvdΩI_{v}=\\frac{d \\Phi_{v}\\ }{d \\Omega}Iv =dΩdΦv . The unit of luminous intensity is candela (cdcdcd). Candela is the luminous intensity in a specified direction of a light source emitting monochromatic radiation with a frequency of 540×1012Hz540 \\times 10^{12} Hz540×1012Hz, and the radiant intensity of the light source in this direction is 1683W⋅ sr\\frac{1}{683} W \\cdot sr6831 W⋅sr.\n\n * Luminance: Luminance (IvI_{v}Iv ) refers to the luminous intensity of a surface light source per unit projected area in a certain direction along that direction. The area of the small surface element on the surface light source is dAdAdA, the angle between a certain direction and the surface element normal is θ\\thetaθ, and the projected area of the surface element along this direction is dA⋅cosθdA \\cdot cos \\thetadA⋅cosθ, Then the luminance of the surface element along this direction is: IvdA⋅cosθ\\frac{I_{v}}{dA \\cdot cos \\theta}dA⋅cosθIv , namely Iv\\ =dΦvdA⋅cosθ⋅dΩI_{v}=\\frac{d \\Phi_{v}}{dA \\cdot cos \\theta \\cdot d \\Omega\\ }Iv =dA⋅cosθ⋅dΩdΦv . It can be seen from this that the unit of brightness is cd/m2cd/m^{2}cd/m2 (candela/square meter), also known as nit. For example, the brightness of the screen of the iPhone 12 is: 625 nits maximum brightness (typical), 1200 nits maximum brightness (HDR).\n\n\n# photography HDR\n\nHDR photos can be generated by simultaneously capturing multiple images at different exposures. The camera quickly takes three or more photos with different exposure values, which are then processed to produce a single HDR photo.\n\nHDR image processing is very useful when taking photos of high-contrast scenes.\n\nAs shown in the night scene below, for example, when the exposure is too low, it can make the scenery behind appear dark. If the exposure is adjusted to show what\'s behind, what\'s in front will again be too bright, and may even be completely white. By combining multiple exposures, these different exposures can be averaged so that the scenery behind can appear in the photo without overexposing the scenery in front. [^2]\n\n\n\n\n\nCurrently, the cameras of many smartphones, such as the iPhone, have an HDR option. We can choose to have the camera:\n\n * Turn on HDR (ON)\n * Turn off HDR (OFF)\n * Automatically determine whether to enable HDR (Auto)\n\nWith Auto mode, the camera only takes HDR photos in high-contrast situations. Most cameras will also capture normal photos while compositing HDR photos, allowing us to choose between HDR photos and normal photos.\n\n\n# dodging and burning and tone mapping\n\nIn fact, in the film era, photographers would take multiple negatives with different exposure levels, and in the [darkroom](https://baike.baidu.com/item/%E6%9A%97%E6%88%BF /62291) uses a method of **dodging and burning** to combine several negatives into one with high dynamic range Photo. With the emergence of digital cameras, HDR introduces the technology in the darkroom into the camera imaging sensor. By taking multiple different exposures of the same scene and then synthesizing them, it can finally enhance the dynamic range of the photo and achieve the purpose of increasing the layering of the photo. . In photo HDR, this compositing technique is called tone mapping (tone mapping). Of course, the tone mapping technology will also be used in the next video HDR, the difference is that the tone mapping in video HDR is used to map the HDR content to the SDR display device, so that the SDR device can also display HDR content.\n\n\n# video HDR\n\nHDR video is captured in a similar way to HDR photos. Record each shot simultaneously with a different exposure (or a different ISO configuration). These shots are then fused together to produce a single shot. This method often results in brighter video, although it produces an unreal-looking gray effect. Because of this, HDR videos are often post-processed before release.\n\nIn order to display HDR video correctly, the display device must also support HDR. For example, an HDR-capable TV must support a specific video output standard. These standards will involve: 10-bit color and DCI-P3 color space covering at least 90%. HDR monitors must also support a specific HDR format, such as HDR10, Dolby Vision, or Hybrid Log Gamma (HLG).\n\n\n\nComparison of various color spaces in CIE 1931 xy\n\nTake HDR10 as an example, because the HDR10 standard uses Rec.2020 wide color gamut, 10-bit color depth and SMTPE ST 2084 PQ conversion function, so when displaying HDR10 content on a non-HDR display device, it will appear Two cases [3],[4]:\n\n * Unable to decode resulting in blurred screen\n * It can be decoded, but because the highlighted part of the video will be displayed according to the highest brightness of the display device, the overall video will be grayed out\n\nThis is the difference between HDR photos and HDR videos: In addition to video capture, HDR videos have a strong dependence on the capabilities of display devices.\n\n# PQ & HLG\n\nIn order to display HDR images correctly, it is not enough to simply increase the brightness, it is crucial to display the color and tone in a way that matches human vision. Color and tone are affected by the gamma input-output characteristic. In HDR, there are two gamma curves, PQ and HLG.\n\n * The PQ gamma curve is based on the characteristics of human visual perception, and is most suitable for producing movies or streaming video content on the Internet, where reproduction accuracy is key.\n * The HLG gamma curve is designed to allow display on existing SDR TVs without loss of position and is best suited for broadcast TV and live video feeds.\n\nThe specific differences between PQ and HLG are shown in the following table:\n\n                                       PQ (PERCEPTUAL QUANTIZATION)                                   HLG (HYBRID LOG GAMMA)\nGoals                                  Internet Video Streaming, Movies                               Broadcast TV, Live Video\nAdvantages                             Handles brightness up to 10,000 cd/m² in absolute value, new   Handles brightness as relative value (same as existing\n                                       gamma curve based on human visual perception                   standard) Compatible with SDR TVs\nMaximum Luminance                      1,000 cd/m² Absolute Same, regardless of Display               Relative, Varies by Display\nBlack Level                            0.005 cd/m2 or less                                            0.005 cd/m2 or less\nProposer                               Dolby                                                          BBC and NHK\nReference Standards                    SMPTE ST 2084, ITU-R BT.2100                                   ITU-R BT.2100\nThe effect on the SDR display device   Poor                                                           Good\nLive Effect                            Good                                                           Excellent\n\n\n# HDR Standard\n\n# HDR10\n\n# DOLBY VISION\n\n# HDR10+\n\n# UHD Premium\n\n# DisPlayHDR\n\n# Mobile HDR\n\n# CUVA HDR\n\n# VESA HDR\n\n\n# Mobile phone support for HDR\n\n# iPhone\n\niPhone 12 Pro\'s all-round support for Dolby Vision can be said to rely on its own power to bring the most advanced and avant-garde HDR technology to the eyes of the general public. As the first mobile phone to support HDR shooting, how does iPhone 12\'s Dolby Vision HDR shooting come true? Is it really as good as advertised?\n\nPerhaps we can learn from this expert Samuel Bilodeau, who has 5 years of HDR color correction and teaching experience.Find out in this review we wrote. The author said at the end of the review: I am particularly excited and happy to see the emergence of iPhone and other consumer cameras with HDR shooting capabilities, and push HDR a step forward.\n\n\n# FFmpeg analyzes HDR content\n\n# ffprobe analyzes the metadata of HDR content\n\nMetadata for Mastering Display and Content Light Level can be extracted using the ffprobe command. We only need to extract the relevant data of the first frame, so when analyzing, we can use -read_intervals "%+#1" option to have ffprobe only extract the metadata of the first frame. The specific analysis of fame and fortune is as follows:\n\nffprobe \\\n-hide_banner\\\n-loglevel warning \\\n-select_streams v \\\n-print_format json \\\n-show_frames \\\n-read_intervals "%+#1" \\\n-show_entries "frame=color_space,color_primaries,color_transfer,side_data_list,pix_fmt" \\\nHDR.mp4\n\n\nThe meaning of each option is as follows:\n\n * -hide_banner -loglevel warning: do not display information we do not need\n * -select_streams v: only select video streams to analyze\n * -print_format json: output analysis results in json format\n * -read_intervals "%+#1": only analyze the data in the first frame\n * -show_entries … : only output the data we specified\n\nAfter the command is executed, the following analysis results will be displayed:\n\n{\n     "frames": [\n         {\n             "pix_fmt": "yuv420p10le",\n             "color_space": "bt2020nc",\n             "color_primaries": "bt2020",\n             "color_transfer": "smpte2084",\n             "side_data_list": [\n                 {\n                     "side_data_type": "Mastering display metadata",\n                     "red_x": "34000/50000",\n                     "red_y": "16000/50000",\n                     "green_x": "13250/50000",\n                     "green_y": "34500/50000",\n                     "blue_x": "7500/50000",\n                     "blue_y": "3000/50000",\n                     "white_point_x": "15635/50000",\n                     "white_point_y": "16450/50000",\n                     "min_luminance": "40/10000",\n                     "max_luminance": "11000000/10000"\n                 }\n             ]\n         }\n     ]\n}\n\n\nAs shown above, the color value and the maximum/minimum brightness value in the ffprobe analysis result are both a ratio. The final result of the color value will determine the color space used by the master content, and the maximum/minimum brightness value will determine the master content. Dynamic Range.\n\nAccording to the above results, we can know that on the CIE coordinates, the coordinates of the red, green, blue and white points are:\n\n * red: (0.68, 0.32)\n * green: (0.265, 0.69)\n * blue: (0.15, 0.06)\n * white point: (0.3127, 0.329)\n\nFrom the relevant information of DCI-P3, it can be known that the master content of this video was produced in P3-D65 (Display) color Space, with a minimum brightness of 0.004 nits, a maximum brightness of 1100 nits, and a contrast ratio of 275,000.\n\n# FFmpeg transcoding HDR content\n\nFor the transcoding of HDR video, the content of Mastering Display and Content Light Level metadata needs to be passed to the encoder (note, not to FFmpeg), otherwise, these information will be lost during the transcoding process, This in turn leads to the impact of transcoding on the content of the screen. The specific way is as follows:\n\nffmpeg -i hdr.mp4 \\\n-map 0 -c:v libx265\\\n-x265-params hdr-opt=1:repeat-headers=1:colorprim=bt2020:transfer=smpte2084:colormatrix=bt2020nc:master-display=G(13250,34500)B(7500,3000)R(34000,16000) WP(15635,16450)L(11000000,40)\\\n-crf 20 \\\n-preset veryfast \\\n-pix_fmt yuv420p10le\\\ntest.mp4\n\n\nin,\n\n * hdr-opt=1: means we want to enable HDR\n * repeat-headers=1: Indicates that each frame needs these data\n * colorprim, transfer and colormatrix: consistent with ffprobe\n * master-display: The color string constructed according to the result of ffprobe\n\n\n# Related terms\n\n# Color Calibration of Screens (screen color correction)\n\nA process used to ensure that colors are accurately displayed on the screen. A colorimeter is generally used to measure the native color response of a display, then calculate an index for correction to ensure that the color is displayed correctly on the display, and finally test the corrected response.\n\n# Color Spaces\n\nA color space can refer to an organization of colors, or it can refer to a specific range of colors. In the field of cinema and television, we generally use RGB (using red, green, and blue primary color components to represent a color) or YUV (using the brightness (black and white value) of a color, and its color value calculated based on the difference between color components. degrees) color space (Translator\'s Note: RGB and YUV are color models, not color spaces, there is an error in the original text here). These color spaces are generally based on specific display device characteristics, see D65-P3 entry. Other color spaces, such as XYZ or Lab, are more suitable for representing the human eye color vision model.\n\n# Contrast Ratio\n\nContrast is the ratio of lightness between the lightest (white) part and the darkest (black) part that a system can produce, usually described by an n:1 ratio.\n\n# CRI Color Remapping Information (color remapping information)\n\nA standard set of metadata produced by analyzing two different masters of the same content, such as an HDR and an SDR master. When a master (such as HDR) and CRI metadata are transmitted together, the decoder can only decode HDR content when facing an HDR screen, and can also convert HDR content when facing an SDR screen by using CRI metadata. The content is transformed into SDR content. The main advantage of using this approach is that the author\'s creative intent is preserved for both decoded versions. CRI is a standard component of MPEG (HEVC v2) and is an optional feature for Ultra HD Blu-ray production.\n\n# DCI-P3, D65-P3, ST 428-1\n\nA digital cinema color space. The DCI-P3 color space is an RGB color space introduced by the Digital Cinema Initiatives in 2005 and standardized by SMPTE ST428-1 in 2006.\n\nThis color space has a much wider gamut than sRGB (see Rec.709).\n\nAll digital cinema projectors can fully display the DCI-P3 color space, D65 P3 refers to the color temperature of its white point changed from DCI white point to D65 white point.\n\nThe three triangles in the diagram represent: the largest is the Rec.2020 standard, the new standard for Ultra HD TV (currently only fully implemented by laser displays), the slightly smaller DCI-P3 for digital cinema, and traditional video surveillance The smallest Rec.709 color space used by TV receivers, HD broadcast TV, Blu-ray, OTT.\n\n# EDID Extended Display Identification Data\n\nEDID is an acronym for Extended Display Information Data, a standard developed by the Consumer Technology Association (CTA). This is provided by every DVI monitor, HDMI monitor, or other device that supports a DVI HDMI input (aka DVI/HDMI Sinks). Probably each DVI/HDMI input has its own EDID. EDID tells the device the performance characteristics of the display it is connected to.\n\nThe source device recognizes the presence of EDID memory on the display\'s DVI or HDMI interface and uses the information in it to optimize the output video (resolution, frame rate, color...) and/or audio format. All devices that support DVI/HDMI standard input, that is, TMDS sinks (DVI/HDMI Sinks) must implement EDID.\n\n# EOTF Electro-optic transfer function\n\nEOTF is an acronym for Electro-Optical Transfer Function. It is a mathematical function that maps code values to display brightness. In other words, an ETOF defines how code values in an image are displayed as visible light by a display or projector.\n\nSee OETF Photoelectric Transfer Function, ST2084.\n\n# OETF Photoelectric Transfer Function\n\nOETF is an acronym for Opto-Electronic Transfer Function. It is a mathematical function that maps scene luminosity (the light in a scene) to digitally encoded values that can be transmitted and compressed. This term is generally applied to devices that acquire images, such as digital cameras.\n\nIn post-production, content is typically graded on a screen with a specific EOTF, and historically, an inverse of the OETF was often used as the screen\'s EOTF.\n\n# OOTF Optical-to-optical transfer function\n\nOOTF is an acronym for Optical-to-Optical Transfer Function. It is a mathematical function that maps the brightness of the scene captured by the camera to the brightness of the display.\n\n# Flicker Flicker\n\nCharacteristic of certain kinds of monitors, such as old cathode ray tube monitors (CRTs), or poorly tuned flat panel monitors, or even motion picture film projectors. This unwelcome change in brightness occurs mainly at frequencies below 50 Visible in frames per second. For brighter displays, the human eye can perceive higher frequency flicker.\n\n# f-stop of Dynamic Range The number of f-stops of the dynamic range\n\nIn photography, a change of one f-stop corresponds to doubling (or halving) the light captured when capturing an image.\n\nThe number of f-stops contained in an image describes the contrast of the image (2^N notation). For example, a camera can output an image of 10 stops, which means that the contrast ratio can be as high as 2^10 (1024:1 ), that is, the white part will be 1024 times brighter than the black part. In comparison, the human eye can go up to 18-20 stops (this is a very high dynamic range HDR, a typical SDR video image is 6-7 stops)\n\n# Gamut or Color Gamut\n\nIn color reproduction, including but not limited to computer graphics and photography, a color gamut is a specific subset of colors.\n\ncolor gamutThe most common use of the term is to refer to a subset of colors that can be accurately reflected within a certain range. This premise may be a specific display device or a given color space. The color gamut is generally represented by the area on the CIE 1931 chromaticity diagram, and the edges of the CIE 1931 curve represent the range of colors in the visible light spectrum.\n\n# Gamut Mapping Gamut Mapping\n\nIn almost all translation processes (referring to the representation of a specific color, the conversion process in different color spaces), we have to face the reality that the range covered by the color gamut of different devices is different, which means Making accurate color reproduction impossible.\n\nTherefore, it is necessary to perform some processing on the parts near the edge of the color gamut. Some colors must be shifted into gamut, otherwise they cannot be displayed on the output device, and they will be roughly discarded (clipped). This is known as a gamut mismatch, for example when we convert from the wider RGB color space to the CMYK color space.\n\nA color management system can use a variety of methods to achieve the desired result and give the experienced user the means to control gamut mapping.\n\n# IMF Interactive Master File\n\nThe Interoperable Master Format (IMF) is a standard provided by SMPTE to implement a single-file, interchangeable master file format and structure for commercial content distribution worldwide. It evolved from the Digital Cinema Package (DCP) architecture, which provides a complete exchangeable unit of files for distribution channels. IMF can be said to be a revolutionary progress, IMF provides a real file-based final master. DCP is aimed at theatrical content distribution, while IMF provides a master format for commercial environments that can create multiple cut versions of the same content for different audiences.\n\n# Inverse Tone Mapping (ITM) Inverse tone mapping / reverse tone mapping\n\nRemastering of SDR content for HDR content.\n\nITM takes SDR content and expands its luminosity and color space to match the capabilities of HDR displays while preserving the original content creator\'s intent.\n\n# LUT lookup table\n\nIt is an acronym for Look Up Table. LUTs provide an efficient way of applying complex mathematical operations to incoming data—which would be computationally expensive without LUTs. As such, they are an ideal implementation for mapping an image from one color space to zero one color space.\n\nThe following LUTs exist:\n\nIn 3D LUTs, the output R\', G\', B\' value of each pixel is jointly calculated from the R, G, B value of the input pixel.\n\nFor 1D LUTs, the output R’ is only calculated from the input R value, and the same is true for G’ and B’. They are commonly used to apply corrections for gamma functions and other EOTFs, and are typically found in chipsets in consumer electronics devices.\n\n3D LUTs support more powerful mathematical transformations than 1D LUTs, but are more complex and expensive to implement in chipsets. Typically 3D LUTs are used to apply a creative "look" and color space conversion during post-production.\n\n# MaxCLL Metadata MaxCLL Metadata\n\nMaximum Content Light Level (MaxCLL) is an integer-type metadata value used to define the highest brightness of any pixel in an encoded HDR video stream or file, in units of nits. MaxCLL can be measured during the mastering process or after the production process, but in order to ensure the color transition of HDR monitors in the MaxCLL HDR range, and to apply a hard clip outside the maximum brightness of the monitor, the MaxCLL of the monitor can also be used as the MaxCLL element data.\n\n# MaxFALL Metadata MaxFALL Metadata\n\nMaximum Frame Average Light Level (MaxFALL) is an integer-type metadata value used to define the highest average brightness of any frame in an encoded HDR video stream or file, in units of nits. MaxFALL is obtained by calculating the average brightness of all pixels decoded in each frame.\n\n# Peak Code Value Peak code value\n\nPeak Code Value refers to the maximum digital code value that passes through a system component without generating a clip.\n\n# Peak Display Luminance peak display brightness\n\nThe highest brightness a display can produce.\n\n# Perceptual Quantizer sensory quantization curve\n\nAbbreviation for Perceptual Quantizer, which is an EOTF.\n\nMovieLabs proposed a mathematical curve based on Barton Curve that can be used for high dynamic range (HDR). In 2014, SMPTE established the ST2084 standard.\n\nPerceptual Quantization is an efficient way to encode HDR brightness information. Throughout the dynamic range, the difference between each pair of adjacent code values is slightly smaller than the perceivable difference, making the code value utilization rate extremely high.\n\nHowever, this EOTF is not compatible with legacy displays, and PQ-encoded signals can only be decoded on newer, HDR-capable devices.\n\nPQ is designed for 10bit or 12bit content, and according to SMPTE ST2084 standard, it is not recommended to use it for live broadcast.\n\n# Quantum Dot (QD) Displays\n\nQuantum dot displays use nanoscale (2nm-10nm) crystalline "dots". Each dot emits a different solid color, and the color they emit depends on its size.\n\nBy adding a film with quantum dots in front of the LCD backlight, the color reproduction and overall brightness of the image can be significantly improved.\n\nThese tiny nanocrystals can change the spectrum of the backlight before it reaches the red, green, and blue sub-pixels, achieving a color gamut improvement of up to 20-30%, making the color of the display device closer to the Rec.2020 target color gamut.\n\n# ST2084\n\nEOTF defined in the SMPTE standard for mastering HDR content. Also known as PQ, this EOTF is primarily used for mastering non-broadcast content.\n\n# ST2086\n\nMetadata (primary color coordinates, white point, luminance range) defined in the SMPTE standard to describe the absolute color space of the display used for video mastering.\n\n# Tone Mapping/Tone Mapping Operator (TMO) tone mapping/tone mapping operator\n\nTone mapping is a technique used in image processing and computer graphics that is used to map one set of colors to another to achieve an approximation of a High Dynamic Range (HDR) image in a medium with a more limited dynamic range perception. Printouts, CRT or standard dynamic range (SDR) monitors, and projectors all have low dynamic range, not enough to reproduce the full light intensity present in an HDR image. Tone mapping resolves the dramatic drop in contrast from the recording range to the displayable range, while preserving image detail and color representation, which is important for enjoying original scene content and preserving creative intent. This tone mapping process is generally performed using a Tone Mapping Operator, usually an "S" shaped curve to achieve a soft transition of highlight and shadow detail. See Inverse Tone Mapping (ITM).\n\n# Wide Color Gamut (WCG) wide color gamut\n\nAbbreviation for Wide Color Gamut, which means wide color gamut. The wide color gamut includes more saturated colors than Recommendation ITU-R BT.709. For example, the color space defined by Rec.2020 belongs to the wide color gamut.\n\n# White Point White Point\n\nThe white point (often referred to in technical documentation as reference white or target white) is a set of chromaticity coordinates used to define the "white" color in image acquisition, encoding or reproduction. Depending on the application, different definitions of "white" are required to provide acceptable results. For example, a photo taken indoors might be lit with incandescent lighting, which is relatively more orange than daylight. Therefore, most professional cameras have different settings for shooting under incandescent lighting versus daylight. Likewise, images produced for a D65 white point monitor will not display correctly on a monitor with a different white point.\n\nCIE Definition D65 is often used to define the white point of video displays.\n\nD55 was once the standard white point for film projection, and both the DCI white point and the D60 white point are widely used by digital cinema.',normalizedContent:'warning\n\nthis entry needs to be supplemented\n\nplease refer to https://wangwei1237.github.io/2021/01/26/hdr-introduction/\n\ntip\n\nsvfi is able to automatically recognize if the input video has hdr and handle it properly. to specify the hdr format, please change source hdr to a type other than auto, for example, to specify metadata by yourself, select custom hdr\n\n\n# introduction to hdr technology\n\nthe brightness range in the real world is very wide, and the brightness range that the human eye can perceive is about 100,000 nits. for example, using a spectrocolorimeter to measure a flower blooming towards the sun, the brightness of the yellow area can reach up to 14700 nits, the red edge is 2300 nits, and the central stamen and green leaves are only below 200 nits. however, under an sdr display with a narrow color gamut, brightness generally not exceeding 100 nits, and a contrast ratio of only 1000:1, the color of this photo will be much dimmer. but with the development of technology, hdr technology can achieve wide color gamut, 1000 nit brightness and tens of thousands of contrast. although there is still a big difference from the actual standard, compared with the sdr of 30 years ago, hdr is still a big step forward.\n\n\n\n\n# introduction to hdr\n\nhdr is an abbreviation for high dynamic range. hdr is a digital image technology that can be used to enhance the color and contrast range of digital images.\n\ncompared with sdr (standard dynamic range: standard dynamic range), hdr has the following characteristics:\n\n * wider color range\n * brighter brightness cap\n * darker brightness lower limit\n * the overall image quality has been improved in terms of contrast, grayscale resolution and other dimensions\n\nthus, hdr can bring users a more immersive experience.\n\nhdr technology can be applied to photography (photography) and video (video), but it should be noted that although the goals of photo hdr and video hdr are to make the digital content we see more similar to the real experience of human eyes, but these two are indeed two completely different concepts. essentially, photo hdr is a process in which a camera captures photos (capture), while video hdr is a process in which display is displayed.\n\n\n# photometric concepts and units\n\nbefore further introducing hdr, we need to understand some related concepts of photometry so that we can have a better understanding of hdr understanding and awareness.\n\n * radiation flux: in optical radiometry, the basic physical quantity used is radiation flux or radiation power, the symbol is expressed as: φe\\phi_{e}φe , the unit is watt (www). radiant flux is emitted by a radiation source, transmitted through some propagation medium and received on some surface.\n\n * luminous flux: luminous flux refers to the derived amount of radiant flux evaluated according to the international standard human visual characteristics, that is, the photometric measure of radiant flux, and its symbol is: φv\\phi _{v}φv . the luminous flux can be derived from the radiant flux.\n\n * light amount: light amount is also called light energy, which refers to the time integral of luminous flux within a given time interval (δt\\delta tδt), and the symbol is expressed as: qvq_ {v}qv , and qv=∫δtφv⋅dtq_{v}=\\int_{\\delta t}{\\phi_{v} \\ \\cdot dt}qv =∫δt φv ⋅dt.\n\n * luminous intensity: luminous intensity (ivi_{v}iv ) is the luminous flux dφvd \\phi_ emitted by the light source and propagated within the solid angle element dωd \\omegadω in the specified direction the quotient of {v}dφv and the solid angle element dωd \\omegadω, namely: iv=dφvdωi_{v}=\\frac{d \\phi_{v}\\ }{d \\omega}iv =dωdφv . the unit of luminous intensity is candela (cdcdcd). candela is the luminous intensity in a specified direction of a light source emitting monochromatic radiation with a frequency of 540×1012hz540 \\times 10^{12} hz540×1012hz, and the radiant intensity of the light source in this direction is 1683w⋅ sr\\frac{1}{683} w \\cdot sr6831 w⋅sr.\n\n * luminance: luminance (ivi_{v}iv ) refers to the luminous intensity of a surface light source per unit projected area in a certain direction along that direction. the area of the small surface element on the surface light source is dadada, the angle between a certain direction and the surface element normal is θ\\thetaθ, and the projected area of the surface element along this direction is da⋅cosθda \\cdot cos \\thetada⋅cosθ, then the luminance of the surface element along this direction is: ivda⋅cosθ\\frac{i_{v}}{da \\cdot cos \\theta}da⋅cosθiv , namely iv\\ =dφvda⋅cosθ⋅dωi_{v}=\\frac{d \\phi_{v}}{da \\cdot cos \\theta \\cdot d \\omega\\ }iv =da⋅cosθ⋅dωdφv . it can be seen from this that the unit of brightness is cd/m2cd/m^{2}cd/m2 (candela/square meter), also known as nit. for example, the brightness of the screen of the iphone 12 is: 625 nits maximum brightness (typical), 1200 nits maximum brightness (hdr).\n\n\n# photography hdr\n\nhdr photos can be generated by simultaneously capturing multiple images at different exposures. the camera quickly takes three or more photos with different exposure values, which are then processed to produce a single hdr photo.\n\nhdr image processing is very useful when taking photos of high-contrast scenes.\n\nas shown in the night scene below, for example, when the exposure is too low, it can make the scenery behind appear dark. if the exposure is adjusted to show what\'s behind, what\'s in front will again be too bright, and may even be completely white. by combining multiple exposures, these different exposures can be averaged so that the scenery behind can appear in the photo without overexposing the scenery in front. [^2]\n\n\n\n\n\ncurrently, the cameras of many smartphones, such as the iphone, have an hdr option. we can choose to have the camera:\n\n * turn on hdr (on)\n * turn off hdr (off)\n * automatically determine whether to enable hdr (auto)\n\nwith auto mode, the camera only takes hdr photos in high-contrast situations. most cameras will also capture normal photos while compositing hdr photos, allowing us to choose between hdr photos and normal photos.\n\n\n# dodging and burning and tone mapping\n\nin fact, in the film era, photographers would take multiple negatives with different exposure levels, and in the [darkroom](https://baike.baidu.com/item/%e6%9a%97%e6%88%bf /62291) uses a method of **dodging and burning** to combine several negatives into one with high dynamic range photo. with the emergence of digital cameras, hdr introduces the technology in the darkroom into the camera imaging sensor. by taking multiple different exposures of the same scene and then synthesizing them, it can finally enhance the dynamic range of the photo and achieve the purpose of increasing the layering of the photo. . in photo hdr, this compositing technique is called tone mapping (tone mapping). of course, the tone mapping technology will also be used in the next video hdr, the difference is that the tone mapping in video hdr is used to map the hdr content to the sdr display device, so that the sdr device can also display hdr content.\n\n\n# video hdr\n\nhdr video is captured in a similar way to hdr photos. record each shot simultaneously with a different exposure (or a different iso configuration). these shots are then fused together to produce a single shot. this method often results in brighter video, although it produces an unreal-looking gray effect. because of this, hdr videos are often post-processed before release.\n\nin order to display hdr video correctly, the display device must also support hdr. for example, an hdr-capable tv must support a specific video output standard. these standards will involve: 10-bit color and dci-p3 color space covering at least 90%. hdr monitors must also support a specific hdr format, such as hdr10, dolby vision, or hybrid log gamma (hlg).\n\n\n\ncomparison of various color spaces in cie 1931 xy\n\ntake hdr10 as an example, because the hdr10 standard uses rec.2020 wide color gamut, 10-bit color depth and smtpe st 2084 pq conversion function, so when displaying hdr10 content on a non-hdr display device, it will appear two cases [3],[4]:\n\n * unable to decode resulting in blurred screen\n * it can be decoded, but because the highlighted part of the video will be displayed according to the highest brightness of the display device, the overall video will be grayed out\n\nthis is the difference between hdr photos and hdr videos: in addition to video capture, hdr videos have a strong dependence on the capabilities of display devices.\n\n# pq & hlg\n\nin order to display hdr images correctly, it is not enough to simply increase the brightness, it is crucial to display the color and tone in a way that matches human vision. color and tone are affected by the gamma input-output characteristic. in hdr, there are two gamma curves, pq and hlg.\n\n * the pq gamma curve is based on the characteristics of human visual perception, and is most suitable for producing movies or streaming video content on the internet, where reproduction accuracy is key.\n * the hlg gamma curve is designed to allow display on existing sdr tvs without loss of position and is best suited for broadcast tv and live video feeds.\n\nthe specific differences between pq and hlg are shown in the following table:\n\n                                       pq (perceptual quantization)                                   hlg (hybrid log gamma)\ngoals                                  internet video streaming, movies                               broadcast tv, live video\nadvantages                             handles brightness up to 10,000 cd/m² in absolute value, new   handles brightness as relative value (same as existing\n                                       gamma curve based on human visual perception                   standard) compatible with sdr tvs\nmaximum luminance                      1,000 cd/m² absolute same, regardless of display               relative, varies by display\nblack level                            0.005 cd/m2 or less                                            0.005 cd/m2 or less\nproposer                               dolby                                                          bbc and nhk\nreference standards                    smpte st 2084, itu-r bt.2100                                   itu-r bt.2100\nthe effect on the sdr display device   poor                                                           good\nlive effect                            good                                                           excellent\n\n\n# hdr standard\n\n# hdr10\n\n# dolby vision\n\n# hdr10+\n\n# uhd premium\n\n# displayhdr\n\n# mobile hdr\n\n# cuva hdr\n\n# vesa hdr\n\n\n# mobile phone support for hdr\n\n# iphone\n\niphone 12 pro\'s all-round support for dolby vision can be said to rely on its own power to bring the most advanced and avant-garde hdr technology to the eyes of the general public. as the first mobile phone to support hdr shooting, how does iphone 12\'s dolby vision hdr shooting come true? is it really as good as advertised?\n\nperhaps we can learn from this expert samuel bilodeau, who has 5 years of hdr color correction and teaching experience.find out in this review we wrote. the author said at the end of the review: i am particularly excited and happy to see the emergence of iphone and other consumer cameras with hdr shooting capabilities, and push hdr a step forward.\n\n\n# ffmpeg analyzes hdr content\n\n# ffprobe analyzes the metadata of hdr content\n\nmetadata for mastering display and content light level can be extracted using the ffprobe command. we only need to extract the relevant data of the first frame, so when analyzing, we can use -read_intervals "%+#1" option to have ffprobe only extract the metadata of the first frame. the specific analysis of fame and fortune is as follows:\n\nffprobe \\\n-hide_banner\\\n-loglevel warning \\\n-select_streams v \\\n-print_format json \\\n-show_frames \\\n-read_intervals "%+#1" \\\n-show_entries "frame=color_space,color_primaries,color_transfer,side_data_list,pix_fmt" \\\nhdr.mp4\n\n\nthe meaning of each option is as follows:\n\n * -hide_banner -loglevel warning: do not display information we do not need\n * -select_streams v: only select video streams to analyze\n * -print_format json: output analysis results in json format\n * -read_intervals "%+#1": only analyze the data in the first frame\n * -show_entries … : only output the data we specified\n\nafter the command is executed, the following analysis results will be displayed:\n\n{\n     "frames": [\n         {\n             "pix_fmt": "yuv420p10le",\n             "color_space": "bt2020nc",\n             "color_primaries": "bt2020",\n             "color_transfer": "smpte2084",\n             "side_data_list": [\n                 {\n                     "side_data_type": "mastering display metadata",\n                     "red_x": "34000/50000",\n                     "red_y": "16000/50000",\n                     "green_x": "13250/50000",\n                     "green_y": "34500/50000",\n                     "blue_x": "7500/50000",\n                     "blue_y": "3000/50000",\n                     "white_point_x": "15635/50000",\n                     "white_point_y": "16450/50000",\n                     "min_luminance": "40/10000",\n                     "max_luminance": "11000000/10000"\n                 }\n             ]\n         }\n     ]\n}\n\n\nas shown above, the color value and the maximum/minimum brightness value in the ffprobe analysis result are both a ratio. the final result of the color value will determine the color space used by the master content, and the maximum/minimum brightness value will determine the master content. dynamic range.\n\naccording to the above results, we can know that on the cie coordinates, the coordinates of the red, green, blue and white points are:\n\n * red: (0.68, 0.32)\n * green: (0.265, 0.69)\n * blue: (0.15, 0.06)\n * white point: (0.3127, 0.329)\n\nfrom the relevant information of dci-p3, it can be known that the master content of this video was produced in p3-d65 (display) color space, with a minimum brightness of 0.004 nits, a maximum brightness of 1100 nits, and a contrast ratio of 275,000.\n\n# ffmpeg transcoding hdr content\n\nfor the transcoding of hdr video, the content of mastering display and content light level metadata needs to be passed to the encoder (note, not to ffmpeg), otherwise, these information will be lost during the transcoding process, this in turn leads to the impact of transcoding on the content of the screen. the specific way is as follows:\n\nffmpeg -i hdr.mp4 \\\n-map 0 -c:v libx265\\\n-x265-params hdr-opt=1:repeat-headers=1:colorprim=bt2020:transfer=smpte2084:colormatrix=bt2020nc:master-display=g(13250,34500)b(7500,3000)r(34000,16000) wp(15635,16450)l(11000000,40)\\\n-crf 20 \\\n-preset veryfast \\\n-pix_fmt yuv420p10le\\\ntest.mp4\n\n\nin,\n\n * hdr-opt=1: means we want to enable hdr\n * repeat-headers=1: indicates that each frame needs these data\n * colorprim, transfer and colormatrix: consistent with ffprobe\n * master-display: the color string constructed according to the result of ffprobe\n\n\n# related terms\n\n# color calibration of screens (screen color correction)\n\na process used to ensure that colors are accurately displayed on the screen. a colorimeter is generally used to measure the native color response of a display, then calculate an index for correction to ensure that the color is displayed correctly on the display, and finally test the corrected response.\n\n# color spaces\n\na color space can refer to an organization of colors, or it can refer to a specific range of colors. in the field of cinema and television, we generally use rgb (using red, green, and blue primary color components to represent a color) or yuv (using the brightness (black and white value) of a color, and its color value calculated based on the difference between color components. degrees) color space (translator\'s note: rgb and yuv are color models, not color spaces, there is an error in the original text here). these color spaces are generally based on specific display device characteristics, see d65-p3 entry. other color spaces, such as xyz or lab, are more suitable for representing the human eye color vision model.\n\n# contrast ratio\n\ncontrast is the ratio of lightness between the lightest (white) part and the darkest (black) part that a system can produce, usually described by an n:1 ratio.\n\n# cri color remapping information (color remapping information)\n\na standard set of metadata produced by analyzing two different masters of the same content, such as an hdr and an sdr master. when a master (such as hdr) and cri metadata are transmitted together, the decoder can only decode hdr content when facing an hdr screen, and can also convert hdr content when facing an sdr screen by using cri metadata. the content is transformed into sdr content. the main advantage of using this approach is that the author\'s creative intent is preserved for both decoded versions. cri is a standard component of mpeg (hevc v2) and is an optional feature for ultra hd blu-ray production.\n\n# dci-p3, d65-p3, st 428-1\n\na digital cinema color space. the dci-p3 color space is an rgb color space introduced by the digital cinema initiatives in 2005 and standardized by smpte st428-1 in 2006.\n\nthis color space has a much wider gamut than srgb (see rec.709).\n\nall digital cinema projectors can fully display the dci-p3 color space, d65 p3 refers to the color temperature of its white point changed from dci white point to d65 white point.\n\nthe three triangles in the diagram represent: the largest is the rec.2020 standard, the new standard for ultra hd tv (currently only fully implemented by laser displays), the slightly smaller dci-p3 for digital cinema, and traditional video surveillance the smallest rec.709 color space used by tv receivers, hd broadcast tv, blu-ray, ott.\n\n# edid extended display identification data\n\nedid is an acronym for extended display information data, a standard developed by the consumer technology association (cta). this is provided by every dvi monitor, hdmi monitor, or other device that supports a dvi hdmi input (aka dvi/hdmi sinks). probably each dvi/hdmi input has its own edid. edid tells the device the performance characteristics of the display it is connected to.\n\nthe source device recognizes the presence of edid memory on the display\'s dvi or hdmi interface and uses the information in it to optimize the output video (resolution, frame rate, color...) and/or audio format. all devices that support dvi/hdmi standard input, that is, tmds sinks (dvi/hdmi sinks) must implement edid.\n\n# eotf electro-optic transfer function\n\neotf is an acronym for electro-optical transfer function. it is a mathematical function that maps code values to display brightness. in other words, an etof defines how code values in an image are displayed as visible light by a display or projector.\n\nsee oetf photoelectric transfer function, st2084.\n\n# oetf photoelectric transfer function\n\noetf is an acronym for opto-electronic transfer function. it is a mathematical function that maps scene luminosity (the light in a scene) to digitally encoded values that can be transmitted and compressed. this term is generally applied to devices that acquire images, such as digital cameras.\n\nin post-production, content is typically graded on a screen with a specific eotf, and historically, an inverse of the oetf was often used as the screen\'s eotf.\n\n# ootf optical-to-optical transfer function\n\nootf is an acronym for optical-to-optical transfer function. it is a mathematical function that maps the brightness of the scene captured by the camera to the brightness of the display.\n\n# flicker flicker\n\ncharacteristic of certain kinds of monitors, such as old cathode ray tube monitors (crts), or poorly tuned flat panel monitors, or even motion picture film projectors. this unwelcome change in brightness occurs mainly at frequencies below 50 visible in frames per second. for brighter displays, the human eye can perceive higher frequency flicker.\n\n# f-stop of dynamic range the number of f-stops of the dynamic range\n\nin photography, a change of one f-stop corresponds to doubling (or halving) the light captured when capturing an image.\n\nthe number of f-stops contained in an image describes the contrast of the image (2^n notation). for example, a camera can output an image of 10 stops, which means that the contrast ratio can be as high as 2^10 (1024:1 ), that is, the white part will be 1024 times brighter than the black part. in comparison, the human eye can go up to 18-20 stops (this is a very high dynamic range hdr, a typical sdr video image is 6-7 stops)\n\n# gamut or color gamut\n\nin color reproduction, including but not limited to computer graphics and photography, a color gamut is a specific subset of colors.\n\ncolor gamutthe most common use of the term is to refer to a subset of colors that can be accurately reflected within a certain range. this premise may be a specific display device or a given color space. the color gamut is generally represented by the area on the cie 1931 chromaticity diagram, and the edges of the cie 1931 curve represent the range of colors in the visible light spectrum.\n\n# gamut mapping gamut mapping\n\nin almost all translation processes (referring to the representation of a specific color, the conversion process in different color spaces), we have to face the reality that the range covered by the color gamut of different devices is different, which means making accurate color reproduction impossible.\n\ntherefore, it is necessary to perform some processing on the parts near the edge of the color gamut. some colors must be shifted into gamut, otherwise they cannot be displayed on the output device, and they will be roughly discarded (clipped). this is known as a gamut mismatch, for example when we convert from the wider rgb color space to the cmyk color space.\n\na color management system can use a variety of methods to achieve the desired result and give the experienced user the means to control gamut mapping.\n\n# imf interactive master file\n\nthe interoperable master format (imf) is a standard provided by smpte to implement a single-file, interchangeable master file format and structure for commercial content distribution worldwide. it evolved from the digital cinema package (dcp) architecture, which provides a complete exchangeable unit of files for distribution channels. imf can be said to be a revolutionary progress, imf provides a real file-based final master. dcp is aimed at theatrical content distribution, while imf provides a master format for commercial environments that can create multiple cut versions of the same content for different audiences.\n\n# inverse tone mapping (itm) inverse tone mapping / reverse tone mapping\n\nremastering of sdr content for hdr content.\n\nitm takes sdr content and expands its luminosity and color space to match the capabilities of hdr displays while preserving the original content creator\'s intent.\n\n# lut lookup table\n\nit is an acronym for look up table. luts provide an efficient way of applying complex mathematical operations to incoming data—which would be computationally expensive without luts. as such, they are an ideal implementation for mapping an image from one color space to zero one color space.\n\nthe following luts exist:\n\nin 3d luts, the output r\', g\', b\' value of each pixel is jointly calculated from the r, g, b value of the input pixel.\n\nfor 1d luts, the output r’ is only calculated from the input r value, and the same is true for g’ and b’. they are commonly used to apply corrections for gamma functions and other eotfs, and are typically found in chipsets in consumer electronics devices.\n\n3d luts support more powerful mathematical transformations than 1d luts, but are more complex and expensive to implement in chipsets. typically 3d luts are used to apply a creative "look" and color space conversion during post-production.\n\n# maxcll metadata maxcll metadata\n\nmaximum content light level (maxcll) is an integer-type metadata value used to define the highest brightness of any pixel in an encoded hdr video stream or file, in units of nits. maxcll can be measured during the mastering process or after the production process, but in order to ensure the color transition of hdr monitors in the maxcll hdr range, and to apply a hard clip outside the maximum brightness of the monitor, the maxcll of the monitor can also be used as the maxcll element data.\n\n# maxfall metadata maxfall metadata\n\nmaximum frame average light level (maxfall) is an integer-type metadata value used to define the highest average brightness of any frame in an encoded hdr video stream or file, in units of nits. maxfall is obtained by calculating the average brightness of all pixels decoded in each frame.\n\n# peak code value peak code value\n\npeak code value refers to the maximum digital code value that passes through a system component without generating a clip.\n\n# peak display luminance peak display brightness\n\nthe highest brightness a display can produce.\n\n# perceptual quantizer sensory quantization curve\n\nabbreviation for perceptual quantizer, which is an eotf.\n\nmovielabs proposed a mathematical curve based on barton curve that can be used for high dynamic range (hdr). in 2014, smpte established the st2084 standard.\n\nperceptual quantization is an efficient way to encode hdr brightness information. throughout the dynamic range, the difference between each pair of adjacent code values is slightly smaller than the perceivable difference, making the code value utilization rate extremely high.\n\nhowever, this eotf is not compatible with legacy displays, and pq-encoded signals can only be decoded on newer, hdr-capable devices.\n\npq is designed for 10bit or 12bit content, and according to smpte st2084 standard, it is not recommended to use it for live broadcast.\n\n# quantum dot (qd) displays\n\nquantum dot displays use nanoscale (2nm-10nm) crystalline "dots". each dot emits a different solid color, and the color they emit depends on its size.\n\nby adding a film with quantum dots in front of the lcd backlight, the color reproduction and overall brightness of the image can be significantly improved.\n\nthese tiny nanocrystals can change the spectrum of the backlight before it reaches the red, green, and blue sub-pixels, achieving a color gamut improvement of up to 20-30%, making the color of the display device closer to the rec.2020 target color gamut.\n\n# st2084\n\neotf defined in the smpte standard for mastering hdr content. also known as pq, this eotf is primarily used for mastering non-broadcast content.\n\n# st2086\n\nmetadata (primary color coordinates, white point, luminance range) defined in the smpte standard to describe the absolute color space of the display used for video mastering.\n\n# tone mapping/tone mapping operator (tmo) tone mapping/tone mapping operator\n\ntone mapping is a technique used in image processing and computer graphics that is used to map one set of colors to another to achieve an approximation of a high dynamic range (hdr) image in a medium with a more limited dynamic range perception. printouts, crt or standard dynamic range (sdr) monitors, and projectors all have low dynamic range, not enough to reproduce the full light intensity present in an hdr image. tone mapping resolves the dramatic drop in contrast from the recording range to the displayable range, while preserving image detail and color representation, which is important for enjoying original scene content and preserving creative intent. this tone mapping process is generally performed using a tone mapping operator, usually an "s" shaped curve to achieve a soft transition of highlight and shadow detail. see inverse tone mapping (itm).\n\n# wide color gamut (wcg) wide color gamut\n\nabbreviation for wide color gamut, which means wide color gamut. the wide color gamut includes more saturated colors than recommendation itu-r bt.709. for example, the color space defined by rec.2020 belongs to the wide color gamut.\n\n# white point white point\n\nthe white point (often referred to in technical documentation as reference white or target white) is a set of chromaticity coordinates used to define the "white" color in image acquisition, encoding or reproduction. depending on the application, different definitions of "white" are required to provide acceptable results. for example, a photo taken indoors might be lit with incandescent lighting, which is relatively more orange than daylight. therefore, most professional cameras have different settings for shooting under incandescent lighting versus daylight. likewise, images produced for a d65 white point monitor will not display correctly on a monitor with a different white point.\n\ncie definition d65 is often used to define the white point of video displays.\n\nd55 was once the standard white point for film projection, and both the dci white point and the d60 white point are widely used by digital cinema.',charsets:{}},{title:"Quick Start",frontmatter:{title:"Quick Start",date:"2023-04-22T14:09:00.000Z",permalink:"/pages/f8b952/"},regularPath:"/20.Manuals/10.Quick%20Start.html",relativePath:"20.Manuals/10.Quick Start.md",key:"v-d9328e42",path:"/pages/f8b952/",headers:[{level:2,title:"Important notes on using the software",slug:"important-notes-on-using-the-software",normalizedTitle:"important notes on using the software",charIndex:150},{level:2,title:"Common Problem Solving",slug:"common-problem-solving",normalizedTitle:"common problem solving",charIndex:1787},{level:2,title:"Advanced Options Description",slug:"advanced-options-description",normalizedTitle:"advanced options description",charIndex:1889},{level:2,title:"Description of command line options",slug:"description-of-command-line-options",normalizedTitle:"description of command line options",charIndex:1997}],headersStr:"Important notes on using the software Common Problem Solving Advanced Options Description Description of command line options",content:'If you have not used the software, please read the Newbie Tutorial\n\nBefore you officially use the software, please be sure to read the following:\n\n\n# Important notes on using the software\n\n * If you need to use all the functions of the software, with the graphics card is NVIDIA 20 series and above, please install the 30 series optimization dlc\n\nTip\n\nUsers of AMD graphics cards do not need to operate\n\nFollow the instructions below\n\n\n\n * Be sure to set and confirm the output folder before starting the task\n\n\n\n * Please note that the output video file format should be as consistent as possible with the video input file format\n   \n   Example below\n\n\n\nTip\n\nIf there is no corresponding format, it is recommended that you choose the mkv format\n\n * The output folder should be set in a drive letter with a large remaining space as much as possible, and try not to set it in a mobile hard disk drive letter to avoid unexpected disk loss\n * The remaining space of the system disk should not be less than 1G\n * Check whether the "Export with Audio" and "Export with Subtitles" options are enabled before starting the task. If checked, the audio and subtitle tracks of the original video will be preserved without loss\n\n\n\nTip\n\nWhen the mouse hovers over the option, the corresponding floating window will pop up. It is recommended to read the description of such options carefully before adjusting the default settings of the software, which will save you a lot of time!\n\n\n\n * For super resolution only, please set the supplementary frame multiplier to 1, click one-click suppression to start the task; click one-click supplementary frame supplementation and super resolution at the same time (consumes more video memory)\n\nTip\n\nSuper resolution feature is only available in Pro version\n\n\n# Common Problem Solving\n\nIf you encounter problems during the use of the software, please read FAQ\n\n\n# Advanced Options Description\n\nTo explore advanced setting options, please read Advanced Setting Details\n\n\n# Description of command line options\n\nTo learn how to use the command line version of SVFI, please read Advanced Explanation of Command Line Programs',normalizedContent:'if you have not used the software, please read the newbie tutorial\n\nbefore you officially use the software, please be sure to read the following:\n\n\n# important notes on using the software\n\n * if you need to use all the functions of the software, with the graphics card is nvidia 20 series and above, please install the 30 series optimization dlc\n\ntip\n\nusers of amd graphics cards do not need to operate\n\nfollow the instructions below\n\n\n\n * be sure to set and confirm the output folder before starting the task\n\n\n\n * please note that the output video file format should be as consistent as possible with the video input file format\n   \n   example below\n\n\n\ntip\n\nif there is no corresponding format, it is recommended that you choose the mkv format\n\n * the output folder should be set in a drive letter with a large remaining space as much as possible, and try not to set it in a mobile hard disk drive letter to avoid unexpected disk loss\n * the remaining space of the system disk should not be less than 1g\n * check whether the "export with audio" and "export with subtitles" options are enabled before starting the task. if checked, the audio and subtitle tracks of the original video will be preserved without loss\n\n\n\ntip\n\nwhen the mouse hovers over the option, the corresponding floating window will pop up. it is recommended to read the description of such options carefully before adjusting the default settings of the software, which will save you a lot of time!\n\n\n\n * for super resolution only, please set the supplementary frame multiplier to 1, click one-click suppression to start the task; click one-click supplementary frame supplementation and super resolution at the same time (consumes more video memory)\n\ntip\n\nsuper resolution feature is only available in pro version\n\n\n# common problem solving\n\nif you encounter problems during the use of the software, please read faq\n\n\n# advanced options description\n\nto explore advanced setting options, please read advanced setting details\n\n\n# description of command line options\n\nto learn how to use the command line version of svfi, please read advanced explanation of command line programs',charsets:{}},{title:"Detailed Explanation of Advanced Settings",frontmatter:{title:"Detailed Explanation of Advanced Settings",date:"2023-05-10T18:58:00.000Z",permalink:"/pages/052617/"},regularPath:"/20.Manuals/20.Option%20Manuals.html",relativePath:"20.Manuals/20.Option Manuals.md",key:"v-b84e0cea",path:"/pages/052617/",headers:[{level:2,title:"Workflow Recovery",slug:"workflow-recovery",normalizedTitle:"workflow recovery",charIndex:125},{level:3,title:"Auto Configure",slug:"auto-configure",normalizedTitle:"auto configure",charIndex:149},{level:3,title:"Start point and End point",slug:"start-point-and-end-point",normalizedTitle:"start point and end point",charIndex:576},{level:3,title:"Start block count and start input frame number",slug:"start-block-count-and-start-input-frame-number",normalizedTitle:"start block count and start input frame number",charIndex:854},{level:3,title:"Return to origin",slug:"return-to-origin",normalizedTitle:"return to origin",charIndex:1458},{level:3,title:"Risk Mode",slug:"risk-mode",normalizedTitle:"risk mode",charIndex:1612},{level:2,title:"Transition recognition",slug:"transition-recognition",normalizedTitle:"transition recognition",charIndex:1859},{level:3,title:"Enable transition recognition",slug:"enable-transition-recognition",normalizedTitle:"enable transition recognition",charIndex:1888},{level:3,title:"Maximum recognition threshold (no need to adjust by default)",slug:"maximum-recognition-threshold-no-need-to-adjust-by-default",normalizedTitle:"maximum recognition threshold (no need to adjust by default)",charIndex:2524},{level:3,title:"Use fixed transition recognition",slug:"use-fixed-transition-recognition",normalizedTitle:"use fixed transition recognition",charIndex:2759},{level:3,title:"Transitions use frame blending",slug:"transitions-use-frame-blending",normalizedTitle:"transitions use frame blending",charIndex:2990},{level:3,title:"Output transition frame",slug:"output-transition-frame",normalizedTitle:"output transition frame",charIndex:3168},{level:2,title:"Output resolution setting",slug:"output-resolution-setting",normalizedTitle:"output resolution setting",charIndex:3388},{level:3,title:"Output file resolution",slug:"output-file-resolution",normalizedTitle:"output file resolution",charIndex:3420},{level:3,title:"Output black edge length",slug:"output-black-edge-length",normalizedTitle:"output black edge length",charIndex:3667},{level:3,title:"Complete the black border after processing",slug:"complete-the-black-border-after-processing",normalizedTitle:"complete the black border after processing",charIndex:4422},{level:2,title:"Use AI Super Resolution",slug:"use-ai-super-resolution",normalizedTitle:"use ai super resolution",charIndex:4781},{level:2,title:"Super Resolution Model",slug:"super-resolution-model",normalizedTitle:"super resolution model",charIndex:5764},{level:3,title:"realCUGAN",slug:"realcugan",normalizedTitle:"realcugan",charIndex:5015},{level:3,title:"ncnnCUGAN",slug:"ncnncugan",normalizedTitle:"ncnncugan",charIndex:6270},{level:3,title:"waifuCuda",slug:"waifucuda",normalizedTitle:"waifucuda",charIndex:5044},{level:3,title:"realESR",slug:"realesr",normalizedTitle:"realesr",charIndex:5026},{level:3,title:"ncnnRealESR",slug:"ncnnrealesr",normalizedTitle:"ncnnrealesr",charIndex:7041},{level:3,title:"AnimeSR (anime super resolution algorithm developed by Tencent ARC Lab)",slug:"animesr-anime-super-resolution-algorithm-developed-by-tencent-arc-lab",normalizedTitle:"animesr (anime super resolution algorithm developed by tencent arc lab)",charIndex:7339},{level:3,title:"NvidiaSR (ultra-high-speed super resolution algorithm developed by NVIDIA)",slug:"nvidiasr-ultra-high-speed-super-resolution-algorithm-developed-by-nvidia",normalizedTitle:"nvidiasr (ultra-high-speed super resolution algorithm developed by nvidia)",charIndex:7519},{level:3,title:"BasicVSRPlusPlus (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)",slug:"basicvsrplusplus-real-shot-super-resolution-algorithm-the-effect-depends-on-the-length-of-the-super-resolution-sequence",normalizedTitle:"basicvsrplusplus (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)",charIndex:8138},{level:3,title:"BasicVSRPlusPlusRestore (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)",slug:"basicvsrplusplusrestore-real-shot-super-resolution-algorithm-the-effect-depends-on-the-length-of-the-super-resolution-sequence",normalizedTitle:"basicvsrplusplusrestore (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)",charIndex:8668},{level:3,title:"PureBasicVSR (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)",slug:"purebasicvsr-real-shot-super-resolution-algorithm-the-effect-depends-on-the-length-of-the-super-resolution-sequence",normalizedTitle:"purebasicvsr (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)",charIndex:9548},{level:3,title:"RealBasicVSR (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)",slug:"realbasicvsr-real-shot-super-resolution-algorithm-the-effect-depends-on-the-length-of-the-super-resolution-sequence",normalizedTitle:"realbasicvsr (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)",charIndex:9883},{level:3,title:"FTVSR (real-time super resolution algorithm, the effect depends on the length of the super resolution sequence)",slug:"ftvsr-real-time-super-resolution-algorithm-the-effect-depends-on-the-length-of-the-super-resolution-sequence",normalizedTitle:"ftvsr (real-time super resolution algorithm, the effect depends on the length of the super resolution sequence)",charIndex:10079},{level:3,title:"Anime4K (ultra-high-speed real-time animation super resolution algorithm, more conservative)",slug:"anime4k-ultra-high-speed-real-time-animation-super-resolution-algorithm-more-conservative",normalizedTitle:"anime4k (ultra-high-speed real-time animation super resolution algorithm, more conservative)",charIndex:10410},{level:3,title:"waifu2x (classic conservative super resolution algorithm)",slug:"waifu2x-classic-conservative-super-resolution-algorithm",normalizedTitle:"waifu2x (classic conservative super resolution algorithm)",charIndex:10710},{level:3,title:"TensorRT (ultra-fast acceleration for the above part of the super resolution algorithm)",slug:"tensorrt-ultra-fast-acceleration-for-the-above-part-of-the-super-resolution-algorithm",normalizedTitle:"tensorrt (ultra-fast acceleration for the above part of the super resolution algorithm)",charIndex:10918},{level:3,title:"load graphics card",slug:"load-graphics-card",normalizedTitle:"load graphics card",charIndex:11386},{level:3,title:"super resolution Algorithm",slug:"super-resolution-algorithm",normalizedTitle:"super resolution algorithm",charIndex:11466},{level:3,title:"Super resolution model multiple",slug:"super-resolution-model-multiple",normalizedTitle:"super resolution model multiple",charIndex:11545},{level:3,title:"Transfer Resolution Ratio",slug:"transfer-resolution-ratio",normalizedTitle:"transfer resolution ratio",charIndex:11642},{level:3,title:"RealCUGAN cutting mode",slug:"realcugan-cutting-mode",normalizedTitle:"realcugan cutting mode",charIndex:11962},{level:3,title:"RealCUGAN low memory mode",slug:"realcugan-low-memory-mode",normalizedTitle:"realcugan low memory mode",charIndex:12329},{level:3,title:"Cutting block size (it is not recommended to open when using realCUGAN)",slug:"cutting-block-size-it-is-not-recommended-to-open-when-using-realcugan",normalizedTitle:"cutting block size (it is not recommended to open when using realcugan)",charIndex:12525},{level:3,title:"Super-score sequence length",slug:"super-score-sequence-length",normalizedTitle:"super-score sequence length",charIndex:12698},{level:3,title:"Super resolution using half precision",slug:"super-resolution-using-half-precision",normalizedTitle:"super resolution using half precision",charIndex:13242},{level:3,title:"TTA",slug:"tta",normalizedTitle:"tta",charIndex:13523},{level:3,title:"FMNet - HDR10",slug:"fmnet-hdr10",normalizedTitle:"fmnet - hdr10",charIndex:13766},{level:2,title:"Output settings (suppression parameter quality)",slug:"output-settings-suppression-parameter-quality",normalizedTitle:"output settings (suppression parameter quality)",charIndex:13894},{level:3,title:"Render Quality CRF",slug:"render-quality-crf",normalizedTitle:"render quality crf",charIndex:14481},{level:3,title:"target bit rate",slug:"target-bit-rate",normalizedTitle:"target bit rate",charIndex:15261},{level:3,title:"Encoder",slug:"encoder",normalizedTitle:"encoder",charIndex:15442},{level:3,title:"Select compression encoding",slug:"select-compression-encoding",normalizedTitle:"select compression encoding",charIndex:17289},{level:3,title:"Select suppress preset",slug:"select-suppress-preset",normalizedTitle:"select suppress preset",charIndex:17941},{level:3,title:"NVIDIA card hard-coded preset",slug:"nvidia-card-hard-coded-preset",normalizedTitle:"nvidia card hard-coded preset",charIndex:18430},{level:3,title:"Default suppression scheme",slug:"default-suppression-scheme",normalizedTitle:"default suppression scheme",charIndex:18720},{level:3,title:"Audio secondary pressure to AAC",slug:"audio-secondary-pressure-to-aac",normalizedTitle:"audio secondary pressure to aac",charIndex:18932},{level:3,title:"HDR Strict Mode",slug:"hdr-strict-mode",normalizedTitle:"hdr strict mode",charIndex:19107},{level:3,title:"DV Compatible with HDR10",slug:"dv-compatible-with-hdr10",normalizedTitle:"dv compatible with hdr10",charIndex:19187},{level:3,title:"One-click HDR: Convert SDR video to HDR10+",slug:"one-click-hdr-convert-sdr-video-to-hdr10",normalizedTitle:"one-click hdr: convert sdr video to hdr10+",charIndex:19305},{level:2,title:"Decoding quality control",slug:"decoding-quality-control",normalizedTitle:"decoding quality control",charIndex:19413},{level:3,title:"Use vspipe pre-decoding",slug:"use-vspipe-pre-decoding",normalizedTitle:"use vspipe pre-decoding",charIndex:19442},{level:3,title:"Hardware decoding",slug:"hardware-decoding",normalizedTitle:"hardware decoding",charIndex:19718},{level:3,title:"Quick frame splitting",slug:"quick-frame-splitting",normalizedTitle:"quick frame splitting",charIndex:19956},{level:3,title:"High precision optimization workflow",slug:"high-precision-optimization-workflow",normalizedTitle:"high precision optimization workflow",charIndex:20099},{level:3,title:"Turn on deinterlacing",slug:"turn-on-deinterlacing",normalizedTitle:"turn on deinterlacing",charIndex:20760},{level:3,title:"DePan (De-ribbon)",slug:"depan-de-ribbon",normalizedTitle:"depan (de-ribbon)",charIndex:20975},{level:3,title:"Fast Noise Reduction",slug:"fast-noise-reduction",normalizedTitle:"fast noise reduction",charIndex:21041},{level:3,title:"Add noise quickly",slug:"add-noise-quickly",normalizedTitle:"add noise quickly",charIndex:21453},{level:3,title:"Custom frame splitting parameters (professional option)",slug:"custom-frame-splitting-parameters-professional-option",normalizedTitle:"custom frame splitting parameters (professional option)",charIndex:21526},{level:2,title:"Custom encoding settings",slug:"custom-encoding-settings",normalizedTitle:"custom encoding settings",charIndex:21700},{level:3,title:"Specify the number of encoding threads",slug:"specify-the-number-of-encoding-threads",normalizedTitle:"specify the number of encoding threads",charIndex:21729},{level:3,title:"Custom suppression parameters",slug:"custom-suppression-parameters",normalizedTitle:"custom suppression parameters",charIndex:21875},{level:3,title:"Time Remapping: Change the speed of the video",slug:"time-remapping-change-the-speed-of-the-video",normalizedTitle:"time remapping: change the speed of the video",charIndex:22230},{level:3,title:"Head and tail loop",slug:"head-and-tail-loop",normalizedTitle:"head and tail loop",charIndex:23069},{level:2,title:"IO control",slug:"io-control",normalizedTitle:"io control",charIndex:23173},{level:3,title:"Manually specify the buffer memory size",slug:"manually-specify-the-buffer-memory-size",normalizedTitle:"manually specify the buffer memory size",charIndex:23190},{level:3,title:"Single output block size",slug:"single-output-block-size",normalizedTitle:"single output block size",charIndex:23386},{level:3,title:"Keep the project folder after the task is completed",slug:"keep-the-project-folder-after-the-task-is-completed",normalizedTitle:"keep the project folder after the task is completed",charIndex:23749},{level:2,title:"VFI setting",slug:"vfi-setting",normalizedTitle:"vfi setting",charIndex:23904},{level:3,title:"Safe Frame Rate",slug:"safe-frame-rate",normalizedTitle:"safe frame rate",charIndex:23920},{level:3,title:"Reverse Optical Flow",slug:"reverse-optical-flow",normalizedTitle:"reverse optical flow",charIndex:24250},{level:3,title:"absolutely smooth",slug:"absolutely-smooth",normalizedTitle:"absolutely smooth",charIndex:24343},{level:3,title:"Optical Flow Scale",slug:"optical-flow-scale",normalizedTitle:"optical flow scale",charIndex:24516},{level:3,title:"Interlaced frame interpolation",slug:"interlaced-frame-interpolation",normalizedTitle:"interlaced frame interpolation",charIndex:24852},{level:3,title:"Video fluency optimization",slug:"video-fluency-optimization",normalizedTitle:"video fluency optimization",charIndex:25109},{level:3,title:"Load Graphics Card",slug:"load-graphics-card-2",normalizedTitle:"load graphics card",charIndex:26726},{level:3,title:"frame interpolation algorithm",slug:"frame-interpolation-algorithm",normalizedTitle:"frame interpolation algorithm",charIndex:26810},{level:3,title:"VFI model",slug:"vfi-model",normalizedTitle:"vfi model",charIndex:26929},{level:3,title:"TTA mode",slug:"tta-mode",normalizedTitle:"tta mode",charIndex:29354},{level:3,title:"output layer fine-tuning mode (only available for experimental models)",slug:"output-layer-fine-tuning-mode-only-available-for-experimental-models",normalizedTitle:"output layer fine-tuning mode (only available for experimental models)",charIndex:29926},{level:3,title:"Bidirectional Optical Flow",slug:"bidirectional-optical-flow",normalizedTitle:"bidirectional optical flow",charIndex:30134},{level:3,title:"Dynamic Optical Flow Scale",slug:"dynamic-optical-flow-scale",normalizedTitle:"dynamic optical flow scale",charIndex:30322},{level:2,title:"Customize preset column",slug:"customize-preset-column",normalizedTitle:"customize preset column",charIndex:30603},{level:3,title:"Create a new preset based on the current settings",slug:"create-a-new-preset-based-on-the-current-settings",normalizedTitle:"create a new preset based on the current settings",charIndex:30631},{level:3,title:"Remove current preset",slug:"remove-current-preset",normalizedTitle:"remove current preset",charIndex:30747},{level:3,title:"Apply specific presets",slug:"apply-specific-presets",normalizedTitle:"apply specific presets",charIndex:30811},{level:2,title:"Toolbox",slug:"toolbox",normalizedTitle:"toolbox",charIndex:30900},{level:3,title:"Convert video to GIF animation",slug:"convert-video-to-gif-animation",normalizedTitle:"convert video to gif animation",charIndex:30912},{level:3,title:"Loop animation",slug:"loop-animation",normalizedTitle:"loop animation",charIndex:30984},{level:3,title:"Merge existing blocks",slug:"merge-existing-blocks",normalizedTitle:"merge existing blocks",charIndex:31074},{level:3,title:"Audio and video merge",slug:"audio-and-video-merge",normalizedTitle:"audio and video merge",charIndex:31304},{level:3,title:"Export the current settings to a text file",slug:"export-the-current-settings-to-a-text-file",normalizedTitle:"export the current settings to a text file",charIndex:31563},{level:3,title:"Debug",slug:"debug",normalizedTitle:"debug",charIndex:31860},{level:2,title:"Preferences",slug:"preferences",normalizedTitle:"preferences",charIndex:31951},{level:3,title:"Multitasking rest interval",slug:"multitasking-rest-interval",normalizedTitle:"multitasking rest interval",charIndex:31967},{level:3,title:"Select cache folder",slug:"select-cache-folder",normalizedTitle:"select cache folder",charIndex:32062},{level:3,title:"After the completion of the supplementary frame task",slug:"after-the-completion-of-the-supplementary-frame-task",normalizedTitle:"after the completion of the supplementary frame task",charIndex:32199},{level:3,title:"Unavailable features",slug:"unavailable-features",normalizedTitle:"unavailable features",charIndex:32321},{level:3,title:"Enable expert mode",slug:"enable-expert-mode",normalizedTitle:"enable expert mode",charIndex:32391},{level:3,title:"Enable parameter text preview before task",slug:"enable-parameter-text-preview-before-task",normalizedTitle:"enable parameter text preview before task",charIndex:32457},{level:3,title:"Clear the task list after the task is completed",slug:"clear-the-task-list-after-the-task-is-completed",normalizedTitle:"clear the task list after the task is completed",charIndex:32715},{level:3,title:"Use global settings",slug:"use-global-settings",normalizedTitle:"use global settings",charIndex:32826},{level:3,title:"Reckless Exit",slug:"reckless-exit",normalizedTitle:"reckless exit",charIndex:32891},{level:3,title:"Original flavor suppression mode",slug:"original-flavor-suppression-mode",normalizedTitle:"original flavor suppression mode",charIndex:33084},{level:3,title:"Open preview",slug:"open-preview",normalizedTitle:"open preview",charIndex:33220},{level:3,title:"Automatic error correction",slug:"automatic-error-correction",normalizedTitle:"automatic error correction",charIndex:33359},{level:3,title:"Enable quiet mode",slug:"enable-quiet-mode",normalizedTitle:"enable quiet mode",charIndex:33444}],headersStr:"Workflow Recovery Auto Configure Start point and End point Start block count and start input frame number Return to origin Risk Mode Transition recognition Enable transition recognition Maximum recognition threshold (no need to adjust by default) Use fixed transition recognition Transitions use frame blending Output transition frame Output resolution setting Output file resolution Output black edge length Complete the black border after processing Use AI Super Resolution Super Resolution Model realCUGAN ncnnCUGAN waifuCuda realESR ncnnRealESR AnimeSR (anime super resolution algorithm developed by Tencent ARC Lab) NvidiaSR (ultra-high-speed super resolution algorithm developed by NVIDIA) BasicVSRPlusPlus (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence) BasicVSRPlusPlusRestore (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence) PureBasicVSR (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence) RealBasicVSR (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence) FTVSR (real-time super resolution algorithm, the effect depends on the length of the super resolution sequence) Anime4K (ultra-high-speed real-time animation super resolution algorithm, more conservative) waifu2x (classic conservative super resolution algorithm) TensorRT (ultra-fast acceleration for the above part of the super resolution algorithm) load graphics card super resolution Algorithm Super resolution model multiple Transfer Resolution Ratio RealCUGAN cutting mode RealCUGAN low memory mode Cutting block size (it is not recommended to open when using realCUGAN) Super-score sequence length Super resolution using half precision TTA FMNet - HDR10 Output settings (suppression parameter quality) Render Quality CRF target bit rate Encoder Select compression encoding Select suppress preset NVIDIA card hard-coded preset Default suppression scheme Audio secondary pressure to AAC HDR Strict Mode DV Compatible with HDR10 One-click HDR: Convert SDR video to HDR10+ Decoding quality control Use vspipe pre-decoding Hardware decoding Quick frame splitting High precision optimization workflow Turn on deinterlacing DePan (De-ribbon) Fast Noise Reduction Add noise quickly Custom frame splitting parameters (professional option) Custom encoding settings Specify the number of encoding threads Custom suppression parameters Time Remapping: Change the speed of the video Head and tail loop IO control Manually specify the buffer memory size Single output block size Keep the project folder after the task is completed VFI setting Safe Frame Rate Reverse Optical Flow absolutely smooth Optical Flow Scale Interlaced frame interpolation Video fluency optimization Load Graphics Card frame interpolation algorithm VFI model TTA mode output layer fine-tuning mode (only available for experimental models) Bidirectional Optical Flow Dynamic Optical Flow Scale Customize preset column Create a new preset based on the current settings Remove current preset Apply specific presets Toolbox Convert video to GIF animation Loop animation Merge existing blocks Audio and video merge Export the current settings to a text file Debug Preferences Multitasking rest interval Select cache folder After the completion of the supplementary frame task Unavailable features Enable expert mode Enable parameter text preview before task Clear the task list after the task is completed Use global settings Reckless Exit Original flavor suppression mode Open preview Automatic error correction Enable quiet mode",content:'The following content will introduce you to the advanced settings section of the software\n\n\n# Software advanced settings\n\n\n# Workflow Recovery\n\n\n\n\n# Auto Configure\n\nTip\n\nWhen the program exits due to a power outage in the middle of the task or the termination of the task due to other circumstances, you can click "Auto Configure" to restore the last block position.\n\nBefore clicking this button, please click on the task entry to restore the progress. Then click "Interpolate", the software will pop up a window to confirm the starting position of supplementary frame.\n\n\n\n\n# Start point and End point\n\nYou can choose the time period that needs to be supplemented\n\n\n\n> Input format: hours:minutes:seconds\n\nWarning\n\nAfter specifying the start and end complement frame time, progress recovery is not supported after manual termination or power failure\n\n\n# Start block count and start input frame number\n\nIt is used when the automatic search progress fails or the starting position of the supplementary frame needs to be manually specified, and it can be used to manually restore the progress of the supplementary frame\n\n * Starting block count = last chunk number exported in the output folder + 1 (for example chunk-001 in the picture, the starting block count should be 1+1=2)\n * Start input frame number = single output block size * (start block count - 1) in output quality settings (render settings)\n\n\n\n\n\nAs shown above, a video chunk has 1000 frames\n\n\n# Return to origin\n\nReturn the initial block and the initial input frame number to the original value, and start supplementary frames from zero time.\n\n\n\n\n# Risk Mode\n\nWhen you need to restore the progress of the task, enabling this option can reduce the time required for the program to restore the progress, but enabling it may cause audio and video to be out of sync, Not recommended to enable.\n\n\n\n\n# Transition recognition\n\n\n\n\n# Enable transition recognition\n\nRecognize scene switching\n\nIn order to avoid the jelly effect when the scene is switched during the supplementary frame process, it is recommended that you enable transition recognition.\n\nAfter checking Enable Transition Recognition, generally select 12 for the parameter value below; if you find that the final exported video is lagging, you can consider adjusting it to 15; If you find that there are many jelly effects, you can consider adjusting the parameter value to 9, and the parameter value range is generally 9-15.\n\nAs shown in the picture: jelly produced by missed judgment in transition\n\n\n\n\n# Maximum recognition threshold (no need to adjust by default)\n\nWhen use fixed transition recognition is not enabled, the recommended value is 80-90\n\nWhen Use Fixed Transition Recognition is turned on, the recommended value is 40-60\n\n\n# Use fixed transition recognition\n\nUse a fixed threshold (maximum recognition threshold) to identify transitions (unstable), only used when the default transition detection is inaccurate, such as mixed cuts with a lot of shots.\n\n\n# Transitions use frame blending\n\nThe traditional method is to copy the previous frame as a transition frame. The method is to blend the two frames before and after (gradient)\n\n\n# Output transition frame\n\nTransition frames in the output video. The transition frames will be accompanied by relevant judgment information, and will be output in png format in the scene folder of the project folder.\n\n\n# Output resolution setting\n\n\n\n\n# Output file resolution\n\nThe drop down box is used for resolution preset selection. When the preset is Custom, you can set the final output resolution of the video. SVFI will first adjust the resolution of the picture, and then make up frames.\n\n\n# Output black edge length\n\nIt can be used to crop the black border in the video, and the width and height need to be specified manually.\n\n> Example: video has a resolution of 3840x2160, and the actual screen resolution is 3840x1620, then fill in 270 = (original height - actual height) ÷ 2 for height here.\n\n: If AI super resolution is used, the video here refers to the final output video\n\n> Example: The input video is 1920x1080, the actual resolution is 1920x800, and the super resolution 2 times the output is 3840x1600. Then the height of the black border is 280, and the output resolution can be customized to 3840x1600\n\nTip\n\nIf you input -1 for width and height, SVFI will automatically recognize the black border of the input video and crop it\n\n\n# Complete the black border after processing\n\nAfter removing the black borders, perform processing (frame supplementation or super resolution), and then automatically add back the black borders after the frames are completed.\n\nTip\n\nTo a certain extent, the calculation amount of a single frame can be reduced and the processing speed can be accelerated.\n\n\n\n\n\n# Use AI Super Resolution\n\nTip\n\nThis function requires the purchase of Professional DLC\n\nTo make the video picture clearer, it currently supports 9 effective super resolution algorithms.\n\n> Process animation material: Anime4K, AnimeSR, realCUGAN, realESR, waifu2x, waifuCuda\n> \n> Process real footage: BasicVSR series, FTVSR, NvidiaSR, realESR\n\nTip\n\nSVFI\'s definition of animation material and live-action material is as follows:\n\n * Anime material is a motion video clip mainly composed of flat image layers, each layer has a clear border with another layer. For example, hand-painted 2D animation, most three-rendered two-screen, etc.\n\n * Actual Shot Materials are real-world pictures or computer-generated pictures taken with a single-view camera, the naked eye cannot distinguish the layers and their boundaries. Such as live-action movies, 3D CG, 3D game screens, etc.\n\nIn particular, we consider animations made of 3D/3G backgrounds + 2D characters as animation materials.\n\n\n\n\n# Super Resolution Model\n\n\n# realCUGAN\n\nAnime only, the effect is very good\n\n * up2x means 2 times magnification, 3x, 4x are similar\n\n * The pro model is an enhanced version (see the official introduction ailab/Real-CUGAN at main bilibili/ailab (github.com) for details)\n\n * Models with the word conservative are conservative\n\n * The model with no-denoise does not perform noise reduction\n\n * The model with denoise performs noise reduction, and the number behind represents the noise reduction strength\n\n\n# ncnnCUGAN\n\nThe NCNN version of CUGAN (AMD card, NVIDIA card, Intel card), the introduction is the same as above\n\n\n# waifuCuda\n\nUsed for animation super resolution, the speed and effect are somewhat similar to cugan,\n\n\n# realESR\n\n3D animation is available, more suitable for animation\n\n * The RealESRGAN model tends to make up the brain, and the picture is clearer and more beautiful\n\n * RealESRNet model tends to smear, but the picture keeps the original color\n\n * The model marked with anime is dedicated to super resolution of anime, and the speed is slightly faster than the former two\n\n * anime is the official model, anime_110k is the self-training model\n\n * RealESR_RFDN is a self-training super resolution model, which is fast and suitable for animation input\n\n\n# ncnnRealESR\n\nNCNN version of realESR, A card, I card, N card common\n\n * realesr-animevideov3 (a relatively conservative animation video super resolution model, with faster speed and higher quality)\n\n * realesrgan-4xplus (4x enlarged model)\n\n * realesrgan-4xplus-anime (4x anime enlarged model)\n\n\n# AnimeSR (anime super resolution algorithm developed by Tencent ARC Lab)\n\nOnly one quadruple magnification model (AnimeSR_v2_x4.pth), the effect is more conservative than cugan\n\n\n# NvidiaSR (ultra-high-speed super resolution algorithm developed by NVIDIA)\n\nTip\n\nThis algorithm is only available in the sponsored version\n\nThe algorithm needs to be pre-installed according to your own N card version [Video effect function in NVIDIA Broadcast software] (https://www.nvidia.cn/geforce/broadcasting/broadcast-sdk/resources/)\n\nThe model with the word AR has the function of noise reduction and color band removal, and the rest of the models are only enlarged\n\nWith super resolution strength option, the larger the value, the greater the super resolution strength, and vice versa (value range 0 ### 1)\n\n\n# BasicVSRPlusPlus (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)\n\nTip\n\nThis algorithm is only available in the public beta version of the professional version of DLC, you need to manually go to the Steam settings - beta version to select\n\n * basicvsrpp_reds_4x super resolution model trained on the reds dataset\n\n * basicvsrpp_vimeo_bd_4x super resolution model trained on vimeo bd dataset\n\n * basicvsrpp_vimeo_bi_4x super resolution model trained on vimeo bi dataset\n\n\n# BasicVSRPlusPlusRestore (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)\n\nTip\n\nThis algorithm is only available in the public beta version\n\n * basicvsrpp_deblur_dvd_max_4x deblur quadruple magnification model (performs better on dvd material)\n\n * basicvsrpp_deblur_gopro_max_4x deblurring quadruple magnification model (performs better on photographic materials)\n\n * basicvsrpp_denoise_max_4x quadruple magnification noise reduction model\n\n * basicvsrpp_ntire_t1_decompress_max_4x quadruple zoom to decompress model t1\n\n * basicvsrpp_ntire_t2_decompress_max_4x quadruple zoom to decompress model t2\n\n * basicvsrpp_ntire_t3_decompress_max_4x quadruple magnification decompression model t3 (recommended)\n\n * basicvsrpp_ntire_t3_decompress_max_4x_trt quadruple magnification decompression model t3 (TensorRT acceleration)\n\n\n# PureBasicVSR (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)\n\n * RealBasicVSR_4x basic quadruple magnification model\n\n * reds_wogan_x4 quadruple scale-up model trained on the reds dataset (without using gan)\n\n * reds_x4 quadruple scale-up model trained on the reds dataset\n\n\n# RealBasicVSR (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)\n\nrealbasicvsr_reds_4x A quadruple zoom model trained on the reds dataset\n\n\n# FTVSR (real-time super resolution algorithm, the effect depends on the length of the super resolution sequence)\n\nTip\n\nThis algorithm is only available in the public beta version\n\n * ftvsr_reds_4x quadruple scale-up model trained on the reds dataset\n\n * ftvsr_vimeo_4x quadruple magnification model trained on the vimeo dataset\n\n\n# Anime4K (ultra-high-speed real-time animation super resolution algorithm, more conservative)\n\nWe have prepared 6 preset scripts for users\n\n * Anime4K_Upscale_x2 A/B/C/D are all enlarged by 2 times (A is selected by default)\n\n * Anime4K_Upscale_x3 is 3 times enlarged, and the x4 model is similar\n\n\n# waifu2x (classic conservative super resolution algorithm)\n\n * The cunet model is used for animation super resolution\n\n * The photo model is used for real shooting\n\n-anime is used for animation superscore\n\n\n# TensorRT (ultra-fast acceleration for the above part of the super resolution algorithm)\n\nTip\n\nThis algorithm is only available in the public beta version\n\n * Full model acceleration with cugan\n\n * real-animevideov3 is a model specially prepared for animation video super resolution in RealESR\n\n * RealESRGANv2-animevideo-xsx2 double animation video super resolution zoom model\n\n * RealESRGANv2-animevideo-xsx4 quadruple animation video super resolution zoom model\n\n\n# load graphics card\n\nSpecify which graphics card to use for super resolution\n\n\n# super resolution Algorithm\n\nSelect the algorithm used for super resolution\n\n\n# Super resolution model multiple\n\nThe overresolution multiple of the currently selected model\n\n\n# Transfer Resolution Ratio\n\nFirst scale the original video according to the percentage set by the user, and then perform super resolution.\n\n> Example: Original video 1920x1080, transfer resolution ratio 50%, model magnification 4x\n\nRunning process: 1920x1080 -> 960x540 (down scaling) -> 3840x2160 (super resolution)\n\n\n# RealCUGAN cutting mode\n\ndedicated to realCUGAN, the more you cut, the more memory you save\n\n * No Tile: no cutting is used\n\n * 1/2 on Width: Horizontally divided into two\n\n * 1/2 on both W and H: Horizontally and vertically divided into two\n\n * 1/3 on w & h: cut lengthwise and lengthwise into thirds\n\n * 1/4 on w & h: cut into quarters lengthwise and lengthwise\n\n\n# RealCUGAN low memory mode\n\ndedicated to realCUGAN, used when the video memory of the graphics card is insufficient\n\n * Low VRAM Mode: enable low VRAM mode\n\n * None: Do not use low memory mode\n\n\n# Cutting block size (it is not recommended to open when using realCUGAN)\n\n * There are presets for video memory size, and you can also choose to customize the adjustment\n\n\n# Super-score sequence length\n\nIt is only valid when a super resolution algorithm that requires multi-frame input, such as the BasicVSR series, is selected\n\n * The larger the length of the super resolution sequence, the more frames are input for a single super resolution, and the texture is more stable, but at the same time it will increase the video memory usage,\n * It is recommended to keep the value above 10. If the video memory is insufficient, it is recommended to reduce the screen resolution and ensure that the value is above 5\n\n\n\n\n# Super resolution using half precision\n\n * It is recommended to enable it, which can greatly reduce the video memory usage and have little impact on the picture quality.\n * For nVidia\'s 10xx series Pascal architecture graphics card, it will slow down the super resolution speed\n\n\n# TTA\n\nIt is only supported by ncnnCUGAN, which consumes a lot of time in exchange for a small improvement in image quality It is only supported by ncnnCUGAN, which consumes a lot of time in exchange for a small improvement in image quality\n\n\n# FMNet - HDR10\n\nTip\n\nThis feature is only available in the public beta version\n\nConvert SDR video to HDR10 with AI algorithm\n\n\n# Output settings (suppression parameter quality)\n\nTip\n\nOnly for super resolution, please click one-click suppression to start the task; If you want to make supplementary frame super resolution at the same time, please click one-key supplementary frame\n\nWarning\n\nAt the same time, performing supplementary frame super resolution will consume more video memory, and insufficient video memory may cause the task to fail.\n\nIf the video memory is less than 10G, it is recommended to use one-key suppression to complete the super resolution first, and then perform frame supplementation.\n\n\n\n\n# Render Quality CRF\n\nIt is used to adjust the quality loss when exporting video, and it is positively correlated with output bit rate. Using different compression codecs and compression presets will affect the CRF.\n\nCRF numerical parameters are generally 16, which is not lossy to the naked eye at this time; for H.265 encoding, the bit rate will drop significantly. Please judge whether the CRF value is reasonable based on the image quality seen by the naked eye. ** If it is used as a favorite CRF value parameter, it can be set to 12. **The smaller the value of CRF, the smaller the image loss after the operation process, and the larger the volume (bit rate) of the exported finished video. **\n\nNote: The same value, the output quality of different encoders are different\n\n\n# target bit rate\n\nAs an optional option to replace the rendering quality CRF, it is basically the same as the setting standards of Primer Pro, After Effects, and DaVinci Resolve\n\n\n# Encoder\n\n * AUTO Automatically determine encoder options based on the slide bar below the software\n * CPU If you select this option, the quality is the highest, but the CPU usage is also the highest. The performance of the CPU determines whether it will be blocked during frame supplementation or super resolution (resulting in a decrease in graphics card usage), and the time to complete the final operation.\n * NVENC This option is only available for NVIDIA graphics cards that support NVENC function. If your graphics card does not support NVENC function, please do not select this option. Please refer to the NVIDIA NVENC Gen.pdf in the installation directory to check whether your graphics card supports NVENC\n * VCE This option is only available for AMD graphics cards that support VCE function, if your graphics card does not support VCE function, please do not select this option.\n * QSV This option is only supported by users with Intel Core Graphics (such as Intel UHD 630, IrisPro 580), and users who are not of this type should not choose it.\n\nTip\n\nThe following encoders need to purchase Professional DLC\n\n * NVENCC is an optimized version of NVENC, with faster processing speed and better quality of works.\n * QSVENCC is an optimized version of QSV, which can complete tasks more efficiently.\n * VCENCC is an optimized version of VCE, which can complete tasks more efficiently.\n\nPerceptual comparison:\n\nENCODERS   USING HARDWARE        SPEED    QUALITY   FILE SIZE   SELECTION RECOMMENDATIONS\nCPU        CPU                   Medium   High      Medium      Pursue image quality and encoding stability and A card user\n                                                                AU user selection\nNVENC      Ncard                 Fast     Medium                \nQSV        Intel Core Graphics   Fast     Medium                \n\n\n# Select compression encoding\n\nFor the selection of this function, you need to have certain common sense of video suppression.\n\nWarning\n\nIf you are new to suppression, please keep the following rules in mind:\n\n * HDR output must choose H.265 10bit encoding\n * Be sure to choose H.265 encoding for resolutions above 2K (especially 4K, 8K resolutions)\n * If there are problems with both H.264 and H.265 encoding, use ProRes encoding. This encoding output is closest to the lossless naked eye, and the bit rate is extremely high. It is an intermediate encoding format for editing work.\n * It is recommended to use H.265 fast encoding or ProRes encoding\n\n\n# Select suppress preset\n\n * CPU: English meaning The faster the speed, the lower the quality of the work, otherwise the higher the quality.\n\n * NVENC (only for N card): It is recommended to choose p7 without brain\n\n * QSV (for Intel graphics card): Select slow directly\n\n * VCE (only for A card): directly select quality\n\n * NVENCC (only for N card): directly select quality\n\n * QSVENCC (for Intel graphics card): choose best directly\n\n * VCENCC (only for A card): directly select slow\n\n\n# NVIDIA card hard-coded preset\n\nWhen selecting the NVENC encoder, the N card hard-coded preset can reduce the exported video volume without changing the picture quality. You need to check which generation of NVENC compression chip your N card is. If it exceeds 7th, directly select 7th+\n\n\n# Default suppression scheme\n\nUsing the traditional compression scheme, the compatibility is strong, and the exported video volume may increase.\n\nTip\n\nEnabling this feature can solve most broken pipe problems.\n\n\n# Audio secondary pressure to AAC\n\n * Re-encode the audio (usually used on videos uploaded to the platform)\n * Compress all audio tracks in the video to 640kbps aac format.\n\n\n# HDR Strict Mode\n\nHandle HDR content with strict presets, enabled by default\n\n\n# DV Compatible with HDR10\n\nEnable HDR10 compatibility when outputting in Dolby Vision, which is enabled by default\n\n\n# One-click HDR: Convert SDR video to HDR10+\n\nFour one-click HDR modes need to try the effect by yourself\n\n\n# Decoding quality control\n\n\n# Use vspipe pre-decoding\n\nUse vspipe as a pre-decoder, this function is a prerequisite for many specific functions (such as decolorization, fast noise addition, QTGMC deinterlacing),\n\nIf you find that he can\'t decode the input or the task reports an error, please close it\n\n\n# Hardware decoding\n\nIt can reduce the decoding pressure of high-resolution video, but it may reduce the picture quality to a certain extent, and cause the frame supplement module to explode video memory when the video memory is tight.\n\n\n# Quick frame splitting\n\nFast frame splitting can reduce the decoding pressure, but it may result in deviations in the color of the picture.\n\n\n# High precision optimization workflow\n\nTip\n\nThis function requires the purchase of Professional DLC\n\n * If the CPU performance is excessive, it is recommended that you enable this function, which can solve the color deviation problem of most pictures, and can solve the color deviation problem caused by HDR video compression to the greatest extent. This feature will increase the CPU burden, and even affect the speed of supplementary frames.\n * Turning on this function for super resolution work will turn off half-precision (requires more video memory). Please select as appropriate.\n\nTip\n\nIt is recommended to enable this option when inputting HDR video\n\n\n# Turn on deinterlacing\n\nTip\n\nThis function requires the purchase of Professional DLC\n\n * Use ffmpeg to deinterlace input interlaced video.\n\n * When using vspipe pre-decoding, use QTGMC to deinterlace the picture\n\n\n# DePan (De-ribbon)\n\nUse depanStabilise in vs to handle ribbons\n\n\n# Fast Noise Reduction\n\nTip\n\nThis function requires the purchase of Professional DLC\n\nIf there is no special need for the "Quick" option under this column, please keep it off, otherwise it will slow down the task processing speed.\n\nTip\n\nIt is recommended to test whether this option can improve the picture quality by controlling variables by yourself.\n\nIncompatible with high precision optimization workflows\n\n\n# Add noise quickly\n\nAdd noise to video, often used for video overtime\n\n\n# Custom frame splitting parameters (professional option)\n\nUsed to replace the parameters used by ffmpeg or vspipe for decoding, use || intervals between custom parameters\n\n\n# Custom encoding settings\n\n\n# Specify the number of encoding threads\n\nWhen the encoder is a CPU, there is a chance to control the CPU usage to control the rendering speed.\n\n\n# Custom suppression parameters\n\nThis function is a professional option (note that the number of input items must be even number),\n\nThe key value is separated by ||\n\n> Example: Custom suppression parameters for CPU h265 suppression:\n> \n> -x265-params||ref=4:me=3:subme=4:rd=4:merange=38:rdoq-level=2:rc-lookahead=40:scenecut=40:strong-intra-smoothing=0\n\n\n# Time Remapping: Change the speed of the video\n\nTip\n\nThis function requires the purchase of Professional DLC\n\n * This function is used to make "slow motion" clips.\n\n * For example, if the output frame rate is set to 120 frames, and the time remapping is set to 60 frames, the output effect is equivalent to 50% Playback speed slowed down.\n\n * Other situations can be analogized in turn, you can set the output frame rate by yourself, decimals are supported.\n\nWarning\n\nFor animation materials, please enable Space-Time Resampling in Video Fluency Optimization in Fixed Frame Settings as much as possible.\n\nOr use software such as Premiere to reduce the frame rate of the original video to complete the removal of duplicate frames to avoid stuttering after remapping.\n\nThe original video frame rate is generally reduced to 8 or 12 frames\n\n\n# Head and tail loop\n\nPutting the last frame in the first frame to fit some end-to-end looping videos\n\n\n# IO control\n\n\n\n\n# Manually specify the buffer memory size\n\nIf the running memory is tight (less than 16G), it is recommended to manually specify the buffer memory size to be 2-3G to avoid out of memory errors.\n\n\n# Single output block size\n\n * For frame complementing and compression tasks, a small clip without audio will be output every time the number of frames rendered is this value, so that you can preview the effect conveniently.\n * The clips will be generated in the output folder you set, and merged into one file after the frame or compression task is completed.\n\n\n# Keep the project folder after the task is completed\n\nDo not delete the chunk video generated in the middle after the supplementary frame is completed.\n\n\n# VFI setting\n\n\n# Safe Frame Rate\n\nIf the video is to be uploaded to the corresponding media platform for online viewing, please enable this option\n\nTip\n\nIf you find a blurred video screen when playing a video, it is most likely a problem with the decoder. Please try to change the decoder or check this option to reduce the decoding pressure.\n\n\n# Reverse Optical Flow\n\nThis function can make the picture more silky to a certain extent.\n\n\n# absolutely smooth\n\nThis function may make the screen more silky, just enable it by default (if it is not available, it will not be enabled when the software is running)\n\n\n# Optical Flow Scale\n\n * When using the RIFE algorithm, when the original video size is 1080P, the default setting is 0.5, 4K and above is 0.25, and less than 1080P is 1.0\n\n * When using the GMFSS algorithm, when the original video size is 1080P, 1.0 is set by default, 0.5 is set for 4K and above, and 1.0 is set for less than 1080P\n\n\n# Interlaced frame interpolation\n\n * Equivalent to a special cut, used to reduce video memory usage\n\n * Appropriate selection of this item can make the graphics card with small video memory supplement the super-large resolution (such as 4G complement 8K)\n\n\n# Video fluency optimization\n\n * Space-time linearization: Enhance the smoothness of the picture when the output frame rate is 60 (TruMotion)( general )\n * Fixed Threshold Deduplication: It is used to alleviate the picture stuttering caused by repeated frames. The general value is 0.2, and 0.5 is used for animation, 1.0 or higher (General)\n * Remove 1 shot 2: Specially for animation, some materials perform better, but in most cases, duplicate frames cannot be completely removed, and it is easy to introduce stuttering. (anime) (outdated, use spatio-temporal resampling is recommended)\n * Space-time resampling: Completely remove the stuttering of animation video materials, please ensure that the input frame rate is around 24, and the output frame rate can only be an integer multiple of (input frame rate/2)**(anime) (only supported Algorithms and frame complement models available at any time) **\n * First-order difference deduplication: Anime deduplication (Obsolete)\n\nWarning\n\nDue to the limited ability of AI frame supplementation in animation frame supplementation at this stage, choosing deduplication will increase the motion range between frames, resulting in picture distortion when supplementing frames. Please choose the best deduplication for each input video control variable by yourself. heavy mode.\n\nIt is recommended that you choose the deduplication mode carefully. If you need to supplement frames for the entire animation, it is not recommended to enable deduplication.\n\nAfter enabling video fluency optimization (time-space resampling), the effect of supplementary frame is as follows\n\n\n\n\n\n# Load Graphics Card\n\nSpecify which graphics card to use for supplementary frames\n\n\n# frame interpolation algorithm\n\nSelect the frame interpolation algorithm (including RIFE, IFRNet, DAIN, GMFSS, EMA)\n\n\n# VFI model\n\nTip\n\nModels with ncnn use ncnn as the forward reasoning framework, which is compatible with N cards and A cards, and models without this word cannot be used for A card and display.\n\n * RIFE: High-speed, popular new-age supplementary frame algorithm (the following is the model introduction)\n\n> 2.3: classic, hot model, fast speed and good effect.\n> \n> 3.8: (Two-way optical flow must be turned on), the quality is better and clear.\n> \n> 4.4: Slightly lower quality, super fast.\n> \n> 4.5: The quality is close to or even surpasses 2.3, with higher fluency and faster speed.\n> \n> 4.6: An evolution of the 4.5 model, recommended.\n> \n> rpr_v7_1.0: Combined model, blurred, improved fluency.\n> \n> rpr_v7_2.3: Combination model with improved fluency.\n> \n> rpr_v7_2.3_ultra: Combined model, more suitable for complex images.\n> \n> rpr_v7_2.3_ultra#2: Combined model, more suitable for complex images.\n\n * ncnn-rife: supports various graphics card versions of RIFE, good compatibility, fast speed, and slightly worse quality than RIFE.\n\n * IFRNet: real shot & animation model, faster speed, lower quality than RIFE model, not recommended.\n\n * ncnn_dain: traditional old algorithm, animation real shooting can be used, support any time, the speed is very slow, and the fluency is very high.\n\n * GMFSS: Experimental new algorithm, slow speed, high quality (the model is introduced below) (the model marked with trt is an accelerated model)\n\n> union_v: The third-generation GMFSS model, the most powerful animation supplementary frame model, with stable structure and smooth picture\n> \n> union_w: The third-generation GMFSS model, currently the most powerful animation supplementary frame model, the picture is clear and clean\n> \n> pg104: The fourth generation of gmfss animation model (experimental effect is better than union_v)\n> \n> real: gmfss second generation real shot model\n> \n> primaris: gmfss second generation animation model\n> \n> up: The first generation of gmfss model, the speed is very fast, the text may flicker\n> \n> basic: The first generation gmfss model, the speed is very slow, the effect may be more stable than up\n\n * EMA: CVPR 2023 SOTA real-time supplementary frame algorithm\n\n> ema_ours_t officially supports the full blood model at any time\n> \n> ema_outs official full blood model\n> \n> ema_ours_small_t Official fast model that supports any time\n> \n> Quick model given by ema_outs_small official\n\n\n# TTA mode\n\nTip\n\nThis function requires the purchase of Professional DLC\n\n> Enable this function to reduce picture jelly, reduce subtitle jitter, and reduce the problem of objects disappearing. Make the picture more smooth and comfortable\n> \n> Additional frame supplement time is required, and some frame supplement models do not support this function.\n> \n> The bigger the number behind, the slower the speed, and the less jelly, generally fill in 1 or 2.\n> \n> sideways, suitable for rife, 3.x series models\n> \n> Medium orientation, suitable for rife, 2.x series models\n\n\n# output layer fine-tuning mode (only available for experimental models)\n\n> residual: It will make the picture blurred, but the structure is more complete\n> \n> direct: direct output, the picture is clearer\n\n\n# Bidirectional Optical Flow\n\n> The speed is reduced by about half, and the effect is slightly improved (the rife 3.8 model must be turned on) (the rife 4.x model is not supported yet).\n\n\n# Dynamic Optical Flow Scale\n\nTip\n\nThis function requires the purchase of Professional DLC\n\n> Dynamically select the optical flow scale during the frame complementing process, which can reduce the problem of object disappearance and reduce jelly. It is recommended to enable it.\n\n\n# Customize preset column\n\n\n# Create a new preset based on the current settings\n\nAfter giving the preset a name, click to create a new preset\n\n\n# Remove current preset\n\nDelete the currently selected preset\n\n\n# Apply specific presets\n\nLoad previously saved presets, automatically load parameters\n\n\n# Toolbox\n\n\n# Convert video to GIF animation\n\nGenerate high-quality animated GIFs\n\n\n# Loop animation\n\nGenerate a circular animation, it is recommended to keep the default.\n\n\n# Merge existing blocks\n\nMerge the scattered chunk fragments.\n\nTip\n\nIf the task failed during the final merge, you can directly select the task after adjusting the settings and click this button to complete the merge operation.\n\n\n# Audio and video merge\n\n * Fill in the full path of the video (eg: D:\\01\\myvideo.mp4)\n\n * Fill in the audio path of the video (eg: D:\\01\\myvideo.aac)\n\n * Or use a video to input audio (eg: D:\\01\\otherVideo.mp4)\n\n * Output video path (eg: D:\\01\\output.mp4)\n\n\n# Export the current settings to a text file\n\nExport settings information as a text document.\n\nTip\n\nIf the video output of the software does not meet expectations, such as color cast, poor effect, etc., you can click this button and send the setting file to the developer to locate the problem.\n\n\n# Debug\n\nOutput debug information while the task is in progress\n\n\n# title bar function\n\n\n\n\n# Preferences\n\n\n# Multitasking rest interval\n\nGive the device a break every X hours (shortly pause the task)\n\n\n# Select cache folder\n\nSpecify the task folder to a different location. The final output video will still be in the destination folder\n\n\n# After the completion of the supplementary frame task\n\nYou can choose some automatic operations after frame completion\n\n\n# Unavailable features\n\nForced use of CPU for frame-fill superscore\n\n\n# Enable expert mode\n\nEnabled by default, display all functions\n\n\n# Enable parameter text preview before task\n\nBefore clicking to start the supplementary frame, a pop-up box will pop up, you can browse through it to confirm that the parameter settings are correct, and then perform supplementary frame or super resolution\n\n\n# Clear the task list after the task is completed\n\nClear the queue after all tasks in the list are completed\n\n\n# Use global settings\n\nUnified parameter setting for all tasks\n\n\n# Reckless Exit\n\nEnabled by default, forcibly end the software process\n\nTip\n\nIf you find that the video memory is occupied after closing the software, it is recommended to enable this option\n\n\n# Original flavor suppression mode\n\nWhen suppressing tasks alone, operations such as deduplication of repeated frames are not enabled\n\n\n# Open preview\n\nOpening the preview window when supplementing frames will slow down the running speed of the program to a certain extent\n\n\n# Automatic error correction\n\nAutomatically modify settings to prevent task errors\n\n\n# Enable quiet mode\n\nNo prompt box pops up when the software starts',normalizedContent:'the following content will introduce you to the advanced settings section of the software\n\n\n# software advanced settings\n\n\n# workflow recovery\n\n\n\n\n# auto configure\n\ntip\n\nwhen the program exits due to a power outage in the middle of the task or the termination of the task due to other circumstances, you can click "auto configure" to restore the last block position.\n\nbefore clicking this button, please click on the task entry to restore the progress. then click "interpolate", the software will pop up a window to confirm the starting position of supplementary frame.\n\n\n\n\n# start point and end point\n\nyou can choose the time period that needs to be supplemented\n\n\n\n> input format: hours:minutes:seconds\n\nwarning\n\nafter specifying the start and end complement frame time, progress recovery is not supported after manual termination or power failure\n\n\n# start block count and start input frame number\n\nit is used when the automatic search progress fails or the starting position of the supplementary frame needs to be manually specified, and it can be used to manually restore the progress of the supplementary frame\n\n * starting block count = last chunk number exported in the output folder + 1 (for example chunk-001 in the picture, the starting block count should be 1+1=2)\n * start input frame number = single output block size * (start block count - 1) in output quality settings (render settings)\n\n\n\n\n\nas shown above, a video chunk has 1000 frames\n\n\n# return to origin\n\nreturn the initial block and the initial input frame number to the original value, and start supplementary frames from zero time.\n\n\n\n\n# risk mode\n\nwhen you need to restore the progress of the task, enabling this option can reduce the time required for the program to restore the progress, but enabling it may cause audio and video to be out of sync, not recommended to enable.\n\n\n\n\n# transition recognition\n\n\n\n\n# enable transition recognition\n\nrecognize scene switching\n\nin order to avoid the jelly effect when the scene is switched during the supplementary frame process, it is recommended that you enable transition recognition.\n\nafter checking enable transition recognition, generally select 12 for the parameter value below; if you find that the final exported video is lagging, you can consider adjusting it to 15; if you find that there are many jelly effects, you can consider adjusting the parameter value to 9, and the parameter value range is generally 9-15.\n\nas shown in the picture: jelly produced by missed judgment in transition\n\n\n\n\n# maximum recognition threshold (no need to adjust by default)\n\nwhen use fixed transition recognition is not enabled, the recommended value is 80-90\n\nwhen use fixed transition recognition is turned on, the recommended value is 40-60\n\n\n# use fixed transition recognition\n\nuse a fixed threshold (maximum recognition threshold) to identify transitions (unstable), only used when the default transition detection is inaccurate, such as mixed cuts with a lot of shots.\n\n\n# transitions use frame blending\n\nthe traditional method is to copy the previous frame as a transition frame. the method is to blend the two frames before and after (gradient)\n\n\n# output transition frame\n\ntransition frames in the output video. the transition frames will be accompanied by relevant judgment information, and will be output in png format in the scene folder of the project folder.\n\n\n# output resolution setting\n\n\n\n\n# output file resolution\n\nthe drop down box is used for resolution preset selection. when the preset is custom, you can set the final output resolution of the video. svfi will first adjust the resolution of the picture, and then make up frames.\n\n\n# output black edge length\n\nit can be used to crop the black border in the video, and the width and height need to be specified manually.\n\n> example: video has a resolution of 3840x2160, and the actual screen resolution is 3840x1620, then fill in 270 = (original height - actual height) ÷ 2 for height here.\n\n: if ai super resolution is used, the video here refers to the final output video\n\n> example: the input video is 1920x1080, the actual resolution is 1920x800, and the super resolution 2 times the output is 3840x1600. then the height of the black border is 280, and the output resolution can be customized to 3840x1600\n\ntip\n\nif you input -1 for width and height, svfi will automatically recognize the black border of the input video and crop it\n\n\n# complete the black border after processing\n\nafter removing the black borders, perform processing (frame supplementation or super resolution), and then automatically add back the black borders after the frames are completed.\n\ntip\n\nto a certain extent, the calculation amount of a single frame can be reduced and the processing speed can be accelerated.\n\n\n\n\n\n# use ai super resolution\n\ntip\n\nthis function requires the purchase of professional dlc\n\nto make the video picture clearer, it currently supports 9 effective super resolution algorithms.\n\n> process animation material: anime4k, animesr, realcugan, realesr, waifu2x, waifucuda\n> \n> process real footage: basicvsr series, ftvsr, nvidiasr, realesr\n\ntip\n\nsvfi\'s definition of animation material and live-action material is as follows:\n\n * anime material is a motion video clip mainly composed of flat image layers, each layer has a clear border with another layer. for example, hand-painted 2d animation, most three-rendered two-screen, etc.\n\n * actual shot materials are real-world pictures or computer-generated pictures taken with a single-view camera, the naked eye cannot distinguish the layers and their boundaries. such as live-action movies, 3d cg, 3d game screens, etc.\n\nin particular, we consider animations made of 3d/3g backgrounds + 2d characters as animation materials.\n\n\n\n\n# super resolution model\n\n\n# realcugan\n\nanime only, the effect is very good\n\n * up2x means 2 times magnification, 3x, 4x are similar\n\n * the pro model is an enhanced version (see the official introduction ailab/real-cugan at main bilibili/ailab (github.com) for details)\n\n * models with the word conservative are conservative\n\n * the model with no-denoise does not perform noise reduction\n\n * the model with denoise performs noise reduction, and the number behind represents the noise reduction strength\n\n\n# ncnncugan\n\nthe ncnn version of cugan (amd card, nvidia card, intel card), the introduction is the same as above\n\n\n# waifucuda\n\nused for animation super resolution, the speed and effect are somewhat similar to cugan,\n\n\n# realesr\n\n3d animation is available, more suitable for animation\n\n * the realesrgan model tends to make up the brain, and the picture is clearer and more beautiful\n\n * realesrnet model tends to smear, but the picture keeps the original color\n\n * the model marked with anime is dedicated to super resolution of anime, and the speed is slightly faster than the former two\n\n * anime is the official model, anime_110k is the self-training model\n\n * realesr_rfdn is a self-training super resolution model, which is fast and suitable for animation input\n\n\n# ncnnrealesr\n\nncnn version of realesr, a card, i card, n card common\n\n * realesr-animevideov3 (a relatively conservative animation video super resolution model, with faster speed and higher quality)\n\n * realesrgan-4xplus (4x enlarged model)\n\n * realesrgan-4xplus-anime (4x anime enlarged model)\n\n\n# animesr (anime super resolution algorithm developed by tencent arc lab)\n\nonly one quadruple magnification model (animesr_v2_x4.pth), the effect is more conservative than cugan\n\n\n# nvidiasr (ultra-high-speed super resolution algorithm developed by nvidia)\n\ntip\n\nthis algorithm is only available in the sponsored version\n\nthe algorithm needs to be pre-installed according to your own n card version [video effect function in nvidia broadcast software] (https://www.nvidia.cn/geforce/broadcasting/broadcast-sdk/resources/)\n\nthe model with the word ar has the function of noise reduction and color band removal, and the rest of the models are only enlarged\n\nwith super resolution strength option, the larger the value, the greater the super resolution strength, and vice versa (value range 0 ### 1)\n\n\n# basicvsrplusplus (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)\n\ntip\n\nthis algorithm is only available in the public beta version of the professional version of dlc, you need to manually go to the steam settings - beta version to select\n\n * basicvsrpp_reds_4x super resolution model trained on the reds dataset\n\n * basicvsrpp_vimeo_bd_4x super resolution model trained on vimeo bd dataset\n\n * basicvsrpp_vimeo_bi_4x super resolution model trained on vimeo bi dataset\n\n\n# basicvsrplusplusrestore (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)\n\ntip\n\nthis algorithm is only available in the public beta version\n\n * basicvsrpp_deblur_dvd_max_4x deblur quadruple magnification model (performs better on dvd material)\n\n * basicvsrpp_deblur_gopro_max_4x deblurring quadruple magnification model (performs better on photographic materials)\n\n * basicvsrpp_denoise_max_4x quadruple magnification noise reduction model\n\n * basicvsrpp_ntire_t1_decompress_max_4x quadruple zoom to decompress model t1\n\n * basicvsrpp_ntire_t2_decompress_max_4x quadruple zoom to decompress model t2\n\n * basicvsrpp_ntire_t3_decompress_max_4x quadruple magnification decompression model t3 (recommended)\n\n * basicvsrpp_ntire_t3_decompress_max_4x_trt quadruple magnification decompression model t3 (tensorrt acceleration)\n\n\n# purebasicvsr (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)\n\n * realbasicvsr_4x basic quadruple magnification model\n\n * reds_wogan_x4 quadruple scale-up model trained on the reds dataset (without using gan)\n\n * reds_x4 quadruple scale-up model trained on the reds dataset\n\n\n# realbasicvsr (real shot super resolution algorithm, the effect depends on the length of the super resolution sequence)\n\nrealbasicvsr_reds_4x a quadruple zoom model trained on the reds dataset\n\n\n# ftvsr (real-time super resolution algorithm, the effect depends on the length of the super resolution sequence)\n\ntip\n\nthis algorithm is only available in the public beta version\n\n * ftvsr_reds_4x quadruple scale-up model trained on the reds dataset\n\n * ftvsr_vimeo_4x quadruple magnification model trained on the vimeo dataset\n\n\n# anime4k (ultra-high-speed real-time animation super resolution algorithm, more conservative)\n\nwe have prepared 6 preset scripts for users\n\n * anime4k_upscale_x2 a/b/c/d are all enlarged by 2 times (a is selected by default)\n\n * anime4k_upscale_x3 is 3 times enlarged, and the x4 model is similar\n\n\n# waifu2x (classic conservative super resolution algorithm)\n\n * the cunet model is used for animation super resolution\n\n * the photo model is used for real shooting\n\n-anime is used for animation superscore\n\n\n# tensorrt (ultra-fast acceleration for the above part of the super resolution algorithm)\n\ntip\n\nthis algorithm is only available in the public beta version\n\n * full model acceleration with cugan\n\n * real-animevideov3 is a model specially prepared for animation video super resolution in realesr\n\n * realesrganv2-animevideo-xsx2 double animation video super resolution zoom model\n\n * realesrganv2-animevideo-xsx4 quadruple animation video super resolution zoom model\n\n\n# load graphics card\n\nspecify which graphics card to use for super resolution\n\n\n# super resolution algorithm\n\nselect the algorithm used for super resolution\n\n\n# super resolution model multiple\n\nthe overresolution multiple of the currently selected model\n\n\n# transfer resolution ratio\n\nfirst scale the original video according to the percentage set by the user, and then perform super resolution.\n\n> example: original video 1920x1080, transfer resolution ratio 50%, model magnification 4x\n\nrunning process: 1920x1080 -> 960x540 (down scaling) -> 3840x2160 (super resolution)\n\n\n# realcugan cutting mode\n\ndedicated to realcugan, the more you cut, the more memory you save\n\n * no tile: no cutting is used\n\n * 1/2 on width: horizontally divided into two\n\n * 1/2 on both w and h: horizontally and vertically divided into two\n\n * 1/3 on w & h: cut lengthwise and lengthwise into thirds\n\n * 1/4 on w & h: cut into quarters lengthwise and lengthwise\n\n\n# realcugan low memory mode\n\ndedicated to realcugan, used when the video memory of the graphics card is insufficient\n\n * low vram mode: enable low vram mode\n\n * none: do not use low memory mode\n\n\n# cutting block size (it is not recommended to open when using realcugan)\n\n * there are presets for video memory size, and you can also choose to customize the adjustment\n\n\n# super-score sequence length\n\nit is only valid when a super resolution algorithm that requires multi-frame input, such as the basicvsr series, is selected\n\n * the larger the length of the super resolution sequence, the more frames are input for a single super resolution, and the texture is more stable, but at the same time it will increase the video memory usage,\n * it is recommended to keep the value above 10. if the video memory is insufficient, it is recommended to reduce the screen resolution and ensure that the value is above 5\n\n\n\n\n# super resolution using half precision\n\n * it is recommended to enable it, which can greatly reduce the video memory usage and have little impact on the picture quality.\n * for nvidia\'s 10xx series pascal architecture graphics card, it will slow down the super resolution speed\n\n\n# tta\n\nit is only supported by ncnncugan, which consumes a lot of time in exchange for a small improvement in image quality it is only supported by ncnncugan, which consumes a lot of time in exchange for a small improvement in image quality\n\n\n# fmnet - hdr10\n\ntip\n\nthis feature is only available in the public beta version\n\nconvert sdr video to hdr10 with ai algorithm\n\n\n# output settings (suppression parameter quality)\n\ntip\n\nonly for super resolution, please click one-click suppression to start the task; if you want to make supplementary frame super resolution at the same time, please click one-key supplementary frame\n\nwarning\n\nat the same time, performing supplementary frame super resolution will consume more video memory, and insufficient video memory may cause the task to fail.\n\nif the video memory is less than 10g, it is recommended to use one-key suppression to complete the super resolution first, and then perform frame supplementation.\n\n\n\n\n# render quality crf\n\nit is used to adjust the quality loss when exporting video, and it is positively correlated with output bit rate. using different compression codecs and compression presets will affect the crf.\n\ncrf numerical parameters are generally 16, which is not lossy to the naked eye at this time; for h.265 encoding, the bit rate will drop significantly. please judge whether the crf value is reasonable based on the image quality seen by the naked eye. ** if it is used as a favorite crf value parameter, it can be set to 12. **the smaller the value of crf, the smaller the image loss after the operation process, and the larger the volume (bit rate) of the exported finished video. **\n\nnote: the same value, the output quality of different encoders are different\n\n\n# target bit rate\n\nas an optional option to replace the rendering quality crf, it is basically the same as the setting standards of primer pro, after effects, and davinci resolve\n\n\n# encoder\n\n * auto automatically determine encoder options based on the slide bar below the software\n * cpu if you select this option, the quality is the highest, but the cpu usage is also the highest. the performance of the cpu determines whether it will be blocked during frame supplementation or super resolution (resulting in a decrease in graphics card usage), and the time to complete the final operation.\n * nvenc this option is only available for nvidia graphics cards that support nvenc function. if your graphics card does not support nvenc function, please do not select this option. please refer to the nvidia nvenc gen.pdf in the installation directory to check whether your graphics card supports nvenc\n * vce this option is only available for amd graphics cards that support vce function, if your graphics card does not support vce function, please do not select this option.\n * qsv this option is only supported by users with intel core graphics (such as intel uhd 630, irispro 580), and users who are not of this type should not choose it.\n\ntip\n\nthe following encoders need to purchase professional dlc\n\n * nvencc is an optimized version of nvenc, with faster processing speed and better quality of works.\n * qsvencc is an optimized version of qsv, which can complete tasks more efficiently.\n * vcencc is an optimized version of vce, which can complete tasks more efficiently.\n\nperceptual comparison:\n\nencoders   using hardware        speed    quality   file size   selection recommendations\ncpu        cpu                   medium   high      medium      pursue image quality and encoding stability and a card user\n                                                                au user selection\nnvenc      ncard                 fast     medium                \nqsv        intel core graphics   fast     medium                \n\n\n# select compression encoding\n\nfor the selection of this function, you need to have certain common sense of video suppression.\n\nwarning\n\nif you are new to suppression, please keep the following rules in mind:\n\n * hdr output must choose h.265 10bit encoding\n * be sure to choose h.265 encoding for resolutions above 2k (especially 4k, 8k resolutions)\n * if there are problems with both h.264 and h.265 encoding, use prores encoding. this encoding output is closest to the lossless naked eye, and the bit rate is extremely high. it is an intermediate encoding format for editing work.\n * it is recommended to use h.265 fast encoding or prores encoding\n\n\n# select suppress preset\n\n * cpu: english meaning the faster the speed, the lower the quality of the work, otherwise the higher the quality.\n\n * nvenc (only for n card): it is recommended to choose p7 without brain\n\n * qsv (for intel graphics card): select slow directly\n\n * vce (only for a card): directly select quality\n\n * nvencc (only for n card): directly select quality\n\n * qsvencc (for intel graphics card): choose best directly\n\n * vcencc (only for a card): directly select slow\n\n\n# nvidia card hard-coded preset\n\nwhen selecting the nvenc encoder, the n card hard-coded preset can reduce the exported video volume without changing the picture quality. you need to check which generation of nvenc compression chip your n card is. if it exceeds 7th, directly select 7th+\n\n\n# default suppression scheme\n\nusing the traditional compression scheme, the compatibility is strong, and the exported video volume may increase.\n\ntip\n\nenabling this feature can solve most broken pipe problems.\n\n\n# audio secondary pressure to aac\n\n * re-encode the audio (usually used on videos uploaded to the platform)\n * compress all audio tracks in the video to 640kbps aac format.\n\n\n# hdr strict mode\n\nhandle hdr content with strict presets, enabled by default\n\n\n# dv compatible with hdr10\n\nenable hdr10 compatibility when outputting in dolby vision, which is enabled by default\n\n\n# one-click hdr: convert sdr video to hdr10+\n\nfour one-click hdr modes need to try the effect by yourself\n\n\n# decoding quality control\n\n\n# use vspipe pre-decoding\n\nuse vspipe as a pre-decoder, this function is a prerequisite for many specific functions (such as decolorization, fast noise addition, qtgmc deinterlacing),\n\nif you find that he can\'t decode the input or the task reports an error, please close it\n\n\n# hardware decoding\n\nit can reduce the decoding pressure of high-resolution video, but it may reduce the picture quality to a certain extent, and cause the frame supplement module to explode video memory when the video memory is tight.\n\n\n# quick frame splitting\n\nfast frame splitting can reduce the decoding pressure, but it may result in deviations in the color of the picture.\n\n\n# high precision optimization workflow\n\ntip\n\nthis function requires the purchase of professional dlc\n\n * if the cpu performance is excessive, it is recommended that you enable this function, which can solve the color deviation problem of most pictures, and can solve the color deviation problem caused by hdr video compression to the greatest extent. this feature will increase the cpu burden, and even affect the speed of supplementary frames.\n * turning on this function for super resolution work will turn off half-precision (requires more video memory). please select as appropriate.\n\ntip\n\nit is recommended to enable this option when inputting hdr video\n\n\n# turn on deinterlacing\n\ntip\n\nthis function requires the purchase of professional dlc\n\n * use ffmpeg to deinterlace input interlaced video.\n\n * when using vspipe pre-decoding, use qtgmc to deinterlace the picture\n\n\n# depan (de-ribbon)\n\nuse depanstabilise in vs to handle ribbons\n\n\n# fast noise reduction\n\ntip\n\nthis function requires the purchase of professional dlc\n\nif there is no special need for the "quick" option under this column, please keep it off, otherwise it will slow down the task processing speed.\n\ntip\n\nit is recommended to test whether this option can improve the picture quality by controlling variables by yourself.\n\nincompatible with high precision optimization workflows\n\n\n# add noise quickly\n\nadd noise to video, often used for video overtime\n\n\n# custom frame splitting parameters (professional option)\n\nused to replace the parameters used by ffmpeg or vspipe for decoding, use || intervals between custom parameters\n\n\n# custom encoding settings\n\n\n# specify the number of encoding threads\n\nwhen the encoder is a cpu, there is a chance to control the cpu usage to control the rendering speed.\n\n\n# custom suppression parameters\n\nthis function is a professional option (note that the number of input items must be even number),\n\nthe key value is separated by ||\n\n> example: custom suppression parameters for cpu h265 suppression:\n> \n> -x265-params||ref=4:me=3:subme=4:rd=4:merange=38:rdoq-level=2:rc-lookahead=40:scenecut=40:strong-intra-smoothing=0\n\n\n# time remapping: change the speed of the video\n\ntip\n\nthis function requires the purchase of professional dlc\n\n * this function is used to make "slow motion" clips.\n\n * for example, if the output frame rate is set to 120 frames, and the time remapping is set to 60 frames, the output effect is equivalent to 50% playback speed slowed down.\n\n * other situations can be analogized in turn, you can set the output frame rate by yourself, decimals are supported.\n\nwarning\n\nfor animation materials, please enable space-time resampling in video fluency optimization in fixed frame settings as much as possible.\n\nor use software such as premiere to reduce the frame rate of the original video to complete the removal of duplicate frames to avoid stuttering after remapping.\n\nthe original video frame rate is generally reduced to 8 or 12 frames\n\n\n# head and tail loop\n\nputting the last frame in the first frame to fit some end-to-end looping videos\n\n\n# io control\n\n\n\n\n# manually specify the buffer memory size\n\nif the running memory is tight (less than 16g), it is recommended to manually specify the buffer memory size to be 2-3g to avoid out of memory errors.\n\n\n# single output block size\n\n * for frame complementing and compression tasks, a small clip without audio will be output every time the number of frames rendered is this value, so that you can preview the effect conveniently.\n * the clips will be generated in the output folder you set, and merged into one file after the frame or compression task is completed.\n\n\n# keep the project folder after the task is completed\n\ndo not delete the chunk video generated in the middle after the supplementary frame is completed.\n\n\n# vfi setting\n\n\n# safe frame rate\n\nif the video is to be uploaded to the corresponding media platform for online viewing, please enable this option\n\ntip\n\nif you find a blurred video screen when playing a video, it is most likely a problem with the decoder. please try to change the decoder or check this option to reduce the decoding pressure.\n\n\n# reverse optical flow\n\nthis function can make the picture more silky to a certain extent.\n\n\n# absolutely smooth\n\nthis function may make the screen more silky, just enable it by default (if it is not available, it will not be enabled when the software is running)\n\n\n# optical flow scale\n\n * when using the rife algorithm, when the original video size is 1080p, the default setting is 0.5, 4k and above is 0.25, and less than 1080p is 1.0\n\n * when using the gmfss algorithm, when the original video size is 1080p, 1.0 is set by default, 0.5 is set for 4k and above, and 1.0 is set for less than 1080p\n\n\n# interlaced frame interpolation\n\n * equivalent to a special cut, used to reduce video memory usage\n\n * appropriate selection of this item can make the graphics card with small video memory supplement the super-large resolution (such as 4g complement 8k)\n\n\n# video fluency optimization\n\n * space-time linearization: enhance the smoothness of the picture when the output frame rate is 60 (trumotion)( general )\n * fixed threshold deduplication: it is used to alleviate the picture stuttering caused by repeated frames. the general value is 0.2, and 0.5 is used for animation, 1.0 or higher (general)\n * remove 1 shot 2: specially for animation, some materials perform better, but in most cases, duplicate frames cannot be completely removed, and it is easy to introduce stuttering. (anime) (outdated, use spatio-temporal resampling is recommended)\n * space-time resampling: completely remove the stuttering of animation video materials, please ensure that the input frame rate is around 24, and the output frame rate can only be an integer multiple of (input frame rate/2)**(anime) (only supported algorithms and frame complement models available at any time) **\n * first-order difference deduplication: anime deduplication (obsolete)\n\nwarning\n\ndue to the limited ability of ai frame supplementation in animation frame supplementation at this stage, choosing deduplication will increase the motion range between frames, resulting in picture distortion when supplementing frames. please choose the best deduplication for each input video control variable by yourself. heavy mode.\n\nit is recommended that you choose the deduplication mode carefully. if you need to supplement frames for the entire animation, it is not recommended to enable deduplication.\n\nafter enabling video fluency optimization (time-space resampling), the effect of supplementary frame is as follows\n\n\n\n\n\n# load graphics card\n\nspecify which graphics card to use for supplementary frames\n\n\n# frame interpolation algorithm\n\nselect the frame interpolation algorithm (including rife, ifrnet, dain, gmfss, ema)\n\n\n# vfi model\n\ntip\n\nmodels with ncnn use ncnn as the forward reasoning framework, which is compatible with n cards and a cards, and models without this word cannot be used for a card and display.\n\n * rife: high-speed, popular new-age supplementary frame algorithm (the following is the model introduction)\n\n> 2.3: classic, hot model, fast speed and good effect.\n> \n> 3.8: (two-way optical flow must be turned on), the quality is better and clear.\n> \n> 4.4: slightly lower quality, super fast.\n> \n> 4.5: the quality is close to or even surpasses 2.3, with higher fluency and faster speed.\n> \n> 4.6: an evolution of the 4.5 model, recommended.\n> \n> rpr_v7_1.0: combined model, blurred, improved fluency.\n> \n> rpr_v7_2.3: combination model with improved fluency.\n> \n> rpr_v7_2.3_ultra: combined model, more suitable for complex images.\n> \n> rpr_v7_2.3_ultra#2: combined model, more suitable for complex images.\n\n * ncnn-rife: supports various graphics card versions of rife, good compatibility, fast speed, and slightly worse quality than rife.\n\n * ifrnet: real shot & animation model, faster speed, lower quality than rife model, not recommended.\n\n * ncnn_dain: traditional old algorithm, animation real shooting can be used, support any time, the speed is very slow, and the fluency is very high.\n\n * gmfss: experimental new algorithm, slow speed, high quality (the model is introduced below) (the model marked with trt is an accelerated model)\n\n> union_v: the third-generation gmfss model, the most powerful animation supplementary frame model, with stable structure and smooth picture\n> \n> union_w: the third-generation gmfss model, currently the most powerful animation supplementary frame model, the picture is clear and clean\n> \n> pg104: the fourth generation of gmfss animation model (experimental effect is better than union_v)\n> \n> real: gmfss second generation real shot model\n> \n> primaris: gmfss second generation animation model\n> \n> up: the first generation of gmfss model, the speed is very fast, the text may flicker\n> \n> basic: the first generation gmfss model, the speed is very slow, the effect may be more stable than up\n\n * ema: cvpr 2023 sota real-time supplementary frame algorithm\n\n> ema_ours_t officially supports the full blood model at any time\n> \n> ema_outs official full blood model\n> \n> ema_ours_small_t official fast model that supports any time\n> \n> quick model given by ema_outs_small official\n\n\n# tta mode\n\ntip\n\nthis function requires the purchase of professional dlc\n\n> enable this function to reduce picture jelly, reduce subtitle jitter, and reduce the problem of objects disappearing. make the picture more smooth and comfortable\n> \n> additional frame supplement time is required, and some frame supplement models do not support this function.\n> \n> the bigger the number behind, the slower the speed, and the less jelly, generally fill in 1 or 2.\n> \n> sideways, suitable for rife, 3.x series models\n> \n> medium orientation, suitable for rife, 2.x series models\n\n\n# output layer fine-tuning mode (only available for experimental models)\n\n> residual: it will make the picture blurred, but the structure is more complete\n> \n> direct: direct output, the picture is clearer\n\n\n# bidirectional optical flow\n\n> the speed is reduced by about half, and the effect is slightly improved (the rife 3.8 model must be turned on) (the rife 4.x model is not supported yet).\n\n\n# dynamic optical flow scale\n\ntip\n\nthis function requires the purchase of professional dlc\n\n> dynamically select the optical flow scale during the frame complementing process, which can reduce the problem of object disappearance and reduce jelly. it is recommended to enable it.\n\n\n# customize preset column\n\n\n# create a new preset based on the current settings\n\nafter giving the preset a name, click to create a new preset\n\n\n# remove current preset\n\ndelete the currently selected preset\n\n\n# apply specific presets\n\nload previously saved presets, automatically load parameters\n\n\n# toolbox\n\n\n# convert video to gif animation\n\ngenerate high-quality animated gifs\n\n\n# loop animation\n\ngenerate a circular animation, it is recommended to keep the default.\n\n\n# merge existing blocks\n\nmerge the scattered chunk fragments.\n\ntip\n\nif the task failed during the final merge, you can directly select the task after adjusting the settings and click this button to complete the merge operation.\n\n\n# audio and video merge\n\n * fill in the full path of the video (eg: d:\\01\\myvideo.mp4)\n\n * fill in the audio path of the video (eg: d:\\01\\myvideo.aac)\n\n * or use a video to input audio (eg: d:\\01\\othervideo.mp4)\n\n * output video path (eg: d:\\01\\output.mp4)\n\n\n# export the current settings to a text file\n\nexport settings information as a text document.\n\ntip\n\nif the video output of the software does not meet expectations, such as color cast, poor effect, etc., you can click this button and send the setting file to the developer to locate the problem.\n\n\n# debug\n\noutput debug information while the task is in progress\n\n\n# title bar function\n\n\n\n\n# preferences\n\n\n# multitasking rest interval\n\ngive the device a break every x hours (shortly pause the task)\n\n\n# select cache folder\n\nspecify the task folder to a different location. the final output video will still be in the destination folder\n\n\n# after the completion of the supplementary frame task\n\nyou can choose some automatic operations after frame completion\n\n\n# unavailable features\n\nforced use of cpu for frame-fill superscore\n\n\n# enable expert mode\n\nenabled by default, display all functions\n\n\n# enable parameter text preview before task\n\nbefore clicking to start the supplementary frame, a pop-up box will pop up, you can browse through it to confirm that the parameter settings are correct, and then perform supplementary frame or super resolution\n\n\n# clear the task list after the task is completed\n\nclear the queue after all tasks in the list are completed\n\n\n# use global settings\n\nunified parameter setting for all tasks\n\n\n# reckless exit\n\nenabled by default, forcibly end the software process\n\ntip\n\nif you find that the video memory is occupied after closing the software, it is recommended to enable this option\n\n\n# original flavor suppression mode\n\nwhen suppressing tasks alone, operations such as deduplication of repeated frames are not enabled\n\n\n# open preview\n\nopening the preview window when supplementing frames will slow down the running speed of the program to a certain extent\n\n\n# automatic error correction\n\nautomatically modify settings to prevent task errors\n\n\n# enable quiet mode\n\nno prompt box pops up when the software starts',charsets:{cjk:!0}},{title:"Advanced Settings of Command Line Interface",frontmatter:{title:"Advanced Settings of Command Line Interface",date:"2022-08-28T22:23:12.000Z",permalink:"/pages/ceb849/"},regularPath:"/20.Manuals/21.Advanced%20CLI.html",relativePath:"20.Manuals/21.Advanced CLI.md",key:"v-70336c66",path:"/pages/ceb849/",headers:[{level:2,title:"Environment construction",slug:"environment-construction",normalizedTitle:"environment construction",charIndex:165},{level:2,title:"Description of typical usage scenarios",slug:"description-of-typical-usage-scenarios",normalizedTitle:"description of typical usage scenarios",charIndex:3231},{level:3,title:"Open more SVFI",slug:"open-more-svfi",normalizedTitle:"open more svfi",charIndex:3274},{level:3,title:"Pipeline input",slug:"pipeline-input",normalizedTitle:"pipeline input",charIndex:3903},{level:3,title:"Pipeline output",slug:"pipeline-output",normalizedTitle:"pipeline output",charIndex:4297},{level:3,title:"Command line example",slug:"command-line-example",normalizedTitle:"command line example",charIndex:4848},{level:4,title:"Use ffmpeg for high-precision frame splitting and input SVFI for processing, use Y4M input ffmpeg for compression",slug:"use-ffmpeg-for-high-precision-frame-splitting-and-input-svfi-for-processing-use-y4m-input-ffmpeg-for-compression",normalizedTitle:"use ffmpeg for high-precision frame splitting and input svfi for processing, use y4m input ffmpeg for compression",charIndex:4872},{level:4,title:"Use ffmpeg for high-precision frame splitting and input the SVFI pipeline with a length of 2 for processing, output Y4M and use ffmpeg for compression",slug:"use-ffmpeg-for-high-precision-frame-splitting-and-input-the-svfi-pipeline-with-a-length-of-2-for-processing-output-y4m-and-use-ffmpeg-for-compression",normalizedTitle:"use ffmpeg for high-precision frame splitting and input the svfi pipeline with a length of 2 for processing, output y4m and use ffmpeg for compression",charIndex:5547},{level:4,title:"Use Vapoursynth to preprocess the input and then use ffmpeg to perform high-precision frame splitting and SVFI pipeline processing",slug:"use-vapoursynth-to-preprocess-the-input-and-then-use-ffmpeg-to-perform-high-precision-frame-splitting-and-svfi-pipeline-processing",normalizedTitle:"use vapoursynth to preprocess the input and then use ffmpeg to perform high-precision frame splitting and svfi pipeline processing",charIndex:6568}],headersStr:"Environment construction Description of typical usage scenarios Open more SVFI Pipeline input Pipeline output Command line example Use ffmpeg for high-precision frame splitting and input SVFI for processing, use Y4M input ffmpeg for compression Use ffmpeg for high-precision frame splitting and input the SVFI pipeline with a length of 2 for processing, output Y4M and use ffmpeg for compression Use Vapoursynth to preprocess the input and then use ffmpeg to perform high-precision frame splitting and SVFI pipeline processing",content:"Warning\n\nThe description on this page belongs to advanced content, please read Advanced Content Detailed Explanation\n\nSVFI supports using command line interface\n\n\n# Environment construction\n\n 1. Create a new steam_appid.txt in the SVFI installation root directory and fill in the following content\n\n> 1692080\n\nTip\n\nFilling out this text file will cause SVFI to start in standalone application mode to avoid interference from the Steam client.\n\nSpecial operations such as multi-opening of software and migration of software installation locations can be realized.\n\n 2. Start the command prompt cmd in the root directory of the SVFI installation (if the installation location is not on the system disk, administrator privileges are not required), enter one_line_shot_args.exe -h and press Enter, you should be able to see something similar to the following:\n\nusage: #### SVFI CLI tool by Jeanna #### [-h] -i INPUT -c CONFIG -t TASK_ID\n                                         [--concat-only] [--extract-only]\n                                         [--render-only] [-p] [--pipe-in]\n                                         [--pipe-out]\n                                         [--pipe-iw PIPE_IN_WIDTH]\n                                         [--pipe-ih PIPE_IN_HEIGHT]\n                                         [--pipe-in-fps PIPE_IN_FPS]\n                                         [--pipe-in-pixfmt {rgb24,rgb48be,rgb48le,rgb48}]\n                                         [--pipe-rgb]\n                                         [--pipe-colormatrix {470bg,170m,2020ncl,709}]\n\nTo enhance Long video/image sequence quality\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nBasic Settings:\n  -i INPUT, --input INPUT\n                        Path of input video/image sequence folder\n  -c CONFIG, --config CONFIG\n                        Path of config\n  -t TASK_ID, --task-id TASK_ID\n                        13-digit Task id\n  --concat-only         Concat Chunk only\n  --extract-only        Extract input to frames Only\n  --render-only         Render only\n  -p, --preview         Preview Settings\n\nPipe Settings:\n  Set the follow parameters when '-mid' is assigned, or you will encounter\n  exceptions.Output Y4M at YUV444P10\n\n  --pipe-in             This enables OLS to obtain input data from stdin\n  --pipe-out            This enables OLS to pipe output to stdout\n  --pipe-iw PIPE_IN_WIDTH\n                        Width of input raw RGB, effective when --pipe-in\n                        appointed\n  --pipe-ih PIPE_IN_HEIGHT\n                        Height of input raw RGB, effective when --pipe-in\n                        appointed\n  --pipe-in-fps PIPE_IN_FPS\n                        Input stream FPS, effective when --pipe-in appointed\n  --pipe-in-pixfmt {rgb24,rgb48be,rgb48le,rgb48}\n                        Pixel format of input raw RGB, effective when --pipe-\n                        in appointed\n  --pipe-rgb            Pipe RGB Raw data to stdout, effective when --pipe-out\n                        appointed\n  --pipe-colormatrix {470bg,170m,2020ncl,709}\n                        Colormatrix for RGB-YUV Conversion, effective when\n                        --pipe-in appointed, --pipe-rgb not appointed\n\n\n\n\n\n# Description of typical usage scenarios\n\n\n# Open more SVFI\n\nThe one_line_shot_args.exe (hereinafter referred to as OLS) program supports multiple openings after the above configuration is completed, and is used for multi-process tasks. Please note that this may cause a huge resource occupation.\n\nThere are three required parameters that need to be entered:\n\n * --input: The folder where the video file or image sequence to be processed is located\n * --config: The configuration file generated by the SVFI GUI program, generally located in the Configs folder of the installation directory\n * --task-id: task ID, a non-empty string, used to distinguish different tasks.\n\n\n# Pipeline input\n\nSVFI supports RGB stream input from other processes. Currently, it supports raw stream input in four pixel formats rgb24, rgb48, rgb48le, rgb48be.\n\n--pipe-in must be filled in to enable this feature, and additionally specify --pipe-iw input image pixel length, --pipe-ih width, --pipe-in-fps input stream frame Rate (float only), the pixel format of --pipe-in-pixfmt input.\n\n\n# Pipeline output\n\nSVFI supports outputting RGB or Y4M streams of YUV444P10 to stdout.\n\n--pipe-out must be filled in to enable this feature,\n\n * If you need to output YUV444P10, please specify --pipe-colormatrix for SVFI to convert RGB to YUV stream\n * If you want to output the RGB stream, please specify --pipe-rgb, the specific output pixel format is controlled by the options of the configuration file, generally speaking, rgb48 is output when high-precision workflow is enabled, and rgb48 is output for non-high-precision workflow output rgb24\n\n\n# Command line example\n\n# Use ffmpeg for high-precision frame splitting and input SVFI for processing, use Y4M input ffmpeg for compression\n\nffmpeg -loglevel error -vsync passthrough -hwaccel auto -i test/test.mp4 -map 0:v:0 -sws_flags +bicubic+full_chroma_int+accurate_rnd -vf copy,format=yuv444p10le,format=rgb48be,format=rgb24,minterpolate=fps=24.000:mi_mode=dup -f image2pipe -pix_fmt rgb24 -vcodec rawvideo - |  one_line_shot_args.exe  -i - -c Configs/SVFI_Config_pipe_test.ini -t pipe_2 --pipe-in --pipe-iw 960 --pipe-ih 540  --pipe-in-fps 24 --pipe-out |  ffmpeg.exe -loglevel error -hide_banner -y -vsync cfr -i - -preset:v slow -c:v hevc_nvenc -pix_fmt yuv420p -crf 16 test/output.mp4 -y\n\n\n# Use ffmpeg for high-precision frame splitting and input the SVFI pipeline with a length of 2 for processing, output Y4M and use ffmpeg for compression\n\nThe first OLS process is used for super resolution, and the second OLS process is used for supplementary frame\n\nffmpeg -loglevel error -vsync passthrough -hwaccel auto -i test/test.mp4 -map 0:v:0 -sws_flags +bicubic+full_chroma_int+accurate_rnd -vf copy,format=yuv444p10le,format=rgb48be,format=rgb24,minterpolate=fps=24.000:mi_mode=dup -f image2pipe -pix_fmt rgb48be -vcodec rawvideo - | one_line_shot_args.exe  -i - -c Configs/SVFI_Config_pipe_1.ini -t pipe_1 --pipe-in --pipe-iw 960 --pipe-ih 540 --pipe-in-fps 24 --pipe-out --pipe-rgb --pipe-in-pixfmt rgb48be | one_line_shot_args.exe -i - -c Configs/SVFI_Config_pipe_2.ini -t pipe_2 --pipe-in --pipe-iw 960 --pipe-ih 540  --pipe-in-fps 24 --pipe-in-pixfmt rgb48 --pipe-out |  ffmpeg.exe -loglevel error -hide_banner -y -vsync cfr -i - -preset:v slow -c:v hevc_nvenc -pix_fmt yuv420p -crf 16 test/output.mp4 -y\n\n\n# Use Vapoursynth to preprocess the input and then use ffmpeg to perform high-precision frame splitting and SVFI pipeline processing\n\nThe Vapoursynth script input.vpy used:\n\nimport vapoursynth as vs\nfrom vapoursynth import core\nvideo = core.lsmas.LWLibavSource(r'test.mp4')  # 960x540, 24fps\n\n# DO SOMETHING\n\nvideo.set_output()  # output yuv\n\n\nCommand Line:\n\nvspipe input.vpy --y4m - | ffmpeg -loglevel error -vsync passthrough -hwaccel auto -i - -map 0:v:0 -sws_flags +bicubic+full_chroma_int+accurate_rnd -vf copy,format=yuv444p10le,format=rgb48be,format=rgb24,minterpolate=fps=24.000:mi_mode=dup -f image2pipe -pix_fmt rgb24 -vcodec rawvideo - |  one_line_shot_args.exe  -i - -c Configs/SVFI_Config_pipe_test.ini -t pipe_2 --pipe-in --pipe-iw 960 --pipe-ih 540  --pipe-in-fps 24 --pipe-out |  ffmpeg.exe -loglevel error -hide_banner -y -vsync cfr -i - -preset:v slow -c:v hevc_nvenc -pix_fmt yuv420p -crf 16 test/output.mp4 -y\n\n\nTip\n\nAfter using SVFI to configure task parameters and start the task, use Config.ini generated under the Configs folder to perform command line tasks",normalizedContent:"warning\n\nthe description on this page belongs to advanced content, please read advanced content detailed explanation\n\nsvfi supports using command line interface\n\n\n# environment construction\n\n 1. create a new steam_appid.txt in the svfi installation root directory and fill in the following content\n\n> 1692080\n\ntip\n\nfilling out this text file will cause svfi to start in standalone application mode to avoid interference from the steam client.\n\nspecial operations such as multi-opening of software and migration of software installation locations can be realized.\n\n 2. start the command prompt cmd in the root directory of the svfi installation (if the installation location is not on the system disk, administrator privileges are not required), enter one_line_shot_args.exe -h and press enter, you should be able to see something similar to the following:\n\nusage: #### svfi cli tool by jeanna #### [-h] -i input -c config -t task_id\n                                         [--concat-only] [--extract-only]\n                                         [--render-only] [-p] [--pipe-in]\n                                         [--pipe-out]\n                                         [--pipe-iw pipe_in_width]\n                                         [--pipe-ih pipe_in_height]\n                                         [--pipe-in-fps pipe_in_fps]\n                                         [--pipe-in-pixfmt {rgb24,rgb48be,rgb48le,rgb48}]\n                                         [--pipe-rgb]\n                                         [--pipe-colormatrix {470bg,170m,2020ncl,709}]\n\nto enhance long video/image sequence quality\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nbasic settings:\n  -i input, --input input\n                        path of input video/image sequence folder\n  -c config, --config config\n                        path of config\n  -t task_id, --task-id task_id\n                        13-digit task id\n  --concat-only         concat chunk only\n  --extract-only        extract input to frames only\n  --render-only         render only\n  -p, --preview         preview settings\n\npipe settings:\n  set the follow parameters when '-mid' is assigned, or you will encounter\n  exceptions.output y4m at yuv444p10\n\n  --pipe-in             this enables ols to obtain input data from stdin\n  --pipe-out            this enables ols to pipe output to stdout\n  --pipe-iw pipe_in_width\n                        width of input raw rgb, effective when --pipe-in\n                        appointed\n  --pipe-ih pipe_in_height\n                        height of input raw rgb, effective when --pipe-in\n                        appointed\n  --pipe-in-fps pipe_in_fps\n                        input stream fps, effective when --pipe-in appointed\n  --pipe-in-pixfmt {rgb24,rgb48be,rgb48le,rgb48}\n                        pixel format of input raw rgb, effective when --pipe-\n                        in appointed\n  --pipe-rgb            pipe rgb raw data to stdout, effective when --pipe-out\n                        appointed\n  --pipe-colormatrix {470bg,170m,2020ncl,709}\n                        colormatrix for rgb-yuv conversion, effective when\n                        --pipe-in appointed, --pipe-rgb not appointed\n\n\n\n\n\n# description of typical usage scenarios\n\n\n# open more svfi\n\nthe one_line_shot_args.exe (hereinafter referred to as ols) program supports multiple openings after the above configuration is completed, and is used for multi-process tasks. please note that this may cause a huge resource occupation.\n\nthere are three required parameters that need to be entered:\n\n * --input: the folder where the video file or image sequence to be processed is located\n * --config: the configuration file generated by the svfi gui program, generally located in the configs folder of the installation directory\n * --task-id: task id, a non-empty string, used to distinguish different tasks.\n\n\n# pipeline input\n\nsvfi supports rgb stream input from other processes. currently, it supports raw stream input in four pixel formats rgb24, rgb48, rgb48le, rgb48be.\n\n--pipe-in must be filled in to enable this feature, and additionally specify --pipe-iw input image pixel length, --pipe-ih width, --pipe-in-fps input stream frame rate (float only), the pixel format of --pipe-in-pixfmt input.\n\n\n# pipeline output\n\nsvfi supports outputting rgb or y4m streams of yuv444p10 to stdout.\n\n--pipe-out must be filled in to enable this feature,\n\n * if you need to output yuv444p10, please specify --pipe-colormatrix for svfi to convert rgb to yuv stream\n * if you want to output the rgb stream, please specify --pipe-rgb, the specific output pixel format is controlled by the options of the configuration file, generally speaking, rgb48 is output when high-precision workflow is enabled, and rgb48 is output for non-high-precision workflow output rgb24\n\n\n# command line example\n\n# use ffmpeg for high-precision frame splitting and input svfi for processing, use y4m input ffmpeg for compression\n\nffmpeg -loglevel error -vsync passthrough -hwaccel auto -i test/test.mp4 -map 0:v:0 -sws_flags +bicubic+full_chroma_int+accurate_rnd -vf copy,format=yuv444p10le,format=rgb48be,format=rgb24,minterpolate=fps=24.000:mi_mode=dup -f image2pipe -pix_fmt rgb24 -vcodec rawvideo - |  one_line_shot_args.exe  -i - -c configs/svfi_config_pipe_test.ini -t pipe_2 --pipe-in --pipe-iw 960 --pipe-ih 540  --pipe-in-fps 24 --pipe-out |  ffmpeg.exe -loglevel error -hide_banner -y -vsync cfr -i - -preset:v slow -c:v hevc_nvenc -pix_fmt yuv420p -crf 16 test/output.mp4 -y\n\n\n# use ffmpeg for high-precision frame splitting and input the svfi pipeline with a length of 2 for processing, output y4m and use ffmpeg for compression\n\nthe first ols process is used for super resolution, and the second ols process is used for supplementary frame\n\nffmpeg -loglevel error -vsync passthrough -hwaccel auto -i test/test.mp4 -map 0:v:0 -sws_flags +bicubic+full_chroma_int+accurate_rnd -vf copy,format=yuv444p10le,format=rgb48be,format=rgb24,minterpolate=fps=24.000:mi_mode=dup -f image2pipe -pix_fmt rgb48be -vcodec rawvideo - | one_line_shot_args.exe  -i - -c configs/svfi_config_pipe_1.ini -t pipe_1 --pipe-in --pipe-iw 960 --pipe-ih 540 --pipe-in-fps 24 --pipe-out --pipe-rgb --pipe-in-pixfmt rgb48be | one_line_shot_args.exe -i - -c configs/svfi_config_pipe_2.ini -t pipe_2 --pipe-in --pipe-iw 960 --pipe-ih 540  --pipe-in-fps 24 --pipe-in-pixfmt rgb48 --pipe-out |  ffmpeg.exe -loglevel error -hide_banner -y -vsync cfr -i - -preset:v slow -c:v hevc_nvenc -pix_fmt yuv420p -crf 16 test/output.mp4 -y\n\n\n# use vapoursynth to preprocess the input and then use ffmpeg to perform high-precision frame splitting and svfi pipeline processing\n\nthe vapoursynth script input.vpy used:\n\nimport vapoursynth as vs\nfrom vapoursynth import core\nvideo = core.lsmas.lwlibavsource(r'test.mp4')  # 960x540, 24fps\n\n# do something\n\nvideo.set_output()  # output yuv\n\n\ncommand line:\n\nvspipe input.vpy --y4m - | ffmpeg -loglevel error -vsync passthrough -hwaccel auto -i - -map 0:v:0 -sws_flags +bicubic+full_chroma_int+accurate_rnd -vf copy,format=yuv444p10le,format=rgb48be,format=rgb24,minterpolate=fps=24.000:mi_mode=dup -f image2pipe -pix_fmt rgb24 -vcodec rawvideo - |  one_line_shot_args.exe  -i - -c configs/svfi_config_pipe_test.ini -t pipe_2 --pipe-in --pipe-iw 960 --pipe-ih 540  --pipe-in-fps 24 --pipe-out |  ffmpeg.exe -loglevel error -hide_banner -y -vsync cfr -i - -preset:v slow -c:v hevc_nvenc -pix_fmt yuv420p -crf 16 test/output.mp4 -y\n\n\ntip\n\nafter using svfi to configure task parameters and start the task, use config.ini generated under the configs folder to perform command line tasks",charsets:{cjk:!0}},{title:"Q&A",frontmatter:{title:"Q&A",date:"2020-05-25T12:01:52.000Z",permalink:"/pages/9cc27d",article:!1},regularPath:"/30.FAQ/01.Q&A.html",relativePath:"30.FAQ/01.Q&A.md",key:"v-54036cfc",path:"/pages/9cc27d/",headers:[{level:3,title:"The software prompts that the video memory is insufficient or how to solve the problem of insufficient memory",slug:"the-software-prompts-that-the-video-memory-is-insufficient-or-how-to-solve-the-problem-of-insufficient-memory",normalizedTitle:"the software prompts that the video memory is insufficient or how to solve the problem of insufficient memory",charIndex:2},{level:3,title:"Is it possible to restore the progress if the frame complement is quit in the middle? Can't find progress?",slug:"is-it-possible-to-restore-the-progress-if-the-frame-complement-is-quit-in-the-middle-can-t-find-progress",normalizedTitle:"is it possible to restore the progress if the frame complement is quit in the middle? can't find progress?",charIndex:1066},{level:3,title:"What should I do if the supplementary frame effect is not silky",slug:"what-should-i-do-if-the-supplementary-frame-effect-is-not-silky",normalizedTitle:"what should i do if the supplementary frame effect is not silky",charIndex:2e3},{level:3,title:"What to do if the image of the exported video is noisy",slug:"what-to-do-if-the-image-of-the-exported-video-is-noisy",normalizedTitle:"what to do if the image of the exported video is noisy",charIndex:2313},{level:3,title:"What should I do if the graphics card usage is low?",slug:"what-should-i-do-if-the-graphics-card-usage-is-low",normalizedTitle:"what should i do if the graphics card usage is low?",charIndex:2516},{level:3,title:"What should I do if the frame-filling video is distorted?",slug:"what-should-i-do-if-the-frame-filling-video-is-distorted",normalizedTitle:"what should i do if the frame-filling video is distorted?",charIndex:3001},{level:3,title:"Will supplementing the frame for a long time hurt the graphics card",slug:"will-supplementing-the-frame-for-a-long-time-hurt-the-graphics-card",normalizedTitle:"will supplementing the frame for a long time hurt the graphics card",charIndex:3751},{level:3,title:"What to do if the video memory is not enough",slug:"what-to-do-if-the-video-memory-is-not-enough",normalizedTitle:"what to do if the video memory is not enough",charIndex:4229},{level:3,title:"What about Broken Pipe",slug:"what-about-broken-pipe",normalizedTitle:"what about broken pipe",charIndex:4584},{level:3,title:"Solve the abnormal size of SVFI window",slug:"solve-the-abnormal-size-of-svfi-window",normalizedTitle:"solve the abnormal size of svfi window",charIndex:4791},{level:3,title:"Fewer features available than in the tutorial?",slug:"fewer-features-available-than-in-the-tutorial",normalizedTitle:"fewer features available than in the tutorial?",charIndex:5082},{level:3,title:"How to only perform super resolution without supplementing frames at the same time",slug:"how-to-only-perform-super-resolution-without-supplementing-frames-at-the-same-time",normalizedTitle:"how to only perform super resolution without supplementing frames at the same time",charIndex:5287},{level:3,title:"How to make slow motion supplementary frames (upgrade, etc.)",slug:"how-to-make-slow-motion-supplementary-frames-upgrade-etc",normalizedTitle:"how to make slow motion supplementary frames (upgrade, etc.)",charIndex:5536}],headersStr:"The software prompts that the video memory is insufficient or how to solve the problem of insufficient memory Is it possible to restore the progress if the frame complement is quit in the middle? Can't find progress? What should I do if the supplementary frame effect is not silky What to do if the image of the exported video is noisy What should I do if the graphics card usage is low? What should I do if the frame-filling video is distorted? Will supplementing the frame for a long time hurt the graphics card What to do if the video memory is not enough What about Broken Pipe Solve the abnormal size of SVFI window Fewer features available than in the tutorial? How to only perform super resolution without supplementing frames at the same time How to make slow motion supplementary frames (upgrade, etc.)",content:'# The software prompts that the video memory is insufficient or how to solve the problem of insufficient memory\n\nInsufficient video memory:\n\n * Frame supplement 1080P video requires at least 2G video memory, 4K video requires at least 6G video memory. Please try to enable interleaved frames to avoid exceeding the video memory limit.\n\n * If you are sure that the video memory of your machine is large enough, you can try restart the machine to see if it solves the problem.\n\n * In the output resolution setting, to reduce the video resolution, you can try to turn on the half-precision mode in the output quality setting. You can also try to lower the optical flow scale in the supplementary frame setting (setting 2.0 and above will greatly increase the memory usage, please be sure to check the value of this option).\n\nNot enough storage:\n\n * For machines with a running memory below 16G, it is recommended to use the Manually specify the buffer memory size function in Output Quality Settings to specify the memory usage size. Recommended minimum value: 2G.\n\n\n# Is it possible to restore the progress if the frame complement is quit in the middle? Can\'t find progress?\n\n * To Work Status Recovery midpoint Auto Find Progress.\n * If the software displays "No work progress found" after clicking this button, please follow the steps below to troubleshoot the problem:\n   * Verify that the output folder exists\n   * Confirm that there is a video file named chunk-xxx-yyyyyy-zzzzzz in the output folder, if it does not exist, it means that there is no unrecoverable work progress\n   * Confirm that the last 10 digits of the output folder are the same as the task id of the input file, if not, please manually change the task id and click the button to try again.\n   * If the problem still cannot be solved, please move the mouse over the option button, read the option description carefully, and follow the prompts to fill in appropriate values in Start Block Count and Start Input Frame Number.\n\n\n# What should I do if the supplementary frame effect is not silky\n\n * In this case, there may be duplicate frames in the original video, or animation (with 1 beat N), you can try to enable duplicate frame removal in the Transition Recognition and Animation Optimization Tab, and adjust the deduplication value.\n\n\n# What to do if the image of the exported video is noisy\n\n * Before complementing the frame, go to the encoding quality setting lower the rendering quality CRF value, or adjust the compression preset.\n\n\n# What should I do if the graphics card usage is low?\n\n * First, in the task manager, click the small triangle next to the 3D occupancy rate and replace it with CUDA.\n * CUDA usage is generally around 85% or higher is normal.\n * If the usage is still low, please check whether the CPU utilization reaches 100%; if it reaches 100%, it means that the software encounters a CPU bottleneck. In this case, you can adjust other options until the graphics card usage reaches more than 80%.\n\n\n# What should I do if the frame-filling video is distorted?\n\n * If this happens in animation supplementary frames, it is recommended to turn off the deduplication frame, or reduce the deduplication value, or try to increase the optical flow scale to reduce the possibility of picture jelly. **\n * If you have the skills related to video editing, you can patiently use different parameters to add more frames to produce several videos, and splicing each segment with an excellent result.\n * If the embedded subtitles (hard subtitles, burned subtitles) are distorted, there is currently no better solution. If you want to add frames to animations, it is recommended to use resources without subtitles to add frames, and then find subtitles to embed.\n\n\n# Will supplementing the frame for a long time hurt the graphics card\n\n * Running CUDA for a long time will generally not affect the life of the graphics card, but if the heat dissipation measures are not done well, the temperature is too high, or running the frame recovery program after overclocking will still cause damage to the graphics card.\n * According to the EULA (Software User Agreement), SVFI is not responsible for hardware damage caused by supplementary frames.\n\n\n# What to do if the video memory is not enough\n\n * The more video memory is occupied, the faster the speed will be. Even if the video memory is fully occupied by running SVFI programs, the speed may not be improved, because there are many other limiting factors such as the length of the rendering queue and the limitation of machine power consumption.\n\n\n# What about Broken Pipe\n\n * You can try to reduce the parameter selected by the N card hard-coded preset in the output quality setting, or turn off this function.\n * Use CPU H.265 Fast or ProRes encoding\n\n\n# Solve the abnormal size of SVFI window\n\nOpen the location of SVFI from Steam, the community version finds SVFI.Community.exe The professional version finds SVFI.Professional.exe Right click, properties, compatibility, change high DPI settings, override high DPI scaling behavior, system\n\n\n# Fewer features available than in the tutorial?\n\n * Not updated to latest version\n * Some functions are in internal testing and will be made public later\n * The purchased version is not the pro version\n\n\n# How to only perform super resolution without supplementing frames at the same time\n\nAfter setting the parameters, double the magnification of the supplementary frame, and then click one-key suppression (do not click one-key supplementary frame)\n\n\n# How to make slow motion supplementary frames (upgrade, etc.)\n\nFor the actual shooting, refer to the time remapping in the custom code For animation, first open the one-shot three-deduplication, and then refer to the time remapping in the custom coding, or use AE to perform post-processing such as variable speed and supplementary frames after one-shot three deduplication and re-replenishment',normalizedContent:'# the software prompts that the video memory is insufficient or how to solve the problem of insufficient memory\n\ninsufficient video memory:\n\n * frame supplement 1080p video requires at least 2g video memory, 4k video requires at least 6g video memory. please try to enable interleaved frames to avoid exceeding the video memory limit.\n\n * if you are sure that the video memory of your machine is large enough, you can try restart the machine to see if it solves the problem.\n\n * in the output resolution setting, to reduce the video resolution, you can try to turn on the half-precision mode in the output quality setting. you can also try to lower the optical flow scale in the supplementary frame setting (setting 2.0 and above will greatly increase the memory usage, please be sure to check the value of this option).\n\nnot enough storage:\n\n * for machines with a running memory below 16g, it is recommended to use the manually specify the buffer memory size function in output quality settings to specify the memory usage size. recommended minimum value: 2g.\n\n\n# is it possible to restore the progress if the frame complement is quit in the middle? can\'t find progress?\n\n * to work status recovery midpoint auto find progress.\n * if the software displays "no work progress found" after clicking this button, please follow the steps below to troubleshoot the problem:\n   * verify that the output folder exists\n   * confirm that there is a video file named chunk-xxx-yyyyyy-zzzzzz in the output folder, if it does not exist, it means that there is no unrecoverable work progress\n   * confirm that the last 10 digits of the output folder are the same as the task id of the input file, if not, please manually change the task id and click the button to try again.\n   * if the problem still cannot be solved, please move the mouse over the option button, read the option description carefully, and follow the prompts to fill in appropriate values in start block count and start input frame number.\n\n\n# what should i do if the supplementary frame effect is not silky\n\n * in this case, there may be duplicate frames in the original video, or animation (with 1 beat n), you can try to enable duplicate frame removal in the transition recognition and animation optimization tab, and adjust the deduplication value.\n\n\n# what to do if the image of the exported video is noisy\n\n * before complementing the frame, go to the encoding quality setting lower the rendering quality crf value, or adjust the compression preset.\n\n\n# what should i do if the graphics card usage is low?\n\n * first, in the task manager, click the small triangle next to the 3d occupancy rate and replace it with cuda.\n * cuda usage is generally around 85% or higher is normal.\n * if the usage is still low, please check whether the cpu utilization reaches 100%; if it reaches 100%, it means that the software encounters a cpu bottleneck. in this case, you can adjust other options until the graphics card usage reaches more than 80%.\n\n\n# what should i do if the frame-filling video is distorted?\n\n * if this happens in animation supplementary frames, it is recommended to turn off the deduplication frame, or reduce the deduplication value, or try to increase the optical flow scale to reduce the possibility of picture jelly. **\n * if you have the skills related to video editing, you can patiently use different parameters to add more frames to produce several videos, and splicing each segment with an excellent result.\n * if the embedded subtitles (hard subtitles, burned subtitles) are distorted, there is currently no better solution. if you want to add frames to animations, it is recommended to use resources without subtitles to add frames, and then find subtitles to embed.\n\n\n# will supplementing the frame for a long time hurt the graphics card\n\n * running cuda for a long time will generally not affect the life of the graphics card, but if the heat dissipation measures are not done well, the temperature is too high, or running the frame recovery program after overclocking will still cause damage to the graphics card.\n * according to the eula (software user agreement), svfi is not responsible for hardware damage caused by supplementary frames.\n\n\n# what to do if the video memory is not enough\n\n * the more video memory is occupied, the faster the speed will be. even if the video memory is fully occupied by running svfi programs, the speed may not be improved, because there are many other limiting factors such as the length of the rendering queue and the limitation of machine power consumption.\n\n\n# what about broken pipe\n\n * you can try to reduce the parameter selected by the n card hard-coded preset in the output quality setting, or turn off this function.\n * use cpu h.265 fast or prores encoding\n\n\n# solve the abnormal size of svfi window\n\nopen the location of svfi from steam, the community version finds svfi.community.exe the professional version finds svfi.professional.exe right click, properties, compatibility, change high dpi settings, override high dpi scaling behavior, system\n\n\n# fewer features available than in the tutorial?\n\n * not updated to latest version\n * some functions are in internal testing and will be made public later\n * the purchased version is not the pro version\n\n\n# how to only perform super resolution without supplementing frames at the same time\n\nafter setting the parameters, double the magnification of the supplementary frame, and then click one-key suppression (do not click one-key supplementary frame)\n\n\n# how to make slow motion supplementary frames (upgrade, etc.)\n\nfor the actual shooting, refer to the time remapping in the custom code for animation, first open the one-shot three-deduplication, and then refer to the time remapping in the custom coding, or use ae to perform post-processing such as variable speed and supplementary frames after one-shot three deduplication and re-replenishment',charsets:{}},{title:"Tips and Presets",frontmatter:{title:"Tips and Presets",date:"2023-05-28T22:46:25.000Z",permalink:"/pages/18309a/"},regularPath:"/30.FAQ/10.Tips.html",relativePath:"30.FAQ/10.Tips.md",key:"v-cc7d6564",path:"/pages/18309a/",headers:[{level:2,title:"Recommended task presets",slug:"recommended-task-presets",normalizedTitle:"recommended task presets",charIndex:2},{level:3,title:"Community Edition",slug:"community-edition",normalizedTitle:"community edition",charIndex:107},{level:4,title:"Animation (VFI)",slug:"animation-vfi",normalizedTitle:"animation (vfi)",charIndex:128},{level:4,title:"Anime (slow-motion supplementary frames and animation clips)",slug:"anime-slow-motion-supplementary-frames-and-animation-clips",normalizedTitle:"anime (slow-motion supplementary frames and animation clips)",charIndex:1162},{level:4,title:"real people",slug:"real-people",normalizedTitle:"real people",charIndex:2084},{level:3,title:"Professional Edition",slug:"professional-edition",normalizedTitle:"professional edition",charIndex:2981},{level:4,title:"Animation (VFI)",slug:"animation-vfi-2",normalizedTitle:"animation (vfi)",charIndex:128},{level:4,title:"Anime (slow-motion supplementary frames and animation clips)",slug:"anime-slow-motion-supplementary-frames-and-animation-clips-2",normalizedTitle:"anime (slow-motion supplementary frames and animation clips)",charIndex:1162},{level:4,title:"real people",slug:"real-people-2",normalizedTitle:"real people",charIndex:2084},{level:2,title:"skills",slug:"skills",normalizedTitle:"skills",charIndex:6085}],headersStr:"Recommended task presets Community Edition Animation (VFI) Anime (slow-motion supplementary frames and animation clips) real people Professional Edition Animation (VFI) Anime (slow-motion supplementary frames and animation clips) real people skills",content:'# Recommended task presets\n\nFor compression mode, resolution above 2K MUST use H265 or ProRes encoding\n\n\n# Community Edition\n\n# Animation (VFI)\n\nSCENE                                                      VIDEO FLUENCY OPTIMIZATION   SUPPRESSION MODE       OPTICAL FLOW SCALE   FRAME INTERPOLATION MODEL\nVery high quality                                          Space-time linearization     CPU H.265 medium       1.0                  gmfss union v\nquality                                                    space-time linearization     CPU H.265 medium       0.5                  rife 2.3\ngeneral                                                    space-time linearization     CPU H.265 fast         0.5                  rife 2.3\nspeed (N card)                                             space-time linearization     NVENCC H.264 quality   0.5                  rife 4.6\nSpeed (N card, nuclear display acceleration suppression)   Space-time linearization     QSVENCC H.264 best     0.5                  rife 4.6\nSpeed (A card)                                             Space-time linearization     VCEENCC H.264 slow     0.5                  ncnn rife 4.6\n\n# Anime (slow-motion supplementary frames and animation clips)\n\nSCENE                                                        VIDEO FLUENCY OPTIMIZATION             SUPPRESSION MODE                     OPTICAL FLOW SCALE   FRAME INTERPOLATION MODEL\nExtremely smooth (the material frame rate is around 24fps)   Time-space resampling                  Choose from the preset table above   1.0                  gmfss union v\nExtremely smooth (the material frame rate is around 24fps)   First-order difference deduplication   Choose from the preset table above   0.5                  rife 2.3\nExtremely smooth (the material frame rate is around 24fps)   Remove 1 shot 2 and 1 shot 3           Choose from the preset table above   0.5                  rife 2.3\nReduce stutter only, suppress jelly introduction             Fixed threshold deduplication 0.80     Choose from preset table above       0.5                  Rife 2.3\n\n# real people\n\nSCENE                                                      VIDEO FLUENCY OPTIMIZATION     SUPPRESSION MODE       OPTICAL FLOW SCALE   FRAME INTERPOLATION MODEL\nquality                                                    spatiotemporal linearization   CPU H.265 medium       1.0                  rife 2.3\ngeneral                                                    space-time linearization       CPU H.265 fast         1.0                  rife 2.3\nspeed (N card)                                             space-time linearization       NVENCC H.264 quality   1.0                  rife 4.6\nSpeed (N card, nuclear display acceleration suppression)   Space-time linearization       QSVENCC H.264 best     1.0                  rife 4.6\nSpeed (A card)                                             Space-time linearization       VCEENCC H.264 slow     1.0                  ncnn rife 4.6\n\n\n# Professional Edition\n\nWarning\n\nMake sure your player can play H.265 10bit, Otherwise choose H.264 veryslow\n\n# Animation (VFI)\n\nSCENE                                                      VIDEO FLUENCY OPTIMIZATION      SUPPRESSION MODE       OPTICAL FLOW SCALE             FRAME INTERPOLATION MODEL\nVery high quality                                          Space-time linearization        CPU H.265 medium       1.0                            gmfss union v\nquality                                                    spatiotemporal linearization    CPU H.265 medium       dynamic optical flow scaling   rife 2.3\nregular (balanced)                                         spatio-temporal linearization   CPU H.265 fast         dynamic optical flow scaling   rife 2.3\nSpeed (N card)                                             Space-time linearization        NVENCC H.264 quality   Dynamic optical flow scale     rife 4.6\nSpeed (N card, nuclear display acceleration suppression)   Space-time linearization        QSVENCC H.264 best     Dynamic optical flow scale     rife 4.6\nSpeed (A card)                                             Space-time linearization        VCEENCC H.264 slow     Dynamic optical flow scale     ncnn rife 4.6\n\n# Anime (slow-motion supplementary frames and animation clips)\n\nSCENE                                                        VIDEO FLUENCY OPTIMIZATION               SUPPRESSION MODE                     OPTICAL FLOW SCALE           FRAME INTERPOLATION MODEL\nExtremely smooth (the material frame rate is around 24fps)   Time-space resampling                    Choose from the preset table above   1.0                          gmfss union v\nExtremely smooth (the material frame rate is around 24fps)   First-order difference de-emphasis       Choose from the preset list above    Dynamic optical flow scale   rife 2.3\nExtremely smooth (the material frame rate is around 24fps)   Remove one shot two and one shot three   Choose from the preset table above   Dynamic optical flow scale   rife 2.3\nOnly reduce stuttering, suppress the introduction of jelly   Fixed threshold de-emphasis 0.80         Choose from the preset table above   Dynamic optical flow scale   Rife 2.3\n\n# real people\n\nSCENE                                                      VIDEO FLUENCY OPTIMIZATION     SUPPRESSION MODE       OPTICAL FLOW SCALE   FRAME INTERPOLATION MODEL\nquality                                                    spatiotemporal linearization   CPU H.265 medium       1.0                  rife 2.3\nRegular (balanced)                                         Space-time linearization       CPU H.265 fast         1.0                  rife 4.6\nspeed (N card)                                             space-time linearization       NVENCC H.264 quality   1.0                  rife 4.6\nSpeed (N card, nuclear display acceleration suppression)   Space-time linearization       QSVENCC H.264 best     1.0                  rife 4.6\nSpeed (A card)                                             Space-time linearization       VCEENCC H.264 slow     1.0                  ncnn rife 4.6\n\n\n# skills\n\n * In SVFI, most options are not the higher the "number", the better the effect (such as transition parameters, deduplication parameters, CRF, optical flow scale, frame complement model). If you don\'t understand, it is recommended to choose strictly according to the instructions or default.\n\n * **For each option with (?) or (!), hovering over it will show the corresponding explanation. **\n\n * If pursuing quality, all select presets above normal; speed options are relatively ineffective.\n\n * If you do not have relevant knowledge about CRF, select parameter 16 by default or learn relevant knowledge through Baidu.\n\n * Hover the mouse over Transition Options, you can see corresponding instructions. Transition options are usually parameters set at 12.\n\n * Deduplication option Only 2d anime needs to use the deduplication option 3d, real people will choose not to remove duplicate frames.\n\n * Community Edition: In the single recognition function, according to user feedback, it is better to set the parameter value at 0.8-1.0**.\n\n * Pro version: Features 1 shot 2 more conservative, software fewer bugs. The function 1 shot 3 and above is more aggressive and smoother, but problems and bugs are often more (such as screen disappearing, etc.).\n\n * If there are no special circumstances, it is recommended that you select function 1 and shoot 2, and all other options and parameters can be defaulted.\n\n * When svfi complements the frame, it will generally occupy the CPU (when CPU encoding is selected) and the cuda of the n card. If the CPU is always fully loaded and the cuda usage fluctuates greatly, the CPU is the bottleneck. At this time, you can adjust the compression preset or replace the encoder to speed up (such as adjusting CPU H.265 very slow to fast, CPU H.264 slow to NVENC H.264 slow).\n\n * Note that better encoders and presets are recommended if CPU is not the bottleneck. Because the cuda of the graphics card determines the speed of supplementary frames, and the CPU determines the speed of encoding. The speed of supplementary frame can only be improved by replacing a better graphics card, but encoding speed can be considered by choosing a faster, lower-quality encoder and preset.\n\n * The CPU encoding is soft editing, soft editing ** generally slow speed, small files, good quality **.\n\n * NVENC, QSV, and VCE are hardware encoding, among which NVENC uses nVidia graphics card, QSV uses Intel graphics card, and VCE uses AMD graphics card. The characteristics of hardware encoding are fast speed and large file size. The CPU is poor.\n\n * For hard encoding, NVENC is preferred. In the N card hard encoding preset (you can hover the mouse to see the description), you can set it yourself in <https://developer.nvidia.com/video-encode-and-decode-gpu -support-matrix-new> Query the hard-coded presets of your graphics card. The 20 series and 30 series are generally 7th+.\n\n * Hard coding will have a certain load on the graphics card. If you choose NVENC and a Broken Pipe error occurs, then Reduce the N card hard coding preset or switch to the core video code QSV. If you still get the same error, then Use CPU.\n\n * Suggestions for super resolution VFI: If you use cugan, esrgan and other algorithms for super resolution, it is recommended to perform super resolution supplementary frames separately, otherwise the video memory will overflow (CUDA out of memory will cause many problems) If you only have one graphics card that can be used for super resolution and supplementary frames, it is recommended to close other software that may occupy video memory before starting the task. If you don’t mind the trouble, because the Windows desktop itself will occupy a certain amount of video memory, if your computer has a nuclear display, you can restart the computer and enter the BIOS to set the nuclear display output, and insert the display signal plug into the motherboard, and then turn it on to use SVFI. In this way, the maximum available video memory can be obtained, and the processing speed of super-complemented frames will be a little faster.\n\n * Suggestions for suppression settings: If the compression group is not very strict on compression, hardware encoding (NVENC, VCE, QSV, etc.) can be used to avoid CPU compression bottlenecks. CPU bottlenecks will cause graphics card usage to drop CRF is set to 16, and the rest can be done as above\n\n * The most commonly used models for supplementary frames: Use the 2.3 model\n\n * Commonly used models for animation super resolution: Using the CUGAN model',normalizedContent:'# recommended task presets\n\nfor compression mode, resolution above 2k must use h265 or prores encoding\n\n\n# community edition\n\n# animation (vfi)\n\nscene                                                      video fluency optimization   suppression mode       optical flow scale   frame interpolation model\nvery high quality                                          space-time linearization     cpu h.265 medium       1.0                  gmfss union v\nquality                                                    space-time linearization     cpu h.265 medium       0.5                  rife 2.3\ngeneral                                                    space-time linearization     cpu h.265 fast         0.5                  rife 2.3\nspeed (n card)                                             space-time linearization     nvencc h.264 quality   0.5                  rife 4.6\nspeed (n card, nuclear display acceleration suppression)   space-time linearization     qsvencc h.264 best     0.5                  rife 4.6\nspeed (a card)                                             space-time linearization     vceencc h.264 slow     0.5                  ncnn rife 4.6\n\n# anime (slow-motion supplementary frames and animation clips)\n\nscene                                                        video fluency optimization             suppression mode                     optical flow scale   frame interpolation model\nextremely smooth (the material frame rate is around 24fps)   time-space resampling                  choose from the preset table above   1.0                  gmfss union v\nextremely smooth (the material frame rate is around 24fps)   first-order difference deduplication   choose from the preset table above   0.5                  rife 2.3\nextremely smooth (the material frame rate is around 24fps)   remove 1 shot 2 and 1 shot 3           choose from the preset table above   0.5                  rife 2.3\nreduce stutter only, suppress jelly introduction             fixed threshold deduplication 0.80     choose from preset table above       0.5                  rife 2.3\n\n# real people\n\nscene                                                      video fluency optimization     suppression mode       optical flow scale   frame interpolation model\nquality                                                    spatiotemporal linearization   cpu h.265 medium       1.0                  rife 2.3\ngeneral                                                    space-time linearization       cpu h.265 fast         1.0                  rife 2.3\nspeed (n card)                                             space-time linearization       nvencc h.264 quality   1.0                  rife 4.6\nspeed (n card, nuclear display acceleration suppression)   space-time linearization       qsvencc h.264 best     1.0                  rife 4.6\nspeed (a card)                                             space-time linearization       vceencc h.264 slow     1.0                  ncnn rife 4.6\n\n\n# professional edition\n\nwarning\n\nmake sure your player can play h.265 10bit, otherwise choose h.264 veryslow\n\n# animation (vfi)\n\nscene                                                      video fluency optimization      suppression mode       optical flow scale             frame interpolation model\nvery high quality                                          space-time linearization        cpu h.265 medium       1.0                            gmfss union v\nquality                                                    spatiotemporal linearization    cpu h.265 medium       dynamic optical flow scaling   rife 2.3\nregular (balanced)                                         spatio-temporal linearization   cpu h.265 fast         dynamic optical flow scaling   rife 2.3\nspeed (n card)                                             space-time linearization        nvencc h.264 quality   dynamic optical flow scale     rife 4.6\nspeed (n card, nuclear display acceleration suppression)   space-time linearization        qsvencc h.264 best     dynamic optical flow scale     rife 4.6\nspeed (a card)                                             space-time linearization        vceencc h.264 slow     dynamic optical flow scale     ncnn rife 4.6\n\n# anime (slow-motion supplementary frames and animation clips)\n\nscene                                                        video fluency optimization               suppression mode                     optical flow scale           frame interpolation model\nextremely smooth (the material frame rate is around 24fps)   time-space resampling                    choose from the preset table above   1.0                          gmfss union v\nextremely smooth (the material frame rate is around 24fps)   first-order difference de-emphasis       choose from the preset list above    dynamic optical flow scale   rife 2.3\nextremely smooth (the material frame rate is around 24fps)   remove one shot two and one shot three   choose from the preset table above   dynamic optical flow scale   rife 2.3\nonly reduce stuttering, suppress the introduction of jelly   fixed threshold de-emphasis 0.80         choose from the preset table above   dynamic optical flow scale   rife 2.3\n\n# real people\n\nscene                                                      video fluency optimization     suppression mode       optical flow scale   frame interpolation model\nquality                                                    spatiotemporal linearization   cpu h.265 medium       1.0                  rife 2.3\nregular (balanced)                                         space-time linearization       cpu h.265 fast         1.0                  rife 4.6\nspeed (n card)                                             space-time linearization       nvencc h.264 quality   1.0                  rife 4.6\nspeed (n card, nuclear display acceleration suppression)   space-time linearization       qsvencc h.264 best     1.0                  rife 4.6\nspeed (a card)                                             space-time linearization       vceencc h.264 slow     1.0                  ncnn rife 4.6\n\n\n# skills\n\n * in svfi, most options are not the higher the "number", the better the effect (such as transition parameters, deduplication parameters, crf, optical flow scale, frame complement model). if you don\'t understand, it is recommended to choose strictly according to the instructions or default.\n\n * **for each option with (?) or (!), hovering over it will show the corresponding explanation. **\n\n * if pursuing quality, all select presets above normal; speed options are relatively ineffective.\n\n * if you do not have relevant knowledge about crf, select parameter 16 by default or learn relevant knowledge through baidu.\n\n * hover the mouse over transition options, you can see corresponding instructions. transition options are usually parameters set at 12.\n\n * deduplication option only 2d anime needs to use the deduplication option 3d, real people will choose not to remove duplicate frames.\n\n * community edition: in the single recognition function, according to user feedback, it is better to set the parameter value at 0.8-1.0**.\n\n * pro version: features 1 shot 2 more conservative, software fewer bugs. the function 1 shot 3 and above is more aggressive and smoother, but problems and bugs are often more (such as screen disappearing, etc.).\n\n * if there are no special circumstances, it is recommended that you select function 1 and shoot 2, and all other options and parameters can be defaulted.\n\n * when svfi complements the frame, it will generally occupy the cpu (when cpu encoding is selected) and the cuda of the n card. if the cpu is always fully loaded and the cuda usage fluctuates greatly, the cpu is the bottleneck. at this time, you can adjust the compression preset or replace the encoder to speed up (such as adjusting cpu h.265 very slow to fast, cpu h.264 slow to nvenc h.264 slow).\n\n * note that better encoders and presets are recommended if cpu is not the bottleneck. because the cuda of the graphics card determines the speed of supplementary frames, and the cpu determines the speed of encoding. the speed of supplementary frame can only be improved by replacing a better graphics card, but encoding speed can be considered by choosing a faster, lower-quality encoder and preset.\n\n * the cpu encoding is soft editing, soft editing ** generally slow speed, small files, good quality **.\n\n * nvenc, qsv, and vce are hardware encoding, among which nvenc uses nvidia graphics card, qsv uses intel graphics card, and vce uses amd graphics card. the characteristics of hardware encoding are fast speed and large file size. the cpu is poor.\n\n * for hard encoding, nvenc is preferred. in the n card hard encoding preset (you can hover the mouse to see the description), you can set it yourself in <https://developer.nvidia.com/video-encode-and-decode-gpu -support-matrix-new> query the hard-coded presets of your graphics card. the 20 series and 30 series are generally 7th+.\n\n * hard coding will have a certain load on the graphics card. if you choose nvenc and a broken pipe error occurs, then reduce the n card hard coding preset or switch to the core video code qsv. if you still get the same error, then use cpu.\n\n * suggestions for super resolution vfi: if you use cugan, esrgan and other algorithms for super resolution, it is recommended to perform super resolution supplementary frames separately, otherwise the video memory will overflow (cuda out of memory will cause many problems) if you only have one graphics card that can be used for super resolution and supplementary frames, it is recommended to close other software that may occupy video memory before starting the task. if you don’t mind the trouble, because the windows desktop itself will occupy a certain amount of video memory, if your computer has a nuclear display, you can restart the computer and enter the bios to set the nuclear display output, and insert the display signal plug into the motherboard, and then turn it on to use svfi. in this way, the maximum available video memory can be obtained, and the processing speed of super-complemented frames will be a little faster.\n\n * suggestions for suppression settings: if the compression group is not very strict on compression, hardware encoding (nvenc, vce, qsv, etc.) can be used to avoid cpu compression bottlenecks. cpu bottlenecks will cause graphics card usage to drop crf is set to 16, and the rest can be done as above\n\n * the most commonly used models for supplementary frames: use the 2.3 model\n\n * commonly used models for animation super resolution: using the cugan model',charsets:{}},{title:"Support this project",frontmatter:{title:"Support this project",date:"2020-05-12T15:09:57.000Z",permalink:"/pages/1b12ed",sidebar:!1,article:!1},regularPath:"/40.Support/01.Support.html",relativePath:"40.Support/01.Support.md",key:"v-5c59659c",path:"/pages/1b12ed/",headersStr:null,content:"BUY SVFI !",normalizedContent:"buy svfi !",charsets:{}},{title:"博客文章",frontmatter:{archivesPage:!0,title:"博客文章",permalink:"/blog/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-70767908",path:"/blog/",headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"Home",frontmatter:{home:!0,heroImage:"/img/logo.webp",heroText:"Squirrel-RIFE Video Frame Interpolation",tagline:null,actionText:"Designed for Anime Enhance →",actionLink:"pages/0e988c/",bannerBg:"none",features:[{title:"Out of the Box",details:"Automatic preset system, automatically adjust settings to get output quality"},{title:"High Quality",details:"Settings with high granularity and tons of options for high quality control"},{title:"Fast Process",details:"Lots of optimizations to avoid jelly scenes, blurred transitions, etc."}],postList:"none"},regularPath:"/",relativePath:"index.md",key:"v-0ef24a56",path:"/",headers:[{level:2,title:"🌎 Demonstrations on BiliBili",slug:"🌎-demonstrations-on-bilibili",normalizedTitle:"🌎 demonstrations on bilibili",charIndex:16},{level:2,title:"🎖 Free Apps!",slug:"🎖-free-apps",normalizedTitle:"🎖 free apps!",charIndex:913},{level:2,title:"💻 System Requirements",slug:"💻-system-requirements",normalizedTitle:"💻 system requirements",charIndex:1511},{level:2,title:"⚡ Feedback & Communication",slug:"⚡-feedback-communication",normalizedTitle:"⚡ feedback &amp; communication",charIndex:null},{level:2,title:"🤝 Reference & Acknowledgement",slug:"🤝-reference-acknowledgement",normalizedTitle:"🤝 reference &amp; acknowledgement",charIndex:null},{level:2,title:"👓 Some notes",slug:"👓-some-notes",normalizedTitle:"👓 some notes",charIndex:2403}],headersStr:"🌎 Demonstrations on BiliBili 🎖 Free Apps! 💻 System Requirements ⚡ Feedback & Communication 🤝 Reference & Acknowledgement 👓 Some notes",content:"On STEAM\n\n\n\n\n\n# 🌎 Demonstrations on BiliBili\n\nGenshin Impact\n\nDrama CM short film, 8K 60fps\n\nSVFI Vision\n\nUmaron\n\nSeason 2 NCOP 8K 60fps\n\nSVFI Vision\n\nRe Zero-Starting Life in Another World\n\nSeason 2 NCED Believe in you\n\nSVFI Vision\n\nconfig:\n    target: _blank\n    imgHeight: auto\n    objectFit: contain\n    lineClamp: 1\n\ndata:\n- img: /img/bilibili/yuan.jpg\n  name: Genshin Impact\n  desc: Drama CM short film, 8K 60fps\n  link: https://www.bilibili.com/video/BV1FS4y1C7RD\n  author: SVFI Vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/umaron.jpg\n  name: Umaron\n  desc: Season 2 NCOP 8K 60fps\n  link: https://www.bilibili.com/video/BV1QY411b7e4\n  author: SVFI Vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/emilia.jpg\n  name: Re Zero-Starting Life in Another World\n  desc: Season 2 NCED Believe in you\n  link: https://www.bilibili.com/video/BV1kF411p7FB\n  author: SVFI Vision\n  avatar: /img/svfi.ico\n\n\n\n# 🎖 Free Apps!\n\nSquirrel Anime Enhance\n\n🚀Open source super resolution application\n\nSVFI Demo\n\nThe demo version of SVFI, high quality, freely customized VFI and SR output\n\n- name: Squirrel Anime Enhance\n  desc: 🚀Open source super resolution application\n  link: https://github.com/Justin62628/Squirrel-RIFE/releases/tag/v3.20.4\n  bgColor: '#eaeef1'\n  textColor: '#2A3344'\n- name: SVFI Demo\n  desc: The demo version of SVFI, high quality, freely customized VFI and SR output\n  link: https://store.steampowered.com/search/?sort_by=_ASC&term=SVFI+Demo\n  bgColor: '#eaeef1'\n  textColor: '#2A3344'\n\n\n\n\n# 💻 System Requirements\n\n * Windows 10 or above\n\n * NVIDIA GPU (> GTX 750ti) or AMD GPU released within 5 years\n\n * 2GB+ of VRAM, 4GB+ of RAM and 10GB+ of free disk space\n\n\n# ⚡ Feedback & Communication\n\nIf you have any questions or advice using of the software, please feel free to post in the Steam Forum. Or join the BBS Forum:\n\n\n# 🤝 Reference & Acknowledgement\n\n * Video Frame Interpolation Algorithm: RIFE\n * NCNN Support: RIFE-NCNN\n * SWIG Wraps: Everything-ncnn-vulkan-python\n * UI Design: QCandyUi\n * Steamworks Interface: SteamworksPy\n * Encode Supports: FFmpeg, QSVEnc, [NVEnc](https://github.com/ rigaya/NVEnc), dovi_tool, dlb_mp4base, [hdr10plus_parser](https://github.com /quietvoid/hdr10plus_parser)\n * Super Resolution Algorithm: waifu2x, RealESR, [RealCUGAN](https://github .com/bilibili/ailab/blob/main/Real-CUGAN/LICENSE)\n * View More at LICENSE and LICENSES_BUNDLE FILE\n\n\n# 👓 Some notes\n\n * According to EULA, SVFI's output is not allowed for commercial use.",normalizedContent:"on steam\n\n\n\n\n\n# 🌎 demonstrations on bilibili\n\ngenshin impact\n\ndrama cm short film, 8k 60fps\n\nsvfi vision\n\numaron\n\nseason 2 ncop 8k 60fps\n\nsvfi vision\n\nre zero-starting life in another world\n\nseason 2 nced believe in you\n\nsvfi vision\n\nconfig:\n    target: _blank\n    imgheight: auto\n    objectfit: contain\n    lineclamp: 1\n\ndata:\n- img: /img/bilibili/yuan.jpg\n  name: genshin impact\n  desc: drama cm short film, 8k 60fps\n  link: https://www.bilibili.com/video/bv1fs4y1c7rd\n  author: svfi vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/umaron.jpg\n  name: umaron\n  desc: season 2 ncop 8k 60fps\n  link: https://www.bilibili.com/video/bv1qy411b7e4\n  author: svfi vision\n  avatar: /img/svfi.ico\n- img: /img/bilibili/emilia.jpg\n  name: re zero-starting life in another world\n  desc: season 2 nced believe in you\n  link: https://www.bilibili.com/video/bv1kf411p7fb\n  author: svfi vision\n  avatar: /img/svfi.ico\n\n\n\n# 🎖 free apps!\n\nsquirrel anime enhance\n\n🚀open source super resolution application\n\nsvfi demo\n\nthe demo version of svfi, high quality, freely customized vfi and sr output\n\n- name: squirrel anime enhance\n  desc: 🚀open source super resolution application\n  link: https://github.com/justin62628/squirrel-rife/releases/tag/v3.20.4\n  bgcolor: '#eaeef1'\n  textcolor: '#2a3344'\n- name: svfi demo\n  desc: the demo version of svfi, high quality, freely customized vfi and sr output\n  link: https://store.steampowered.com/search/?sort_by=_asc&term=svfi+demo\n  bgcolor: '#eaeef1'\n  textcolor: '#2a3344'\n\n\n\n\n# 💻 system requirements\n\n * windows 10 or above\n\n * nvidia gpu (> gtx 750ti) or amd gpu released within 5 years\n\n * 2gb+ of vram, 4gb+ of ram and 10gb+ of free disk space\n\n\n# ⚡ feedback & communication\n\nif you have any questions or advice using of the software, please feel free to post in the steam forum. or join the bbs forum:\n\n\n# 🤝 reference & acknowledgement\n\n * video frame interpolation algorithm: rife\n * ncnn support: rife-ncnn\n * swig wraps: everything-ncnn-vulkan-python\n * ui design: qcandyui\n * steamworks interface: steamworkspy\n * encode supports: ffmpeg, qsvenc, [nvenc](https://github.com/ rigaya/nvenc), dovi_tool, dlb_mp4base, [hdr10plus_parser](https://github.com /quietvoid/hdr10plus_parser)\n * super resolution algorithm: waifu2x, realesr, [realcugan](https://github .com/bilibili/ailab/blob/main/real-cugan/license)\n * view more at license and licenses_bundle file\n\n\n# 👓 some notes\n\n * according to eula, svfi's output is not allowed for commercial use.",charsets:{}}],themeConfig:{nav:[{text:"Homepage",link:"/"},{text:"Quick Start",link:"/pages/dc46b8",items:[{text:"What is framerate, resolution and bitrate",link:"/pages/dc46b8/"},{text:"What is VFI",link:"/pages/88ed7f/"},{text:"What is Super Resolution",link:"/pages/681961/"},{text:"Quick start for SVFI",link:"/pages/0e988c/"}]},{text:"Related Articles",link:"/pages/8cc1b5",items:[{text:"Image and quality",link:"/pages/8cc1b5/"},{text:"What is encoding, encoder and container",link:"/pages/76d9d4/"},{text:"What is encoding",link:"/pages/7b7d11/"},{text:"What is HDR",link:"/pages/89244b/"}]},{text:"Software User Guide",link:"/pages/f8b952/",items:[{text:"Quick Guide",link:"/pages/f8b952/"},{text:"Advanced Settings",link:"/pages/052617/"},{text:"Command Line Interface",link:"/pages/ceb849/"}]},{text:"FAQs and presets",link:"/pages/9cc27d/",items:[{text:"FAQ",link:"/pages/9cc27d/"},{text:"Tips and presets",link:"/pages/18309a/"}]},{text:"Support",link:"/pages/1b12ed/"}],sidebarDepth:2,logo:"/img/svfi.ico",repo:"Justin62628/Squirrel-RIFE",searchMaxSuggestions:10,lastUpdated:"Last Update",sidebar:{"/10.Getting Started/":[{title:"Getting Started",collapsable:!1,children:[["10.Getting Started/10.Framerate, resolution, bitrate.md","What is frame rate, resolution and bitrate","/pages/dc46b8/"],["10.Getting Started/20.What is VFI.md","What is VFI","/pages/88ed7f/"],["10.Getting Started/30.What is SR.md","What is SR","/pages/681961/"],["10.Getting Started/40.Quick tour of SVFI.md","Get Started with SVFI","/pages/0e988c/"]]},{title:"Related Articles",collapsable:!1,children:[["20.Related Articles/01.Image representation and quality.md","Image representation and quality","/pages/8cc1b5/"],["20.Related Articles/03.Encodings, Encoders, and Encapsulation Formats.md","Encoding, Encoder and Encapsulation Format","/pages/76d9d4/"],["20.Related Articles/07.what is encoding.md","What is encoding","/pages/7b7d11/"],["20.Related Articles/08.What is HDR.md","What is HDR","/pages/89244b/"]]}],catalogue:{},"/20.Manuals/":[["10.Quick Start.md","Quick Start","/pages/f8b952/"],["20.Option Manuals.md","Detailed Explanation of Advanced Settings","/pages/052617/"],["21.Advanced CLI.md","Advanced Settings of Command Line Interface","/pages/ceb849/"]],"/30.FAQ/":[["01.Q&A.md","Q&A","/pages/9cc27d"],["10.Tips.md","Tips and Presets","/pages/18309a/"]],"/40.Support/":[["01.Support.md","Support this project","/pages/1b12ed"]]},updateBar:{showToArticle:!1},pageStyle:"line",category:!1,tag:!1,author:{name:"Justin62628",href:"https://github.com/Justin62628"},social:{icons:[{iconClass:"icon-youjian",title:"Send Email",link:"2410377391@qq.com"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/Justin62628/Squirrel-RIFE"}]},footer:{createYear:2022,copyrightInfo:"Justin62628 | MIT License"},htmlModules:{}}};var xl=n(88),kl=n.n(xl),_l=n(95),Tl=n(96),Cl=n(11);var Sl={computed:{$filterPosts(){return this.$site.pages.filter(e=>{const{frontmatter:{pageComponent:t,article:n,home:o}}=e;return!(t||!1===n||!0===o)})},$sortPosts(){return(e=this.$filterPosts).sort((e,t)=>{const n=e.frontmatter.sticky,o=t.frontmatter.sticky;return n&&o?n==o?Object(Cl.a)(e,t):n-o:n&&!o?-1:!n&&o?1:Object(Cl.a)(e,t)}),e;var e},$sortPostsByDate(){return(e=this.$filterPosts).sort((e,t)=>Object(Cl.a)(e,t)),e;var e},$groupPosts(){return function(e){const t={},n={};for(let o=0,i=e.length;o<i;o++){const{frontmatter:{categories:i,tags:r}}=e[o];"array"===Object(Cl.n)(i)&&i.forEach(n=>{n&&(t[n]||(t[n]=[]),t[n].push(e[o]))}),"array"===Object(Cl.n)(r)&&r.forEach(t=>{t&&(n[t]||(n[t]=[]),n[t].push(e[o]))})}return{categories:t,tags:n}}(this.$sortPosts)},$categoriesAndTags(){return function(e){const t=[],n=[];for(let n in e.categories)t.push({key:n,length:e.categories[n].length});for(let t in e.tags)n.push({key:t,length:e.tags[t].length});return{categories:t,tags:n}}(this.$groupPosts)}}};Gn.component(_l.default),Gn.component(Tl.default);function Il(e){return e.toString().padStart(2,"0")}n(244);Gn.component("Badge",()=>Promise.all([n.e(0),n.e(3)]).then(n.bind(null,352))),Gn.component("CodeBlock",()=>Promise.resolve().then(n.bind(null,95))),Gn.component("CodeGroup",()=>Promise.resolve().then(n.bind(null,96)));n(245),n(246),n(247);var Al=[kl.a,({Vue:e,options:t,router:n,siteData:o})=>{o.pages.map(e=>{const{frontmatter:{date:t,author:n}}=e;"string"==typeof t&&"Z"===t.charAt(t.length-1)&&(e.frontmatter.date=function(e){e instanceof Date||(e=new Date(e));return`${e.getUTCFullYear()}-${Il(e.getUTCMonth()+1)}-${Il(e.getUTCDate())} ${Il(e.getUTCHours())}:${Il(e.getUTCMinutes())}:${Il(e.getUTCSeconds())}`}(t)),n?e.author=n:o.themeConfig.author&&(e.author=o.themeConfig.author)}),e.mixin(Sl)},{},({Vue:e})=>{e.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},{},({Vue:e})=>{const{ignoredElements:t}=e.config;t.every(e=>"/^mjx-/"!==e.toString())&&t.push(/^mjx-/)},({router:e})=>{0}],El=[];class Rl extends class{constructor(){this.store=new Gn({data:{state:{}}})}$get(e){return this.store.state[e]}$set(e,t){Gn.set(this.store.state,e,t)}$emit(...e){this.store.$emit(...e)}$on(...e){this.store.$on(...e)}}{}Object.assign(Rl.prototype,{getPageAsyncComponent:as,getLayoutAsyncComponent:ss,getAsyncComponent:ls,getVueComponent:cs});var Dl={install(e){const t=new Rl;e.$vuepress=t,e.prototype.$vuepress=t}};function zl(e,t){const n=t.toLowerCase();return e.options.routes.some(e=>e.path.toLowerCase()===n)}var Ol={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(e){const t=this.pageKey||this.$parent.$page.key;return us("pageKey",t),Gn.component(t)||Gn.component(t,as(t)),Gn.component(t)?e(t):e("")}},Pl={functional:!0,props:{slotKey:String,required:!0},render:(e,{props:t,slots:n})=>e("div",{class:["content__"+t.slotKey]},n()[t.slotKey])},ql={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},jl=(n(248),n(249),Object(vl.a)(ql,(function(){var e=this._self._c;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),Fl={functional:!0,render(e,{parent:t,children:n}){if(t._isMounted)return n;t.$once("hook:mounted",()=>{t.$forceUpdate()})}};Gn.config.productionTip=!1,Gn.use(Ua),Gn.use(Dl),Gn.mixin(function(e,t,n=Gn){!function(e){e.locales&&Object.keys(e.locales).forEach(t=>{e.locales[t].path=t});Object.freeze(e)}(t),n.$vuepress.$set("siteData",t);const o=new(e(n.$vuepress.$get("siteData"))),i=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(o)),r={};return Object.keys(i).reduce((e,t)=>(t.startsWith("$")&&(e[t]=i[t].get),e),r),{computed:r}}(e=>class{setPage(e){this.__page=e}get $site(){return e}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:e={}}=this.$site;let t,n;for(const o in e)"/"===o?n=e[o]:0===this.$page.path.indexOf(o)&&(t=e[o]);return t||n||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:e}=this.$page.frontmatter;return"string"==typeof e&&e}get $title(){const e=this.$page,{metaTitle:t}=this.$page.frontmatter;if("string"==typeof t)return t;const n=this.$siteTitle,o=e.frontmatter.home?null:e.frontmatter.title||e.title;return n?o?o+" | "+n:n:o||"VuePress"}get $description(){const e=function(e){if(e){const t=e.filter(e=>"description"===e.name)[0];if(t)return t.content}}(this.$page.frontmatter.meta);return e||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(e,t){for(let n=0;n<e.length;n++){const o=e[n];if(o.path.toLowerCase()===t.toLowerCase())return o}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},wl)),Gn.component("Content",Ol),Gn.component("ContentSlotsDistributor",Pl),Gn.component("OutboundLink",jl),Gn.component("ClientOnly",Fl),Gn.component("Layout",ss("Layout")),Gn.component("NotFound",ss("NotFound")),Gn.prototype.$withBase=function(e){const t=this.$site.base;return"/"===e.charAt(0)?t+e.slice(1):e},window.__VUEPRESS__={version:"1.9.2",hash:""},async function(e){const t="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:wl.routerBase||wl.base,n=new Ua({base:t,mode:"history",fallback:!1,routes:bl,scrollBehavior:(e,t,n)=>n||(e.hash?!Gn.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(e.hash)}:{x:0,y:0})});!function(e){e.beforeEach((t,n,o)=>{if(zl(e,t.path))o();else if(/(\/|\.html)$/.test(t.path))if(/\/$/.test(t.path)){const n=t.path.replace(/\/$/,"")+".html";zl(e,n)?o(n):o()}else o();else{const n=t.path+"/",i=t.path+".html";zl(e,i)?o(i):zl(e,n)?o(n):o()}})}(n);const o={};try{await Promise.all(Al.filter(e=>"function"==typeof e).map(t=>t({Vue:Gn,options:o,router:n,siteData:wl,isServer:e})))}catch(e){console.error(e)}return{app:new Gn(Object.assign(o,{router:n,render:e=>e("div",{attrs:{id:"app"}},[e("RouterView",{ref:"layout"}),e("div",{class:"global-ui"},El.map(t=>e(t)))])})),router:n}}(!1).then(({app:e,router:t})=>{t.onReady(()=>{e.$mount("#app")})})}]);